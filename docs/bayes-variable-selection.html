<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 11 Bayes variable selection | Notes on Statistics</title>
  <meta name="description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 11 Bayes variable selection | Notes on Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Bayes variable selection | Notes on Statistics" />
  
  <meta name="twitter:description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2019-06-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="hamiltonian-monte-carlo.html">
<link rel="next" href="advanced-r.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes in statistics and computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>preliminary</a></li>
<li class="chapter" data-level="" data-path="e69d82e4b883e69d82e585ab.html"><a href="e69d82e4b883e69d82e585ab.html"><i class="fa fa-check"></i>杂七杂八</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html"><i class="fa fa-check"></i><b>2</b> Mathematical statistic Trick</a><ul>
<li class="chapter" data-level="2.1" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#NormalForm"><i class="fa fa-check"></i><b>2.1</b> Normal distribution as exponential family</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html"><i class="fa fa-check"></i><b>3</b> Statistician Tool Box</a><ul>
<li class="chapter" data-level="3.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#matrix"><i class="fa fa-check"></i><b>3.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="3.1.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#block-diagonal-matrices"><i class="fa fa-check"></i><b>3.1.1</b> Block diagonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#sumSquare"><i class="fa fa-check"></i><b>3.2</b> 两个二次型相加</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html"><i class="fa fa-check"></i><b>4</b> Longitudinal data analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#linear-mixed-model"><i class="fa fa-check"></i><b>4.1</b> Linear mixed model</a><ul>
<li class="chapter" data-level="4.1.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#condition-mean-vs-marginal-mean"><i class="fa fa-check"></i><b>4.1.1</b> Condition Mean vs Marginal mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#restricted-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>4.1.2</b> Restricted maximum likelihood estimation</a></li>
<li class="chapter" data-level="4.1.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#prediction"><i class="fa fa-check"></i><b>4.1.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#generalised-linear-mixed-models"><i class="fa fa-check"></i><b>4.2</b> Generalised linear mixed models</a><ul>
<li class="chapter" data-level="4.2.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#expFam"><i class="fa fa-check"></i><b>4.2.1</b> Exponential distribution family</a></li>
<li class="chapter" data-level="4.2.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#iteratively-reweighted-least-square-algorithm-iwls"><i class="fa fa-check"></i><b>4.2.2</b> Iteratively reweighted Least square algorithm (IWLS)</a></li>
<li class="chapter" data-level="4.2.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#glmms"><i class="fa fa-check"></i><b>4.2.3</b> GLMMs</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#the-bayesian-analysis-approach-for-covariance-modelling"><i class="fa fa-check"></i><b>4.3</b> The Bayesian analysis approach for covariance modelling</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 统计图形笔记</a></li>
<li class="chapter" data-level="6" data-path="basicmcmc.html"><a href="basicmcmc.html"><i class="fa fa-check"></i><b>6</b> BasicMCMC</a><ul>
<li class="chapter" data-level="6.1" data-path="basicmcmc.html"><a href="basicmcmc.html#metropolis-hastings-update"><i class="fa fa-check"></i><b>6.1</b> Metropolis-Hastings Update</a><ul>
<li class="chapter" data-level="6.1.1" data-path="basicmcmc.html"><a href="basicmcmc.html#metropolis-update"><i class="fa fa-check"></i><b>6.1.1</b> Metropolis Update</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="basicmcmc.html"><a href="basicmcmc.html#the-gibbs-update"><i class="fa fa-check"></i><b>6.2</b> The Gibbs Update</a><ul>
<li class="chapter" data-level="6.2.1" data-path="basicmcmc.html"><a href="basicmcmc.html#variable-at-a-time-metropolis-hastings"><i class="fa fa-check"></i><b>6.2.1</b> Variable-at-a-Time Metropolis-Hastings</a></li>
<li class="chapter" data-level="6.2.2" data-path="basicmcmc.html"><a href="basicmcmc.html#the-gibbs-is-a-special-case-of-metropolis-hastings"><i class="fa fa-check"></i><b>6.2.2</b> The Gibbs is a special case of Metropolis-Hastings:</a></li>
<li class="chapter" data-level="6.2.3" data-path="basicmcmc.html"><a href="basicmcmc.html#gibbs-full-conditional-distibution"><i class="fa fa-check"></i><b>6.2.3</b> Gibbs Full conditional distibution</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="basicmcmc.html"><a href="basicmcmc.html#combining-updates"><i class="fa fa-check"></i><b>6.3</b> Combining Updates</a><ul>
<li class="chapter" data-level="6.3.1" data-path="basicmcmc.html"><a href="basicmcmc.html#composition"><i class="fa fa-check"></i><b>6.3.1</b> Composition</a></li>
<li class="chapter" data-level="6.3.2" data-path="basicmcmc.html"><a href="basicmcmc.html#palindromic-composition"><i class="fa fa-check"></i><b>6.3.2</b> Palindromic Composition</a></li>
<li class="chapter" data-level="6.3.3" data-path="basicmcmc.html"><a href="basicmcmc.html#state-independent-mixing"><i class="fa fa-check"></i><b>6.3.3</b> State-Independent Mixing</a></li>
<li class="chapter" data-level="6.3.4" data-path="basicmcmc.html"><a href="basicmcmc.html#subsampling"><i class="fa fa-check"></i><b>6.3.4</b> Subsampling</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="basicmcmc.html"><a href="basicmcmc.html#a-metropolis-example"><i class="fa fa-check"></i><b>6.4</b> A Metropolis Example</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="r-trick.html"><a href="r-trick.html"><i class="fa fa-check"></i><b>7</b> R trick</a></li>
<li class="chapter" data-level="8" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>8</b> Reversible Jump MCMC</a><ul>
<li class="chapter" data-level="8.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#from-metropolis-hastings-to-reversible-jump"><i class="fa fa-check"></i><b>8.1.1</b> From Metropolis-Hastings to Reversible Jump</a></li>
<li class="chapter" data-level="8.1.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#application-area"><i class="fa fa-check"></i><b>8.1.2</b> Application Area</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#implementation"><i class="fa fa-check"></i><b>8.2</b> Implementation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#example-dimension-matching"><i class="fa fa-check"></i><b>8.2.1</b> Example Dimension Matching</a></li>
<li class="chapter" data-level="8.2.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#example-moment-matching-in-a-finite-mixture-of-univariate-normals"><i class="fa fa-check"></i><b>8.2.2</b> Example: Moment Matching in a Finite Mixture of Univariate Normals</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#mapping-functions-and-proposal-distribution"><i class="fa fa-check"></i><b>8.3</b> Mapping Functions and Proposal Distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#marginalization-and-augmentation"><i class="fa fa-check"></i><b>8.3.1</b> Marginalization and augmentation:</a></li>
<li class="chapter" data-level="8.3.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#centering-and-order-methods"><i class="fa fa-check"></i><b>8.3.2</b> Centering and Order Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html"><i class="fa fa-check"></i><b>9</b> Optimal Proposal Distributions and Adaptive MCMC</a><ul>
<li class="chapter" data-level="9.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#intro-1"><i class="fa fa-check"></i><b>9.1</b> Intro</a><ul>
<li class="chapter" data-level="9.1.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#mh-algorithm"><i class="fa fa-check"></i><b>9.1.1</b> MH algorithm</a></li>
<li class="chapter" data-level="9.1.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-scaling"><i class="fa fa-check"></i><b>9.1.2</b> Optimal Scaling</a></li>
<li class="chapter" data-level="9.1.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-mcmc"><i class="fa fa-check"></i><b>9.1.3</b> Adaptive MCMC</a></li>
<li class="chapter" data-level="9.1.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#comparing-markov-chains"><i class="fa fa-check"></i><b>9.1.4</b> Comparing Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-scaling-of-random-walk-metropolis"><i class="fa fa-check"></i><b>9.2</b> Optimal Scaling of Random-Walk Metropolis</a><ul>
<li class="chapter" data-level="9.2.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#basic-principle"><i class="fa fa-check"></i><b>9.2.1</b> Basic principle</a></li>
<li class="chapter" data-level="9.2.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-acceptance-rate-as-drightarrow-infty"><i class="fa fa-check"></i><b>9.2.2</b> Optimal Acceptance Rate as <span class="math inline">\(d\rightarrow \infty\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#inhomogeneous-target-distributions"><i class="fa fa-check"></i><b>9.2.3</b> Inhomogeneous Target Distributions</a></li>
<li class="chapter" data-level="9.2.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#metropolis-adjusted-langevin-algorithm."><i class="fa fa-check"></i><b>9.2.4</b> Metropolis-Adjusted Langevin Algorithm.</a></li>
<li class="chapter" data-level="9.2.5" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#numerical-examples"><i class="fa fa-check"></i><b>9.2.5</b> Numerical Examples</a></li>
<li class="chapter" data-level="9.2.6" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#inhomogeneous-covariance"><i class="fa fa-check"></i><b>9.2.6</b> Inhomogeneous Covariance</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-mcmc-1"><i class="fa fa-check"></i><b>9.3</b> Adaptive MCMC</a></li>
<li class="chapter" data-level="9.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#ergodicity-of-adaptive-mcmc"><i class="fa fa-check"></i><b>9.4</b> Ergodicity of Adaptive MCMC</a><ul>
<li class="chapter" data-level="9.4.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-metropolis"><i class="fa fa-check"></i><b>9.4.1</b> Adaptive Metropolis</a></li>
<li class="chapter" data-level="9.4.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-metropolis-within-gibbs"><i class="fa fa-check"></i><b>9.4.2</b> Adaptive Metropolis-within-Gibbs</a></li>
<li class="chapter" data-level="9.4.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#state-dependent-proposal-scalings"><i class="fa fa-check"></i><b>9.4.3</b> State-Dependent Proposal Scalings</a></li>
<li class="chapter" data-level="9.4.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#limit-theorem"><i class="fa fa-check"></i><b>9.4.4</b> Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#faq"><i class="fa fa-check"></i><b>9.5</b> FAQ</a></li>
<li class="chapter" data-level="9.6" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#conclusion"><i class="fa fa-check"></i><b>9.6</b> Conclusion</a></li>
<li class="chapter" data-level="9.7" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#a-tutorial-on-adaptive-mcmc"><i class="fa fa-check"></i><b>9.7</b> A tutorial on adaptive MCMC</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>10</b> Hamiltonian Monte Carlo</a><ul>
<li class="chapter" data-level="10.0.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#properties-of-hamiltonian-dynamics"><i class="fa fa-check"></i><b>10.0.1</b> Properties of Hamiltonian Dynamics</a></li>
<li class="chapter" data-level="10.0.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#conservation-of-the-hamiltonian"><i class="fa fa-check"></i><b>10.0.2</b> Conservation of the Hamiltonian</a></li>
<li class="chapter" data-level="10.0.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#volume-preservation"><i class="fa fa-check"></i><b>10.0.3</b> Volume preservation</a></li>
<li class="chapter" data-level="10.0.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#symplecticness-"><i class="fa fa-check"></i><b>10.0.4</b> Symplecticness (辛？)</a></li>
<li class="chapter" data-level="10.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#discretizing-hamiltons-equations-the-leapfrog-method."><i class="fa fa-check"></i><b>10.1</b> Discretizing Hamilton’s Equations: The leapfrog method.</a><ul>
<li class="chapter" data-level="10.1.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#modification-of-eulers-method"><i class="fa fa-check"></i><b>10.1.1</b> Modification of Euler’s Method</a></li>
<li class="chapter" data-level="10.1.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-leapfrog-method"><i class="fa fa-check"></i><b>10.1.2</b> The leapfrog Method</a></li>
<li class="chapter" data-level="10.1.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#local-and-global-error-of-discretization-methods."><i class="fa fa-check"></i><b>10.1.3</b> Local and Global Error of discretization Methods.</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#mcmc-from-hamiltonian-dynamics."><i class="fa fa-check"></i><b>10.2</b> MCMC from Hamiltonian Dynamics.</a><ul>
<li class="chapter" data-level="10.2.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#probability-and-the-hamiltonian-canonical-distributions"><i class="fa fa-check"></i><b>10.2.1</b> Probability and the Hamiltonian: Canonical Distributions</a></li>
<li class="chapter" data-level="10.2.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-hamiltonian-monte-carlo-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> The Hamiltonian Monte Carlo Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#illustrations-of-hmc-and-its-benefits"><i class="fa fa-check"></i><b>10.2.3</b> Illustrations of HMC and Its Benefits</a></li>
<li class="chapter" data-level="10.2.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-benefit-of-avoiding-random-walks"><i class="fa fa-check"></i><b>10.2.4</b> The benefit of avoiding random walks</a></li>
<li class="chapter" data-level="10.2.5" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#sampling-from-a-100-dimensional-distribution"><i class="fa fa-check"></i><b>10.2.5</b> Sampling from a 100-Dimensional Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#hmc-in-practice-and-theory"><i class="fa fa-check"></i><b>10.3</b> HMC in Practice and Theory</a><ul>
<li class="chapter" data-level="10.3.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#effect-of-linear-transformation"><i class="fa fa-check"></i><b>10.3.1</b> Effect of Linear Transformation</a></li>
<li class="chapter" data-level="10.3.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#tuning-hmc"><i class="fa fa-check"></i><b>10.3.2</b> Tuning HMC</a></li>
<li class="chapter" data-level="10.3.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#combining-hmc-with-other-mcmc-updates"><i class="fa fa-check"></i><b>10.3.3</b> Combining HMC with Other MCMC Updates</a></li>
<li class="chapter" data-level="10.3.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#scaling-with-dimensionality"><i class="fa fa-check"></i><b>10.3.4</b> Scaling with Dimensionality</a></li>
<li class="chapter" data-level="10.3.5" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#hmc-for-hierarchical-models"><i class="fa fa-check"></i><b>10.3.5</b> HMC for Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#extensions-of-and-variations-on-hmc"><i class="fa fa-check"></i><b>10.4</b> Extensions of and Variations on HMC</a><ul>
<li class="chapter" data-level="10.4.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#discretization-by-splitting-handling-constraints-and-other-applications"><i class="fa fa-check"></i><b>10.4.1</b> Discretization by Splitting: Handling constraints and Other Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html"><i class="fa fa-check"></i><b>11</b> Bayes variable selection</a><ul>
<li class="chapter" data-level="11.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#prior-specification"><i class="fa fa-check"></i><b>11.1</b> Prior Specification</a></li>
<li class="chapter" data-level="11.2" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#summaries-the-posterior-distribution-and-model-averaged-inference"><i class="fa fa-check"></i><b>11.2</b> Summaries the posterior distribution and model averaged inference</a></li>
<li class="chapter" data-level="11.3" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#numerical-methods"><i class="fa fa-check"></i><b>11.3</b> Numerical Methods</a><ul>
<li class="chapter" data-level="11.3.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>11.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="advanced-r.html"><a href="advanced-r.html"><i class="fa fa-check"></i><b>12</b> Advanced R</a><ul>
<li class="chapter" data-level="12.0.1" data-path="advanced-r.html"><a href="advanced-r.html#vector"><i class="fa fa-check"></i><b>12.0.1</b> Vector</a></li>
<li class="chapter" data-level="12.0.2" data-path="advanced-r.html"><a href="advanced-r.html#types-and-tests"><i class="fa fa-check"></i><b>12.0.2</b> Types and tests:</a></li>
<li class="chapter" data-level="12.0.3" data-path="advanced-r.html"><a href="advanced-r.html#coercion"><i class="fa fa-check"></i><b>12.0.3</b> Coercion</a></li>
<li class="chapter" data-level="12.1" data-path="advanced-r.html"><a href="advanced-r.html#data.frame"><i class="fa fa-check"></i><b>12.1</b> Data.frame</a><ul>
<li class="chapter" data-level="12.1.1" data-path="advanced-r.html"><a href="advanced-r.html#ordering-integer-subsetting"><i class="fa fa-check"></i><b>12.1.1</b> Ordering (integer subsetting)</a></li>
<li class="chapter" data-level="12.1.2" data-path="advanced-r.html"><a href="advanced-r.html#calling-a-function-given-a-list-of-arguments"><i class="fa fa-check"></i><b>12.1.2</b> Calling a function given a list of arguments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="numeric-derivatives.html"><a href="numeric-derivatives.html"><i class="fa fa-check"></i><b>13</b> Numeric Derivatives</a></li>
<li class="chapter" data-level="14" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>14</b> Imputation</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes-variable-selection" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Bayes variable selection</h1>
<p>The model uncertainty problem with the <span class="math inline">\(2^p\)</span> competing models:</p>
<p><span class="math display">\[
M_{\gamma} : \boldsymbol{y}=\alpha \mathbf{1}_{n}+\boldsymbol{X}_{\gamma} \boldsymbol{\beta}_{\gamma}+\boldsymbol{\varepsilon}
\]</span></p>
<p>The Null model:</p>
<p><span class="math display">\[
M_{0} : y=\alpha 1_{n}+\varepsilon
\]</span></p>
<p>Assuming that one of the models in <span class="math inline">\(\mathcal M\)</span> is the true model, the posterior probability of any model is
<span class="math display">\[
\operatorname{Pr}\left(M_{\gamma^{*}} | \boldsymbol{y}\right)=\frac{m_{\gamma *}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma^{*}}\right)}{\sum_{\gamma} m_{\gamma}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma}\right)}
\]</span>
where <span class="math inline">\(Pr(M_\gamma)\)</span> is the prior probability of <span class="math inline">\(M_\gamma\)</span> and <span class="math inline">\(m_\gamma\)</span> is the integrated likelihood with respect to the prior <span class="math inline">\(\pi_\gamma\)</span>:
<span class="math display">\[
m_{\gamma}(\boldsymbol{y})=\int f_{\gamma}\left(\boldsymbol{y} | \boldsymbol{\beta}_{\gamma}, \alpha, \sigma\right) \pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma}, \alpha, \sigma^{2}\right) d \boldsymbol{\beta}_{\gamma} d \alpha d \sigma^{2}
\]</span>
也就是把所有不确定因素积掉以后，y的分布函数,also called the (prior) marginal likelihood.</p>
<p>For <span class="math inline">\(\gamma=0\)</span> this integrated lkelihood becomes:</p>
<p><span class="math display">\[
m_{0}(\boldsymbol{y})=\int f_{0}(\boldsymbol{y} | \alpha, \sigma) \pi_{0}\left(\alpha, \sigma^{2}\right) d \alpha d \sigma^{2}
\]</span></p>
<p>可以用Bayes factor替换 integrated likelihood function:</p>
<p><span class="math display">\[
\operatorname{Pr}\left(M_{\gamma^{*}} | \boldsymbol{y}\right)=\frac{B_{\gamma^{*}}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma^{*}}\right)}{\sum_{\gamma} B_{\gamma}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma}\right)}
\]</span></p>
<p>As stated in the introduction, we are mainly interested in software that implements the formal Bayesian answer which implies that we use the posterior distribution.</p>
<p>Due to the following three aspects</p>
<ul>
<li><p>The priors that the package accomodates, that is, <span class="math inline">\(\pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma}, \alpha, \sigma^{2}\right)\)</span> and <span class="math inline">\(Pr(M_\gamma)\)</span></p></li>
<li><p>the tools provided to summarize the posterior distribution and obtain model averaged inference</p></li>
<li><p>the numerical methods implemented to compute the posterior distribution</p></li>
</ul>
<div id="prior-specification" class="section level2">
<h2><span class="header-section-number">11.1</span> Prior Specification</h2>
<p>The two inputs that are needed to obtain the posterior distribution are <span class="math inline">\(\pi_\gamma\)</span> and <span class="math inline">\(Pr(M_r)\)</span> the <span class="math inline">\(2^p\)</span> prior distributions for the parameters within each model and the prior distribution over the model space, respectively.</p>
<p>不失一般性，先验分布<span class="math inline">\(\pi_\gamma\)</span> 可以写成
<span class="math display">\[
\pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma}, \alpha, \sigma^{2}\right)=\pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma} | \alpha, \sigma^{2}\right) \pi_{\gamma}\left(\alpha, \sigma^{2}\right)
\]</span>
，也就是coefficient，intercept，和variance的先验，
基于最方便的方法，基础Jeffreys’ prior is used for the parameters that are common to all models:</p>
<p><span class="math display">\[
\pi_{\gamma}\left(\alpha, \sigma^{2}\right)=\sigma^{-2}
\]</span>
对于<span class="math inline">\(\beta\)</span>,则要么使用正态，要么使用mixture正态，中心点在0. (“by reasons of similarity”,Jeffreys,1961) and scaled by <span class="math inline">\(\sigma^{2}\left(\boldsymbol{X}_{\gamma}^{t} \boldsymbol{X}_{\gamma}\right)^{-1}\)</span>, “a matrix suggested by the form of the information matrix.” times a factor g, normally called a “g-prior”. 目前的研究表明这样的方便的prior拥有一系列的最优的性质可以扩展到对超参数g做特殊的先验。 Among these properties are invariance under affine transformations of the covariates, several types of predictive matching and consistency ( Bayarri et al., 2002).</p>
<p>The specification of g has inspired many interesting studies in the literature. Of these, we have collected the most popular one in Table1.</p>
<p>Relatedd with the conventional priors, which inspired by asymptotically reproducing the popular Bayesian Information Criterion (Schwarz,1978). Raftery proposes using the same covariance matrix as the Unit Information Prior, but with mean the maximum likelihood estimator <span class="math inline">\(\hat\beta_\gamma\)</span> (instead of the zero mean of the conventional prior).</p>
<p>Other priors specifically used in model uncertainty problems are the <em>spike and slab priors.</em>
Assume that the components of <span class="math inline">\(\beta\)</span> are independent, each having a mixture of two distributions: one highly concentrated on zero (the spike) and the other one quite disperse (the slab). There are two different developments of this idea in the literature.</p>
<p>There are two different developments of this idea. Original version is Mitchell and Beauchamp (1988), the spike is a degenerate distribution at zero so this fits with what we have called the formal approach.</p>
<p>Another proposal by George and McCulloch (1993) which the spike is a continuous distribution with a small variance also received a lot of attention, perhaps for computational advantages.</p>
<p>模型空间的prior,<span class="math inline">\(\mathcal M\)</span>, 一个非常受欢迎的起点是</p>
<p><span class="math display">\[
Pr(M_\gamma|\theta)=\theta^{p_\gamma}(1-\theta)^{p-p_\gamma}
\]</span>
where <span class="math inline">\(p_\gamma\)</span> is the number of covariates in <span class="math inline">\(M_\gamma\)</span> and the hyperparameter <span class="math inline">\(\theta\in(0,1)\)</span> has the interpretation of the common probability that a given variable is included (independently of all others).</p>
<p>Among the most popular default choice for <span class="math inline">\(\theta\)</span> are</p>
<ul>
<li><p>Fixed <span class="math inline">\(\theta=1/2\)</span>, which assign equal prior probability to each model i.e. <span class="math inline">\(\operatorname{Pr}\left(M_{\gamma}\right)=1 / 2^{p}\)</span></p></li>
<li><p>Random <span class="math inline">\(\theta \sim \operatorname{Unif}(0,1)\)</span>, giving euqal probability to each possible number of covariates or model size.</p></li>
</ul>
<p>一般来说，固定的<span class="math inline">\(\theta\)</span>会在多样性上表现非常差，在测试中，伪造的解释变量经常在结果中出现，然后lead to 更有信息的先验。这套个情况可以用随机的<span class="math inline">\(\theta\)</span>进行避免，第二个proposal 见Scott and Berger (2010.) Lay and Steel(2009) 考虑使用<span class="math inline">\(\theta\sim Beta(1,b)\)</span> 可以导出binomial-beta 先验 for the number of covariates in the model or the model size,W:
<span class="math display">\[
\operatorname{Pr}(W=w | b) \propto \left( \begin{array}{c}{p} \\ {w}\end{array}\right) \Gamma(1+w) \Gamma(b+p-w), w=0,1, \ldots, p
\]</span>
注意<span class="math inline">\(b=1\)</span> 时退化到uniform prior on <span class="math inline">\(\theta\)</span> and also on <span class="math inline">\(W\)</span>. Ley and Steel (2009), this setting is useful to incorporate prior information about the mean model size, say <span class="math inline">\(w^*\)</span>. This would translate into <span class="math inline">\(b=(p-w^*)/w^*\)</span>.</p>
</div>
<div id="summaries-the-posterior-distribution-and-model-averaged-inference" class="section level2">
<h2><span class="header-section-number">11.2</span> Summaries the posterior distribution and model averaged inference</h2>
<p>The simplest summary of the posterior model distribution is its mode</p>
<p><span class="math display">\[
\underset{\gamma}{\arg \max } \operatorname{Pr}\left(M_{\gamma} | \boldsymbol{y}\right)
\]</span></p>
<p>This model is the model most supported by the information (data and prior)</p>
<p>This is normally called <em>HPM</em>(jighest posterior model) or MAP (maximum a posteriori) model.</p>
<p>When p is moderate to large, posterior inclusion probabilities (PIP) are very useful.</p>
<p><span class="math display">\[
\operatorname{Pr}\left(\gamma_{i}=1 | \boldsymbol{y}\right)=\sum_{x_{i} \in M_{\gamma}} \operatorname{Pr}\left(M_{\gamma} | \boldsymbol{y}\right)
\]</span>
这个可以理解成每个变量解释response的重要性。</p>
<p>这个概率也可以用于定义另外一个summary，叫median probability model(MPM) which is the model containing the covariates with inclusion probability larger than 0.5. This model in some case is optimal for prediction.</p>
</div>
<div id="numerical-methods" class="section level2">
<h2><span class="header-section-number">11.3</span> Numerical Methods</h2>
<hr />
<p>Bayes Lasso:</p>
<p>目前已知问题和需要解决的问题：</p>
<p>预计写作时间：19分钟</p>
<p>首先，解决了一半的问题：<span class="math inline">\(\beta\)</span> 的先验问题。</p>
<p>由<a href="%5Bhttp://www.math.chalmers.se/Stat/Grundutb/GU/MSA220/S16/bayeslasso.pdf%5D(http://www.math.chalmers.se/Stat/Grundutb/GU/MSA220/S16/bayeslasso.pdf)">Bayes Lasso的完整版notes(相对于论文)</a> 其中，Hierachical model部分，<span class="math inline">\(\beta|\tau_1^2,…,\tau^2_p\sim N_p(0_p,D_\tau)\)</span> 是成立的(根据原始式子推出来的)，但是和论文suggest的hierarchical representation of the full model:
<span class="math display">\[
\begin{aligned} \boldsymbol{y} | \mu, \boldsymbol{X}, \boldsymbol{\beta}, \sigma^{2} &amp; \sim N_{n}\left(\mu \mathbf{1}_{n}+\boldsymbol{X} \boldsymbol{\beta}, \sigma^{2} \boldsymbol{I}_{n}\right) \\ \boldsymbol{\beta} | \tau_{1}^{2}, \ldots, \tau_{p}^{2}, \sigma^{2} &amp; \sim N_{p}\left(\mathbf{0}_{p}, \sigma^{2} \boldsymbol{D}_{\tau}\right), \qquad \boldsymbol{D}_{\tau}=\operatorname{diag}\left(\tau_{1}^{2}, \ldots, \tau_{p}^{2}\right) \\ \tau_{1}^{2}, \ldots, \tau_{p}^{2} &amp; \sim \prod_{j=1}^{p} \frac{\lambda^{2}}{2} e^{-\lambda^{2} \tau_{j}^{2} / 2} d \tau_{j}^{2}, \quad \tau_{1}^{2}, \ldots, \tau_{p}^{2}&gt;0 \\ \sigma^{2} &amp; \sim \pi\left(\sigma^{2}\right) d \sigma^{2} \end{aligned}
\]</span>
有细微的差别：差了一个<span class="math inline">\(\sigma^2\)</span>. 在该notes中的描述则是，第一个形式是成立的，但是会有问题： “possibility of a non-unimodal posterior”,在Section 4 unimodal 问题里有详细描述，然后对<span class="math inline">\(\tau^2_1,…,\tau^2_p\)</span> 有其他先验的话会形成其他和Lasso有关的方法，在Section 6.</p>
<hr />
<blockquote>
<p>为什么会有不含<span class="math inline">\(\sigma^2\)</span> 的形式呢？主要是因为来源于最初的方程式</p>
</blockquote>
<p><span class="math display">\[
\frac{a}{2} e^{-a|z|}=\int_{0}^{\infty} \frac{1}{\sqrt{2 \pi s}} e^{-z^{2} /(2 s)} \frac{a^{2}}{2} e^{-a^{2} s / 2} d s, \quad a&gt;0
\]</span></p>
<p>如果把Laplace先验写成<span class="math inline">\(\pi(\boldsymbol{\beta})=\prod_{j=1}^{p} \frac{\lambda}{2} e^{-\lambda\left|\beta_{j}\right|}\)</span> 的话，那么<span class="math inline">\(a\)</span> 就是 <span class="math inline">\(\lambda\)</span> , <span class="math inline">\(\beta\)</span> 的密度很自然的与<span class="math inline">\(\sigma\)</span> 无关，是<span class="math inline">\(N(0,\tau_j^2)\)</span></p>
<p>如果把Laplace先验写成<span class="math inline">\(\pi\left(\boldsymbol{\beta} | \sigma^{2}\right)=\prod_{j=1}^{p} \frac{\lambda}{2 \sqrt{\sigma^{2}}} e^{-\lambda\left|\beta_{j}\right| / \sqrt{\sigma^{2}}}\)</span> 的话，那么a就是<span class="math inline">\(\frac{\lambda}{\sqrt{\sigma^2}}\)</span> ，则 <span class="math inline">\(\tau_j\)</span> 的分布会带上 <span class="math inline">\(\sigma^2\)</span> , 因为，这两种情况都和Full model对不上号。。。。</p>
<hr />
<blockquote>
<p>假设以上先不管，full mode成立，那么就可以写出来joint density</p>
</blockquote>
<p><span class="math display">\[
\begin{array}{l}{f\left(\boldsymbol{y} | \mu, \boldsymbol{\beta}, \sigma^{2}\right) \pi\left(\sigma^{2}\right) \pi(\mu) \prod_{j=1}^{p} \pi\left(\beta_{j} | \tau_{j}^{2}, \sigma^{2}\right) \pi\left(\tau_{j}^{2}\right)=} \\ {\qquad \frac{1}{\left(2 \pi \sigma^{2}\right)^{n / 2}} e^{-\frac{1}{2 \sigma^{2}}\left(\boldsymbol{y}-\mu \mathbf{1}_{n}-\boldsymbol{X} \boldsymbol{\beta}\right)^{\top}\left(\boldsymbol{y}-\mu \mathbf{1}_{n}-\boldsymbol{X} \boldsymbol{\beta}\right)}} \\ { \times \frac{\gamma^{a}}{\Gamma(a)}\left(\sigma^{2}\right)^{-a-1} e^{-\gamma / \sigma^{2}} \prod_{j=1}^{p} \frac{1}{\left(2 \pi \sigma^{2} \tau_{j}^{2}\right)^{1 / 2}} e^{-\frac{1}{2 \sigma^{2} \tau_{j}^{2}} \beta_{j}^{2}} \frac{\lambda^{2}}{2} e^{-\lambda^{2} \tau_{j}^{2} / 2}}\end{array}
\]</span></p>
<p>这个很清晰，第一块是<span class="math inline">\(\boldsymbol y\)</span> 的Normal，第二块是<span class="math inline">\(\sigma^2\)</span> 的inverse-gamma，第三块是<span class="math inline">\(\beta_j\)</span> 的Normal hierachical prior with 超参数<span class="math inline">\(\tau_j\)</span> 服从$f()= e<sup>{-</sup>{2} _{j}^{2} / 2} $.</p>
<blockquote>
<p>19分钟写到这，没写完，再来19分钟</p>
</blockquote>
<p>然后就是用<span class="math inline">\(\overline y\)</span> 作为<span class="math inline">\(y\)</span> 的均值，重新写一下Normal里面关于Y和<span class="math inline">\(\mu\)</span> 的二次型如下：
<span class="math display">\[
\begin{aligned}\left(\boldsymbol{y}-\mu \mathbf{1}_{n}-\boldsymbol{X} \boldsymbol{\beta}\right)^{\top}\left(\boldsymbol{y}-\mu \mathbf{1}_{n}-\boldsymbol{X} \boldsymbol{\beta}\right) &amp;=\left(\overline{y} \mathbf{1}_{n}-\mu \mathbf{1}_{n}\right)^{\top}\left(\overline{y} \mathbf{1}_{n}-\mu \mathbf{1}_{n}\right)+(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\top}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta}) \\ &amp;=n(\overline{y}-\boldsymbol{\mu})^{2}+(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\top}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta}) \end{aligned}
\]</span>
所以就把关于<span class="math inline">\(\mu\)</span>的部分和关于<span class="math inline">\(y,X,\beta\)</span> 的部分分开了，可以看到，关于<span class="math inline">\(\mu\)</span>的部分还是一个二次型，配合前面的系数，可以凑成一个Normal的形式，有均值<span class="math inline">\(\overline y\)</span> 和方差<span class="math inline">\(\sigma^2/n\)</span>，然后把<span class="math inline">\(\mu\)</span>积掉，因为是正态，还分开了，所以积分就是1，相当于把这项拿走不影响其他项，得到一个新的Marginal only over <span class="math inline">\(\mu\)</span> 的joint density, which is proportional to
<span class="math display">\[
\frac{1}{\left(\sigma^{2}\right)^{(n-1) / 2}} e^{-\frac{1}{2 \sigma^{2}}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\top}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})}\left(\sigma^{2}\right)^{-a-1} e^{-\gamma / \sigma^{2}} \prod_{j=1}^{p} \frac{1}{\left(\sigma^{2} \tau_{j}^{2}\right)^{1 / 2}} e^{-\frac{1}{2 \sigma^{2} \tau_{j}^{2}} \beta_{j}^{2}} e^{-\lambda^{2} \tau_{j}^{2} / 2}
\]</span>
This expression depends on <span class="math inline">\(\boldsymbol y\)</span> only through <span class="math inline">\(\tilde{\boldsymbol{y}}\)</span> . The conjugacy of the other parameters remains unaffected.</p>
<hr />
<blockquote>
<p>然后就能构建Gibbs</p>
</blockquote>
<ul>
<li><span class="math inline">\(\beta\)</span></li>
</ul>
<p><span class="math display">\[
-\frac{1}{2 \sigma^{2}}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\top}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})-\frac{1}{2 \sigma^{2}} \boldsymbol{\beta}^{\top} \boldsymbol{D}_{\tau}^{-1} \boldsymbol{\beta}=-\frac{1}{2 \sigma^{2}}\left\{\boldsymbol{\beta}^{\top}\left(\boldsymbol{X}^{\top} \boldsymbol{X}+\boldsymbol{D}_{\tau}^{-1}\right) \boldsymbol{\beta}-2 \tilde{\boldsymbol{y}}^{\top} \boldsymbol{X} \boldsymbol{\beta}+\tilde{\boldsymbol{y}}^{\top} \tilde{\boldsymbol{y}}\right\}
\]</span></p>
<p><span class="math inline">\(\beta\)</span> involve part is quadratic form with a linear form, which is proportional to Normal density.</p>
<p>Letting <span class="math inline">\(\boldsymbol{A}=\boldsymbol{X}^{\top} \boldsymbol{X}+\boldsymbol{D}_{\tau}^{-1}\)</span> , Then the equation upon can be transformed to
<span class="math display">\[
\boldsymbol{\beta}^{\top} \boldsymbol{A} \boldsymbol{\beta}-2 \tilde{\boldsymbol{y}}^{\top} \boldsymbol{X} \boldsymbol{\beta}+\tilde{\boldsymbol{y}}^{\top} \tilde{\boldsymbol{y}}=\left(\boldsymbol{\beta}-\boldsymbol{A}^{-1} \boldsymbol{X}^{\top} \tilde{\boldsymbol{y}}\right)^{\top} \boldsymbol{A}\left(\boldsymbol{\beta}-\boldsymbol{A}^{-1} \boldsymbol{X}^{\top} \tilde{\boldsymbol{y}}\right)+\tilde{\boldsymbol{y}}^{\mathrm{T}}\left(\boldsymbol{I}_{n}-\boldsymbol{X} \boldsymbol{A}^{-1} \boldsymbol{X}^{\top}\right) \tilde{\boldsymbol{y}}
\]</span>
That is, <span class="math inline">\(\boldsymbol \beta\)</span> is conditionally multivariate normal with mean <span class="math inline">\(A^{-1} \boldsymbol{X}^{\top} \tilde{\boldsymbol{y}}\)</span> and variance <span class="math inline">\(\sigma^{2} \boldsymbol{A}^{-1}\)</span>.</p>
<p>这块应该可以照常用，只要把<span class="math inline">\(\sigma^2\)</span> 那块的东西处理掉，</p>
<ul>
<li><span class="math inline">\(\sigma^2\)</span></li>
</ul>
<p>因为要建模，所以这块应该全部要换成Normal和建模的形式
<span class="math display">\[
\left(\sigma^{2}\right)^{-(n-1) / 2-p / 2-a-1} \exp \left\{-\frac{1}{\sigma^{2}}\left((\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\mathrm{T}}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta}) / 2+\boldsymbol{\beta}^{\top} \boldsymbol{D}_{\tau}^{-1} \boldsymbol{\beta} / 2+\gamma\right)\right\}
\]</span>
这个是conditionally inverse gamma的形式，with shape <span class="math inline">\((n-1) / 2+p / 2+a\)</span> and scale parameter <span class="math inline">\((\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\mathrm{T}}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta}) / 2+\boldsymbol{\beta}^{\top} \boldsymbol{D}_{\tau}^{-1} \boldsymbol{\beta} / 2+\gamma\)</span> .</p>
<ul>
<li><span class="math inline">\(\tau_j^2\)</span> for <span class="math inline">\(j=1,…,p\)</span></li>
</ul>
<p>Density is:
<span class="math display">\[
\left(\tau_{j}^{2}\right)^{-1 / 2} \exp \left\{-\frac{1}{2}\left(\frac{\beta_{j}^{2} / \sigma^{2}}{\tau_{j}^{2}}+\lambda^{2} \tau_{j}^{2}\right)\right\}
\]</span>
Proportional to the density of the reciprocal of an inverse Gaussian random variable.</p>
<hr />
<blockquote>
<p>第四节，等等，怎么变成日常笔记了， The posterior distribution</p>
</blockquote>
<p>The joint posterior distribution of $$ and <span class="math inline">\(\sigma^2\)</span> is proportional to
<span class="math display">\[
\left(\sigma^{2}\right)^{-(n+p-1) / 2-a-1} \exp \left\{-\frac{1}{\sigma^{2}}\left((\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\top}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta}) / 2+\gamma\right)-\frac{\lambda}{\sqrt{\sigma^{2}}} \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}
\]</span>
从这里面来看，至少很像最小二乘+罚函数的形式。</p>
<p>We may safety let <span class="math inline">\(a=0\)</span> .</p>
<blockquote>
<p>这句话没太懂：</p>
</blockquote>
<p>Assuming that the data do not admit a perfect linear fit (i.e. <span class="math inline">\(\tilde y\)</span> is not in the column space of <span class="math inline">\(\boldsymbol X\)</span> ), also let <span class="math inline">\(\gamma=0\)</span> .This corresponds to using the non-informative scale-invariant prior <span class="math inline">\(1/\sigma^2\)</span> on <span class="math inline">\(\sigma^2\)</span> .</p>
<p><span class="math inline">\(\boldsymbol X\)</span> matrix is, of course, unitless because of its scaling.</p>
<p>For comparison, the joint posterior distribution of $$ and <span class="math inline">\(\sigma^2\)</span> under prior (1), 也就是不涉及<span class="math inline">\(\sigma^2\)</span> 的prior，with some independent prior <span class="math inline">\(\pi(\sigma^2)\)</span> on <span class="math inline">\(\sigma^2\)</span>, is proportional to
<span class="math display">\[
\pi\left(\sigma^{2}\right)\left(\sigma^{2}\right)^{-(n-1) / 2} \exp \left\{-\frac{1}{2 \sigma^{2}}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})^{\mathrm{T}}(\tilde{\boldsymbol{y}}-\boldsymbol{X} \boldsymbol{\beta})-\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}
\]</span></p>
<blockquote>
<p>所以果然是为了合并同类项吗</p>
</blockquote>
<p>In this case, <span class="math inline">\(\lambda\)</span> has units that are the reciprocal of the units of the response, and any change in units will require a corresponding change in <span class="math inline">\(\lambda\)</span> to produce the equivalent Bayesian solution.</p>
<p>这段话应该很重要，关于unit change的，类似于给<span class="math inline">\(\beta\)</span> 加了一个 rescaling ? 标记一下，这里值得重点研究一下:</p>
<blockquote>
<p>这个unit，和scaling究竟是什么情况？如果要挪用的话得用什么？因为毕竟没有我的模型里面没有<span class="math inline">\(\sigma^2\)</span>，能不能不管这个问题，得付出什么代价？要管的话应该用什么？</p>
</blockquote>
<p>然后第二个问题就是multi-modal的问题，下次来继续说。</p>
<blockquote>
<p>重新开始</p>
</blockquote>
<p>第二个模型可能会有多于一个极值点。 Taking <span class="math inline">\(p=1,n=10,X^TX=1,X^T\tilde y=5,\tilde y^Ty=26,\lambda=3\)</span>. The mode on the lower right is near the least-squares solution <span class="math inline">\(\beta=5,\sigma^2=1/8\)</span>, while the mode on the upper left is near the values <span class="math inline">\(\beta=0,\sigma^2=26/9\)</span> that would be estimated for the selected model in which <span class="math inline">\(\beta\)</span> is set to zero. The crease in the upper left mode along the line <span class="math inline">\(\beta=0\)</span> is a feature produced by the “sharp corners” of the <span class="math inline">\(L_1\)</span> penalty. Not surprisingly, the marginal density of <span class="math inline">\(\beta\)</span> is also bimodal. When <span class="math inline">\(p&gt;1\)</span>, it may be possible to have more than two modes, though we have not investigated this.</p>
<p>所以这两个峰不是莫名其妙的两个峰，是分别是OLS的结果和，不取该<span class="math inline">\(\beta\)</span> 的结果。</p>
<blockquote>
<p>所以好像直接用Bayes Lasso会出问题，得改，但是设计<span class="math inline">\(\sigma^2\)</span> 感觉会变得很麻烦。。。</p>
</blockquote>
<p>多个峰值会导致计算和概念上的问题，这种情况下，单点的后验均值，中位数，或者最大值点是否能总结一个双峰后验的要素是有疑问的。一个更好的summary将会是分别测量每个峰值点</p>
<blockquote>
<p>下一步是选取<span class="math inline">\(\lambda\)</span> ，original的lasso是通过cross-validation,generalised cross-validation, and ideas based on Stein’s unbiased risk estimate. Bayesian Lasso offers some uniquely Bayesian alternatives: empirical Bayes via marginal (Type 2) maximum likelihood, and use of an appropriate hyperprior. 这是啥，不懂得看看</p>
</blockquote>
<div id="empirical-bayes-by-marginal-maximum-likelihood" class="section level3">
<h3><span class="header-section-number">11.3.1</span> Empirical Bayes by Marginal Maximum Likelihood</h3>
<blockquote>
<p>Finish</p>
</blockquote>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="hamiltonian-monte-carlo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="advanced-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Notes in statistics and computing.pdf", "Notes in statistics and computing.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
