<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Reversible Jump MCMC | Notes on Statistics</title>
  <meta name="description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Reversible Jump MCMC | Notes on Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Reversible Jump MCMC | Notes on Statistics" />
  
  <meta name="twitter:description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  

<meta name="author" content="Jiaming Shen" />


<meta name="date" content="2021-08-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basicmcmc.html"/>
<link rel="next" href="optimal-proposal-distributions-and-adaptive-mcmc.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes in statistics and computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>preliminary</a></li>
<li class="chapter" data-level="1" data-path="-extra.html"><a href="-extra.html"><i class="fa fa-check"></i><b>1</b> 杂七杂八{# extra}</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html"><i class="fa fa-check"></i><b>3</b> Mathematical statistic Trick</a><ul>
<li class="chapter" data-level="3.1" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#NormalForm"><i class="fa fa-check"></i><b>3.1</b> Normal distribution as exponential family</a></li>
<li class="chapter" data-level="3.2" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#密度变换公式"><i class="fa fa-check"></i><b>3.2</b> 密度变换公式：</a></li>
<li class="chapter" data-level="3.3" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#probability-mass-function"><i class="fa fa-check"></i><b>3.3</b> Probability mass function:</a></li>
<li class="chapter" data-level="3.4" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#vector-to-diagonal-matrix"><i class="fa fa-check"></i><b>3.4</b> Vector to diagonal matrix:</a></li>
<li class="chapter" data-level="3.5" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#gaussian-integral-trick"><i class="fa fa-check"></i><b>3.5</b> Gaussian Integral Trick</a></li>
<li class="chapter" data-level="3.6" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#asymptotic-issues"><i class="fa fa-check"></i><b>3.6</b> Asymptotic issues</a></li>
<li class="chapter" data-level="3.7" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#matrix-inverse"><i class="fa fa-check"></i><b>3.7</b> Matrix inverse</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="general-notes.html"><a href="general-notes.html"><i class="fa fa-check"></i><b>4</b> General Notes</a><ul>
<li class="chapter" data-level="4.1" data-path="general-notes.html"><a href="general-notes.html#台湾教授彭明辉教授的研究生手册"><i class="fa fa-check"></i><b>4.1</b> 台湾教授彭明辉教授的研究生手册</a><ul>
<li class="chapter" data-level="4.1.1" data-path="general-notes.html"><a href="general-notes.html#读论文的要求"><i class="fa fa-check"></i><b>4.1.1</b> 读论文的要求</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="general-notes.html"><a href="general-notes.html#论文报告的要求与技巧"><i class="fa fa-check"></i><b>4.2</b> 论文报告的要求与技巧</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html"><i class="fa fa-check"></i><b>5</b> Statistician Tool Box</a><ul>
<li class="chapter" data-level="5.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#matrix"><i class="fa fa-check"></i><b>5.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="5.1.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#block-diagonal-matrices"><i class="fa fa-check"></i><b>5.1.1</b> Block diagonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#sumSquare"><i class="fa fa-check"></i><b>5.2</b> 两个二次型相加</a></li>
<li class="chapter" data-level="5.3" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#正定阵的基础定理"><i class="fa fa-check"></i><b>5.3</b> 正定阵的基础定理：</a></li>
<li class="chapter" data-level="5.4" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#对称阵的谱分解相关定理"><i class="fa fa-check"></i><b>5.4</b> 对称阵的谱分解相关定理</a></li>
<li class="chapter" data-level="5.5" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#covariance-structure"><i class="fa fa-check"></i><b>5.5</b> Covariance Structure</a><ul>
<li class="chapter" data-level="5.5.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#compound-symmetry-covariance结构"><i class="fa fa-check"></i><b>5.5.1</b> Compound Symmetry Covariance结构</a></li>
<li class="chapter" data-level="5.5.2" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#huynh-feldt-structure"><i class="fa fa-check"></i><b>5.5.2</b> Huynh-Feldt Structure</a></li>
<li class="chapter" data-level="5.5.3" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#the-one-dependent-covariance-structure"><i class="fa fa-check"></i><b>5.5.3</b> The One-Dependent Covariance Structure</a></li>
<li class="chapter" data-level="5.5.4" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#ar1-structure-highly-popular"><i class="fa fa-check"></i><b>5.5.4</b> AR(1) Structure (highly popular)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html"><i class="fa fa-check"></i><b>6</b> Longitudinal data analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#linear-mixed-model"><i class="fa fa-check"></i><b>6.1</b> Linear mixed model</a><ul>
<li class="chapter" data-level="6.1.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#condition-mean-vs-marginal-mean"><i class="fa fa-check"></i><b>6.1.1</b> Condition Mean vs Marginal mean</a></li>
<li class="chapter" data-level="6.1.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#restricted-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>6.1.2</b> Restricted maximum likelihood estimation</a></li>
<li class="chapter" data-level="6.1.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#prediction"><i class="fa fa-check"></i><b>6.1.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#generalised-linear-mixed-models"><i class="fa fa-check"></i><b>6.2</b> Generalised linear mixed models</a><ul>
<li class="chapter" data-level="6.2.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#expFam"><i class="fa fa-check"></i><b>6.2.1</b> Exponential distribution family</a></li>
<li class="chapter" data-level="6.2.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#iteratively-reweighted-least-square-algorithm-iwls"><i class="fa fa-check"></i><b>6.2.2</b> Iteratively reweighted Least square algorithm (IWLS)</a></li>
<li class="chapter" data-level="6.2.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#glmms"><i class="fa fa-check"></i><b>6.2.3</b> GLMMs</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#the-bayesian-analysis-approach-for-covariance-modelling"><i class="fa fa-check"></i><b>6.3</b> The Bayesian analysis approach for covariance modelling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="-.html"><a href="-.html"><i class="fa fa-check"></i><b>7</b> 统计图形笔记</a><ul>
<li class="chapter" data-level="7.1" data-path="-.html"><a href="-.html#图例"><i class="fa fa-check"></i><b>7.1</b> 图例：</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="basicmcmc.html"><a href="basicmcmc.html"><i class="fa fa-check"></i><b>8</b> BasicMCMC</a><ul>
<li class="chapter" data-level="8.1" data-path="basicmcmc.html"><a href="basicmcmc.html#metropolis-hastings-update"><i class="fa fa-check"></i><b>8.1</b> Metropolis-Hastings Update</a><ul>
<li class="chapter" data-level="8.1.1" data-path="basicmcmc.html"><a href="basicmcmc.html#metropolis-update"><i class="fa fa-check"></i><b>8.1.1</b> Metropolis Update</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="basicmcmc.html"><a href="basicmcmc.html#the-gibbs-update"><i class="fa fa-check"></i><b>8.2</b> The Gibbs Update</a><ul>
<li class="chapter" data-level="8.2.1" data-path="basicmcmc.html"><a href="basicmcmc.html#variable-at-a-time-metropolis-hastings"><i class="fa fa-check"></i><b>8.2.1</b> Variable-at-a-Time Metropolis-Hastings</a></li>
<li class="chapter" data-level="8.2.2" data-path="basicmcmc.html"><a href="basicmcmc.html#the-gibbs-is-a-special-case-of-metropolis-hastings"><i class="fa fa-check"></i><b>8.2.2</b> The Gibbs is a special case of Metropolis-Hastings:</a></li>
<li class="chapter" data-level="8.2.3" data-path="basicmcmc.html"><a href="basicmcmc.html#gibbs-full-conditional-distibution"><i class="fa fa-check"></i><b>8.2.3</b> Gibbs Full conditional distibution</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="basicmcmc.html"><a href="basicmcmc.html#combining-updates"><i class="fa fa-check"></i><b>8.3</b> Combining Updates</a><ul>
<li class="chapter" data-level="8.3.1" data-path="basicmcmc.html"><a href="basicmcmc.html#composition"><i class="fa fa-check"></i><b>8.3.1</b> Composition</a></li>
<li class="chapter" data-level="8.3.2" data-path="basicmcmc.html"><a href="basicmcmc.html#palindromic-composition"><i class="fa fa-check"></i><b>8.3.2</b> Palindromic Composition</a></li>
<li class="chapter" data-level="8.3.3" data-path="basicmcmc.html"><a href="basicmcmc.html#state-independent-mixing"><i class="fa fa-check"></i><b>8.3.3</b> State-Independent Mixing</a></li>
<li class="chapter" data-level="8.3.4" data-path="basicmcmc.html"><a href="basicmcmc.html#subsampling"><i class="fa fa-check"></i><b>8.3.4</b> Subsampling</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="basicmcmc.html"><a href="basicmcmc.html#a-metropolis-example"><i class="fa fa-check"></i><b>8.4</b> A Metropolis Example</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>9</b> Reversible Jump MCMC</a><ul>
<li class="chapter" data-level="9.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#from-metropolis-hastings-to-reversible-jump"><i class="fa fa-check"></i><b>9.1.1</b> From Metropolis-Hastings to Reversible Jump</a></li>
<li class="chapter" data-level="9.1.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#application-area"><i class="fa fa-check"></i><b>9.1.2</b> Application Area</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#implementation"><i class="fa fa-check"></i><b>9.2</b> Implementation</a><ul>
<li class="chapter" data-level="9.2.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#example-dimension-matching"><i class="fa fa-check"></i><b>9.2.1</b> Example Dimension Matching</a></li>
<li class="chapter" data-level="9.2.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#example-moment-matching-in-a-finite-mixture-of-univariate-normals"><i class="fa fa-check"></i><b>9.2.2</b> Example: Moment Matching in a Finite Mixture of Univariate Normals</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#mapping-functions-and-proposal-distribution"><i class="fa fa-check"></i><b>9.3</b> Mapping Functions and Proposal Distribution</a><ul>
<li class="chapter" data-level="9.3.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#marginalization-and-augmentation"><i class="fa fa-check"></i><b>9.3.1</b> Marginalization and augmentation:</a></li>
<li class="chapter" data-level="9.3.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#centering-and-order-methods"><i class="fa fa-check"></i><b>9.3.2</b> Centering and Order Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html"><i class="fa fa-check"></i><b>10</b> Optimal Proposal Distributions and Adaptive MCMC</a><ul>
<li class="chapter" data-level="10.1" data-path="intro.html"><a href="intro.html#intro"><i class="fa fa-check"></i><b>10.1</b> Intro</a><ul>
<li class="chapter" data-level="10.1.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#mh-algorithm"><i class="fa fa-check"></i><b>10.1.1</b> MH algorithm</a></li>
<li class="chapter" data-level="10.1.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-scaling"><i class="fa fa-check"></i><b>10.1.2</b> Optimal Scaling</a></li>
<li class="chapter" data-level="10.1.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-mcmc"><i class="fa fa-check"></i><b>10.1.3</b> Adaptive MCMC</a></li>
<li class="chapter" data-level="10.1.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#comparing-markov-chains"><i class="fa fa-check"></i><b>10.1.4</b> Comparing Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-scaling-of-random-walk-metropolis"><i class="fa fa-check"></i><b>10.2</b> Optimal Scaling of Random-Walk Metropolis</a><ul>
<li class="chapter" data-level="10.2.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#basic-principle"><i class="fa fa-check"></i><b>10.2.1</b> Basic principle</a></li>
<li class="chapter" data-level="10.2.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-acceptance-rate-as-drightarrow-infty"><i class="fa fa-check"></i><b>10.2.2</b> Optimal Acceptance Rate as <span class="math inline">\(d\rightarrow \infty\)</span></a></li>
<li class="chapter" data-level="10.2.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#inhomogeneous-target-distributions"><i class="fa fa-check"></i><b>10.2.3</b> Inhomogeneous Target Distributions</a></li>
<li class="chapter" data-level="10.2.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#metropolis-adjusted-langevin-algorithm."><i class="fa fa-check"></i><b>10.2.4</b> Metropolis-Adjusted Langevin Algorithm.</a></li>
<li class="chapter" data-level="10.2.5" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#numerical-examples"><i class="fa fa-check"></i><b>10.2.5</b> Numerical Examples</a></li>
<li class="chapter" data-level="10.2.6" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#inhomogeneous-covariance"><i class="fa fa-check"></i><b>10.2.6</b> Inhomogeneous Covariance</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-mcmc-1"><i class="fa fa-check"></i><b>10.3</b> Adaptive MCMC</a></li>
<li class="chapter" data-level="10.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#ergodicity-of-adaptive-mcmc"><i class="fa fa-check"></i><b>10.4</b> Ergodicity of Adaptive MCMC</a><ul>
<li class="chapter" data-level="10.4.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-metropolis"><i class="fa fa-check"></i><b>10.4.1</b> Adaptive Metropolis</a></li>
<li class="chapter" data-level="10.4.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-metropolis-within-gibbs"><i class="fa fa-check"></i><b>10.4.2</b> Adaptive Metropolis-within-Gibbs</a></li>
<li class="chapter" data-level="10.4.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#state-dependent-proposal-scalings"><i class="fa fa-check"></i><b>10.4.3</b> State-Dependent Proposal Scalings</a></li>
<li class="chapter" data-level="10.4.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#limit-theorem"><i class="fa fa-check"></i><b>10.4.4</b> Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#faq"><i class="fa fa-check"></i><b>10.5</b> FAQ</a></li>
<li class="chapter" data-level="10.6" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#conclusion"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
<li class="chapter" data-level="10.7" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#a-tutorial-on-adaptive-mcmc"><i class="fa fa-check"></i><b>10.7</b> A tutorial on adaptive MCMC</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>11</b> Hamiltonian Monte Carlo</a><ul>
<li class="chapter" data-level="11.0.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#properties-of-hamiltonian-dynamics"><i class="fa fa-check"></i><b>11.0.1</b> Properties of Hamiltonian Dynamics</a></li>
<li class="chapter" data-level="11.0.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#conservation-of-the-hamiltonian"><i class="fa fa-check"></i><b>11.0.2</b> Conservation of the Hamiltonian</a></li>
<li class="chapter" data-level="11.0.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#volume-preservation"><i class="fa fa-check"></i><b>11.0.3</b> Volume preservation</a></li>
<li class="chapter" data-level="11.0.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#symplecticness-辛"><i class="fa fa-check"></i><b>11.0.4</b> Symplecticness (辛？)</a></li>
<li class="chapter" data-level="11.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#discretizing-hamiltons-equations-the-leapfrog-method."><i class="fa fa-check"></i><b>11.1</b> Discretizing Hamilton’s Equations: The leapfrog method.</a><ul>
<li class="chapter" data-level="11.1.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#modification-of-eulers-method"><i class="fa fa-check"></i><b>11.1.1</b> Modification of Euler’s Method</a></li>
<li class="chapter" data-level="11.1.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-leapfrog-method"><i class="fa fa-check"></i><b>11.1.2</b> The leapfrog Method</a></li>
<li class="chapter" data-level="11.1.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#local-and-global-error-of-discretization-methods."><i class="fa fa-check"></i><b>11.1.3</b> Local and Global Error of discretization Methods.</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#mcmc-from-hamiltonian-dynamics."><i class="fa fa-check"></i><b>11.2</b> MCMC from Hamiltonian Dynamics.</a><ul>
<li class="chapter" data-level="11.2.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#probability-and-the-hamiltonian-canonical-distributions"><i class="fa fa-check"></i><b>11.2.1</b> Probability and the Hamiltonian: Canonical Distributions</a></li>
<li class="chapter" data-level="11.2.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-hamiltonian-monte-carlo-algorithm"><i class="fa fa-check"></i><b>11.2.2</b> The Hamiltonian Monte Carlo Algorithm</a></li>
<li class="chapter" data-level="11.2.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#illustrations-of-hmc-and-its-benefits"><i class="fa fa-check"></i><b>11.2.3</b> Illustrations of HMC and Its Benefits</a></li>
<li class="chapter" data-level="11.2.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-benefit-of-avoiding-random-walks"><i class="fa fa-check"></i><b>11.2.4</b> The benefit of avoiding random walks</a></li>
<li class="chapter" data-level="11.2.5" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#sampling-from-a-100-dimensional-distribution"><i class="fa fa-check"></i><b>11.2.5</b> Sampling from a 100-Dimensional Distribution</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#hmc-in-practice-and-theory"><i class="fa fa-check"></i><b>11.3</b> HMC in Practice and Theory</a><ul>
<li class="chapter" data-level="11.3.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#effect-of-linear-transformation"><i class="fa fa-check"></i><b>11.3.1</b> Effect of Linear Transformation</a></li>
<li class="chapter" data-level="11.3.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#tuning-hmc"><i class="fa fa-check"></i><b>11.3.2</b> Tuning HMC</a></li>
<li class="chapter" data-level="11.3.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#combining-hmc-with-other-mcmc-updates"><i class="fa fa-check"></i><b>11.3.3</b> Combining HMC with Other MCMC Updates</a></li>
<li class="chapter" data-level="11.3.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#scaling-with-dimensionality"><i class="fa fa-check"></i><b>11.3.4</b> Scaling with Dimensionality</a></li>
<li class="chapter" data-level="11.3.5" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#hmc-for-hierarchical-models"><i class="fa fa-check"></i><b>11.3.5</b> HMC for Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#extensions-of-and-variations-on-hmc"><i class="fa fa-check"></i><b>11.4</b> Extensions of and Variations on HMC</a><ul>
<li class="chapter" data-level="11.4.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#discretization-by-splitting-handling-constraints-and-other-applications"><i class="fa fa-check"></i><b>11.4.1</b> Discretization by Splitting: Handling constraints and Other Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html"><i class="fa fa-check"></i><b>12</b> Bayes variable selection</a><ul>
<li class="chapter" data-level="12.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#prior-specification"><i class="fa fa-check"></i><b>12.1</b> Prior Specification</a></li>
<li class="chapter" data-level="12.2" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#summaries-the-posterior-distribution-and-model-averaged-inference"><i class="fa fa-check"></i><b>12.2</b> Summaries the posterior distribution and model averaged inference</a></li>
<li class="chapter" data-level="12.3" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#numerical-methods"><i class="fa fa-check"></i><b>12.3</b> Numerical Methods</a><ul>
<li class="chapter" data-level="12.3.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>12.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#bayesian-asymptotically-analysis"><i class="fa fa-check"></i><b>12.4</b> Bayesian asymptotically analysis</a></li>
<li class="chapter" data-level="12.5" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#bayes-factor"><i class="fa fa-check"></i><b>12.5</b> Bayes factor</a><ul>
<li class="chapter" data-level="12.5.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#marginal-density-居然可以这么来使"><i class="fa fa-check"></i><b>12.5.1</b> Marginal density 居然可以这么来使</a></li>
<li class="chapter" data-level="12.5.2" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#几个需要可能研究的玩意"><i class="fa fa-check"></i><b>12.5.2</b> 几个需要可能研究的玩意</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="advanced-r.html"><a href="advanced-r.html"><i class="fa fa-check"></i><b>13</b> Advanced R</a><ul>
<li class="chapter" data-level="13.0.1" data-path="advanced-r.html"><a href="advanced-r.html#vector"><i class="fa fa-check"></i><b>13.0.1</b> Vector</a></li>
<li class="chapter" data-level="13.0.2" data-path="advanced-r.html"><a href="advanced-r.html#types-and-tests"><i class="fa fa-check"></i><b>13.0.2</b> Types and tests:</a></li>
<li class="chapter" data-level="13.0.3" data-path="advanced-r.html"><a href="advanced-r.html#coercion"><i class="fa fa-check"></i><b>13.0.3</b> Coercion</a></li>
<li class="chapter" data-level="13.1" data-path="advanced-r.html"><a href="advanced-r.html#data.frame"><i class="fa fa-check"></i><b>13.1</b> Data.frame</a><ul>
<li class="chapter" data-level="13.1.1" data-path="advanced-r.html"><a href="advanced-r.html#ordering-integer-subsetting"><i class="fa fa-check"></i><b>13.1.1</b> Ordering (integer subsetting)</a></li>
<li class="chapter" data-level="13.1.2" data-path="advanced-r.html"><a href="advanced-r.html#calling-a-function-given-a-list-of-arguments"><i class="fa fa-check"></i><b>13.1.2</b> Calling a function given a list of arguments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="numeric-derivatives.html"><a href="numeric-derivatives.html"><i class="fa fa-check"></i><b>14</b> Numeric Derivatives</a></li>
<li class="chapter" data-level="15" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>15</b> Imputation</a></li>
<li class="chapter" data-level="16" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html"><i class="fa fa-check"></i><b>16</b> Simulation approach for computing the marginal likelihood</a><ul>
<li class="chapter" data-level="16.1" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#laplace-metropolis-approximation"><i class="fa fa-check"></i><b>16.1</b> Laplace-Metropolis approximation</a></li>
<li class="chapter" data-level="16.2" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#laplace-metropolis-approximation-1"><i class="fa fa-check"></i><b>16.2</b> Laplace-Metropolis approximation</a></li>
<li class="chapter" data-level="16.3" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#chibs-estimator-from-gibbss-sampling"><i class="fa fa-check"></i><b>16.3</b> Chib’s estimator from Gibbs’s sampling</a></li>
<li class="chapter" data-level="16.4" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#example-seemingly-unrelated-regression-model-with-informative-prior."><i class="fa fa-check"></i><b>16.4</b> Example: Seemingly unrelated regression model with informative prior.</a></li>
<li class="chapter" data-level="16.5" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#bridge-sampling-methods"><i class="fa fa-check"></i><b>16.5</b> Bridge sampling methods</a></li>
<li class="chapter" data-level="16.6" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#the-savage-dickey-density-ratio-approach"><i class="fa fa-check"></i><b>16.6</b> The savage-Dickey density ratio approach</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="r-web-scrape.html"><a href="r-web-scrape.html"><i class="fa fa-check"></i><b>17</b> R web scrape</a></li>
<li class="chapter" data-level="18" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html"><i class="fa fa-check"></i><b>18</b> Guide to Scientific Computing in C++</a><ul>
<li class="chapter" data-level="18.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#basics"><i class="fa fa-check"></i><b>18.1</b> Basics</a></li>
<li class="chapter" data-level="18.2" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#basics-in-c"><i class="fa fa-check"></i><b>18.2</b> Basics in C++</a></li>
<li class="chapter" data-level="18.3" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#redirect-console-output-to-file"><i class="fa fa-check"></i><b>18.3</b> Redirect Console Output to File</a><ul>
<li class="chapter" data-level="18.3.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#reading-from-the-command-line"><i class="fa fa-check"></i><b>18.3.1</b> Reading from the Command Line</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#pointer"><i class="fa fa-check"></i><b>18.4</b> Pointer</a></li>
<li class="chapter" data-level="18.5" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#functions"><i class="fa fa-check"></i><b>18.5</b> Functions</a><ul>
<li class="chapter" data-level="18.5.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#use-of-pointers-as-function-arguments."><i class="fa fa-check"></i><b>18.5.1</b> Use of Pointers as function arguments.</a></li>
</ul></li>
<li class="chapter" data-level="18.6" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#classess"><i class="fa fa-check"></i><b>18.6</b> Classess</a><ul>
<li class="chapter" data-level="18.6.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#header-files"><i class="fa fa-check"></i><b>18.6.1</b> Header Files</a></li>
</ul></li>
<li class="chapter" data-level="18.7" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#using-makefiles-to-compile-multiple-files"><i class="fa fa-check"></i><b>18.7</b> Using Makefiles to Compile Multiple Files</a></li>
<li class="chapter" data-level="18.8" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#类的继承"><i class="fa fa-check"></i><b>18.8</b> 类的继承</a><ul>
<li class="chapter" data-level="18.8.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#继承类的实时多态-run-time-polymorphism"><i class="fa fa-check"></i><b>18.8.1</b> 继承类的实时多态 Run-Time Polymorphism</a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#模板"><i class="fa fa-check"></i><b>18.9</b> 模板</a><ul>
<li class="chapter" data-level="18.9.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#brief-survey-of-the-standard-template-library"><i class="fa fa-check"></i><b>18.9.1</b> Brief Survey of the Standard Template Library</a></li>
</ul></li>
<li class="chapter" data-level="18.10" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#class-for-linear-algebra"><i class="fa fa-check"></i><b>18.10</b> Class for linear algebra</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="rcpp.html"><a href="rcpp.html"><i class="fa fa-check"></i><b>19</b> Rcpp</a><ul>
<li class="chapter" data-level="19.1" data-path="rcpp.html"><a href="rcpp.html#一个r操作对应的armadillo操作的文档"><i class="fa fa-check"></i><b>19.1</b> 一个R操作对应的armadillo操作的文档：</a></li>
<li class="chapter" data-level="19.2" data-path="rcpp.html"><a href="rcpp.html#rcpp-package"><i class="fa fa-check"></i><b>19.2</b> Rcpp package</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="statistical-computing.html"><a href="statistical-computing.html"><i class="fa fa-check"></i><b>20</b> Statistical Computing</a><ul>
<li class="chapter" data-level="20.1" data-path="statistical-computing.html"><a href="statistical-computing.html#generate-multivariate-normal-samples"><i class="fa fa-check"></i><b>20.1</b> Generate Multivariate Normal samples</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="r-trick.html"><a href="r-trick.html"><i class="fa fa-check"></i><b>21</b> R trick</a></li>
<li class="chapter" data-level="22" data-path="statistic-term.html"><a href="statistic-term.html"><i class="fa fa-check"></i><b>22</b> Statistic term</a></li>
<li class="chapter" data-level="23" data-path="proof-and-calculation.html"><a href="proof-and-calculation.html"><i class="fa fa-check"></i><b>23</b> Proof and Calculation</a><ul>
<li class="chapter" data-level="23.0.1" data-path="proof-and-calculation.html"><a href="proof-and-calculation.html#coordinate-descent-algorithm-for-lasso"><i class="fa fa-check"></i><b>23.0.1</b> Coordinate Descent Algorithm for Lasso</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reversible-jump-mcmc" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Reversible Jump MCMC</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">9.1</span> Introduction</h2>
<p>Provides a general framework for Markov chain Monte Carlo simulation in which the dimension of the parameter space can vary between iterates of the Markov chain. Extension of the Metropolis-Hastings algorithm onto more general state spaces.</p>
<p>Suppose that for observed data <span class="math inline">\(x\)</span> we have a countable collection of candidate models <span class="math inline">\(\mathcal{M}=\left\{\mathcal{M}_{1}, \mathcal{M}_{2}, \ldots\right\}\)</span> indexed by a parameter <span class="math inline">\(k \in \mathcal{K}\)</span>. The index k can be considered as an auxiliary model indicator variable, such that <span class="math inline">\(\mathcal{M}_{k^{\prime}}\)</span> denotes the model where <span class="math inline">\(k=k&#39;\)</span>. Each model <span class="math inline">\(\mathcal{M}_k\)</span> has an <span class="math inline">\(n_k\)</span>-dimensional vector of unknown parameters,<span class="math inline">\(\boldsymbol{\theta}_{k} \in \mathbb{R}^{n_{k}}\)</span>, where <span class="math inline">\(n_k\)</span> can take different values for different models <span class="math inline">\(k\in\mathcal K\)</span>. The joint posterior distribution of <span class="math inline">\((k,\boldsymbol{\theta}_k)\)</span> given observed data,x, is obtained as the product of the likelihood, <span class="math inline">\(L\left(\mathbf{x} | k, \boldsymbol{\theta}_{k}\right)\)</span>, and the joint prior, <span class="math inline">\(p\left(k, \boldsymbol{\theta}_{k}\right)=p\left(\boldsymbol{\theta}_{k} | k\right) p(k)\)</span>, constructed from the prior distribution of <span class="math inline">\(\boldsymbol \theta_k\)</span> under model <span class="math inline">\(\mathcal M_k\)</span>, and the prior for the model indicator k(i.e. the prior for model <span class="math inline">\(\mathcal M_k\)</span>). Hence, the joint posterior is
<span class="math display">\[
\pi\left(k, \boldsymbol{\theta}_{k} | \mathbf{x}\right)=\frac{L\left(\mathbf{x} | k, \boldsymbol{\theta}_{k}\right) p\left(\boldsymbol{\theta}_{k} | k\right) p(k)}{\sum_{k^{\prime} \in \mathcal{K}} \int_{\mathbb{R}^{n} k^{\prime}} L\left(\mathbf{x} | k^{\prime}, \boldsymbol{\theta}_{k^{\prime}}^{\prime}\right) p\left(\boldsymbol{\theta}_{k^{\prime}}^{\prime} | k^{\prime}\right) p\left(k^{\prime}\right) d \boldsymbol{\theta}_{k^{\prime}}^{\prime}}
\]</span>
&gt; 也就是说，这个posterior就是选一个模型（自带<span class="math inline">\(n_k\)</span>个参数），然后再这个模型下可以得到一个posterior，而选定了模型以后，对应<span class="math inline">\(n_k\)</span>个参数也有一个prior，在已知k的时候的prior，这样就构造了对于k个模型，已知数据x的posterior
因为reversible jump就按前言中所说的，是用来解决参数维度变化的。而对于coefficient维度变化则对应了不同的模型，也就是说在不同模型之间跳来跳去的，</p>
<p>The target of an MCMC sampler over the state space <span class="math inline">\(\boldsymbol{\Theta}=\bigcup_{k \in \mathcal{K}}\left(\{k\} \times \mathbb{R}^{n_{k}}\right)\)</span>, where the states of the Markov chain are of the form <span class="math inline">\((k,\boldsymbol \theta_k)\)</span>, the dimension of which can vary over the state space. Accordingly, from the output of a single Markov chain sampler, the user is able to obtain a full probabilistic description of the posterior probabilities of each model having observed the data, x, in addition of the posterior porbabilities of each model having observed the data,x, in addition to the posterior distributions of the individual models.</p>
<blockquote>
<p>输出是一个单独的Markov chain sampler。可以的到期描述每个模型的全后验概率，关于观测数据。</p>
</blockquote>
<div id="from-metropolis-hastings-to-reversible-jump" class="section level3">
<h3><span class="header-section-number">9.1.1</span> From Metropolis-Hastings to Reversible Jump</h3>
<p>Stand form of MH relies on the construction of a time-reversible Markov chain via the detailed balance condition. That is, the moves from state <span class="math inline">\(\theta\)</span> to <span class="math inline">\(\theta&#39;\)</span> are made as often as moves from <span class="math inline">\(\theta&#39;\)</span> to <span class="math inline">\(\theta\)</span> with respect to the ratget density. Then extension MH to the setting that the dimension of the parameter vector varies.</p>
<p>The previous part construction of a Markov chain on a general state space <span class="math inline">\(\Theta\)</span> with invariant or stationary distribution <span class="math inline">\(\pi\)</span>, the detailed balance condition can be written as
<span class="math display">\[
\int_{\left(\theta, \theta^{\prime}\right) \in \mathcal{A} \times \mathcal{B}} \pi(d \theta) P\left(\theta, d \theta^{\prime}\right)=\int_{\left(\theta, \theta^{\prime}\right) \in \mathcal{A} \times \mathcal{B}} \pi\left(d \theta^{\prime}\right) P\left(\theta^{\prime}, d \theta\right)
\]</span></p>
<p>所以只是换了state space,其他都没变？
Compare to standard MH algorithm, Markov chain transitions from a current state <span class="math inline">\(\boldsymbol{\theta}=\left(k, \boldsymbol{\theta}_{k}^{\prime}\right) \in \mathcal{A}\)</span>, in model <span class="math inline">\(\mathcal M_k\)</span> are realized by first proposing a new state <span class="math inline">\(\boldsymbol{\theta}^{\prime}=\left(k^{\prime}, \boldsymbol{\theta}_{k^{\prime}}\right) \in \mathcal{B}\)</span>，(<span class="math inline">\(A\)</span>是<span class="math inline">\(k\)</span>的域，比如<span class="math inline">\(\mathbb N\)</span>? <span class="math inline">\(B\)</span>是<span class="math inline">\(\mathbb R^{n_k}\)</span>,也就是正常的<span class="math inline">\(\theta\)</span>的域。) by proposal distribution <span class="math inline">\(q(\boldsymbol\theta,\boldsymbol \theta&#39;)\)</span>. The detailed balance condition is enforced through the acceptance probability, where the move to the candidate state <span class="math inline">\(\boldsymbol \theta&#39;\)</span> is accepted with probability <span class="math inline">\(\alpha\left(\theta, \theta^{\prime}\right)\)</span>. If rejected, the chain remains at the current state <span class="math inline">\(\boldsymbol \theta\)</span> in model <span class="math inline">\(\mathcal M_k\)</span>. Under this mechanism, detail balance condition equation becomes
<span class="math display">\[
\int_{\left(\theta, \theta^{\prime}\right) \in \mathcal{A} \times \mathcal{B}} \pi(\boldsymbol{\theta} | \mathbf{x}) q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right) \alpha\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right) d \boldsymbol{\theta} d \boldsymbol{\theta}^{\prime}=\int_{\left(\theta, \boldsymbol{\theta}^{\prime}\right) \in \mathcal{A} \times \mathcal{B}} \pi\left(\boldsymbol{\theta}^{\prime} | \mathbf{x}\right) q\left(\boldsymbol{\theta}^{\prime}, \boldsymbol{\theta}\right) \alpha\left(\boldsymbol{\theta}^{\prime}, \boldsymbol{\theta}\right) d \boldsymbol{\theta} d \boldsymbol{\theta}^{\prime}
\]</span>
where <span class="math inline">\(\pi(\theta|x)\)</span> are the posterior we want sample.</p>
<p>One way to enforce this equation is by setting the acceptance probability as</p>
<p><span class="math display">\[
\alpha\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right)=\min \left\{1, \frac{\pi(\boldsymbol{\theta} | \mathbf{x}) q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\prime}\right)}{\pi\left(\boldsymbol{\theta}^{\prime} | \mathbf{x}\right) q\left(\boldsymbol{\theta}^{\prime}, \boldsymbol{\theta}\right)}\right\}
\]</span></p>
<p>This resembles the usual metropolis-hastings acceptance ratio. Then a reversible jump sampler with N iterations is commonly constructed as follows:</p>
<ul>
<li>Initialize k and <span class="math inline">\(\boldsymbol \theta_k\)</span> at iteration <span class="math inline">\(t=1\)</span></li>
<li>For iteration <span class="math inline">\(t\geq 1\)</span> perform
<ul>
<li>Within-model move: with a fixed model k, update the parameters <span class="math inline">\(\boldsymbol \theta_k\)</span> acoording to any MCMC updating scheme</li>
<li>Between-models move: simultaneously update model indicator k and the parameter <span class="math inline">\(\boldsymbol \theta_k\)</span> according to the general reversible proposal/acceptance mechanism<br />
</li>
</ul></li>
<li>Increment iteration t=t+1. If <span class="math inline">\(t&lt;N\)</span>, go to step 2.</li>
</ul>
<blockquote>
<p>问题：Within-model move是一个传统的MH move，Between-model move是延拓的MH move，关键是，必须要在本model做一个move，然后再抽model吗？还是说其实只做两个一起抽也是可以的？</p>
</blockquote>
</div>
<div id="application-area" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Application Area</h3>
<p><em>Change-point models</em> One of the original application of the reversible jump sampler was in Bayesian change-point problems. Green(2005) analyzed mining disaster count data using a Poisson process with the rate parameter described as a step function with an unknown number and location of steps. Fan and Brooks(2000) applies the reversible jump sampler to model the shape of prehistoric tombs, where the curvature of the dome changes an unknown number of times</p>
<p><em>Finite mixture models</em>. Mixture models are widely used where each data observation is generated according to some underlying categorical mechanism, which the mechanism is typically unobserved, so there is uncertainty regarding which component of the resulting mixture distribution each data observation was generated from, and uncertainty ober the nuber of mixture components.
<span class="math display">\[
f(x|\theta_k)=\sum_{j=1}^k w_jf_j(x|\phi_j)
\]</span></p>
<p><em>Variable selection</em> The multi-model setting emerges when attempting to identify the most relevant subsets of predictors, making it a natural candidate for the reversible jump sampler.</p>
<p><em>Nonparametrics</em> Within Bayesian nonparametrics, many authors have successfully explored the use of the reversible jump sampler as a method to automate the knot selection process when using a pth-order spline model for curve fitting.
<span class="math display">\[
f(x)=\alpha_0+\sum_{j=1}^{P}\alpha_j x^j+\sum_{i=1}^k \eta_j(x-\kappa_i)^P_+ 
\]</span>
with <span class="math inline">\(x\in[a,b]\)</span></p>
<p><em>Time series models</em> In the modelling of temporally dependent data, <span class="math inline">\(x_1,...,x_T\)</span>, multiple models naturally arise under uncertainty over the degree of dependence. For example, under a kth-order autoregressive process
<span class="math display">\[
X_t=\sum^k_{\tau=1}a_\tau X_{t-\tau}+\epsilon_t
\]</span>
with <span class="math inline">\(t=k+1,...,T\)</span></p>
</div>
</div>
<div id="implementation" class="section level2">
<h2><span class="header-section-number">9.2</span> Implementation</h2>
<p>In practice, the construction of proposal moves between different models is achieved via the concept of “dimension matching.” Most simply, under a general Bayesian model determination setting, suppose that we are currently in state <span class="math inline">\((k,\theta_k)\)</span> in model <span class="math inline">\(\mathcal M_k\)</span>, and we wish to propose a move to a state <span class="math inline">\((k&#39;,\theta_k&#39;)\)</span> in model <span class="math inline">\(\mathcal M_{k&#39;}\)</span>, which is of a higher dimension, so that <span class="math inline">\(n_{k^{\prime}}&gt;n_{k}\)</span>. In order to “math dimensions” between the two model states, a random vector <span class="math inline">\(\boldsymbol u\)</span> of length <span class="math inline">\(d_{k \rightarrow k^{\prime}}=n_{k^{\prime}}-n_{k}\)</span> is generated from a known density <span class="math inline">\(q_{d_{k \rightarrow k^{\prime}}}(\mathbf{u})\)</span>. The current state <span class="math inline">\(\boldsymbol \theta_k\)</span> and the random vector <span class="math inline">\(\boldsymbol u\)</span> are then mapped to the new state <span class="math inline">\(\boldsymbol{\theta}_{k^{\prime}}^{\prime}=g_{k \rightarrow k^{\prime}}\left(\boldsymbol{\theta}_{k}, \mathbf{u}\right)\)</span> through a one-to-one mapping function <span class="math inline">\(g_{k \rightarrow k^{\prime}} : \mathbb{R}^{n_{k}} \times \mathbb{R}^{d_{k}} \rightarrow \mathbb{R}^{n_{k^{\prime}}}\)</span>. The acceptance probability of this proposal, combined with the joint posterior expression becomes
<span class="math display">\[
\alpha\left[\left(k, \theta_{k}\right),\left(k^{\prime}, \theta_{k^{\prime}}^{\prime}\right)\right]=\min \left\{1, \frac{\pi\left(k^{\prime}, \theta_{k^{\prime}}^{\prime} | \mathbf{x}\right) q\left(k^{\prime} \rightarrow k\right)}{\pi\left(k, \theta_{k} | \mathbf{x}\right) q\left(k \rightarrow k^{\prime}\right) q_{d_{k \rightarrow k^{\prime}}}(\mathbf{u})}\left|\frac{\partial g_{k \rightarrow k^{\prime}}\left(\boldsymbol{\theta}_{k}, \mathbf{u}\right)}{\partial\left(\theta_{k}, \mathbf{u}\right)}\right|\right\}
\]</span></p>
<p>为了填补缺少的维度，提出一个随机向量<span class="math inline">\(u\)</span>，维度是缺少的维度，来源于一个已知的密度函数<span class="math inline">\(q_{d_{k\rightarrow k&#39;}}(u)\)</span>.(这个密度怎么取？)。然后目前的状态<span class="math inline">\(\theta_k\)</span>和<span class="math inline">\(u\)</span>可以映射到新的状态<span class="math inline">\(\boldsymbol{\theta}_{k^{\prime}}^{\prime}=g_{k \rightarrow k^{\prime}}\left(\boldsymbol{\theta}_{k}, \mathbf{u}\right)\)</span>,通过g这个一一映射，<span class="math inline">\(g_{k \rightarrow k^{\prime}} : \mathbb{R}^{n_{k}} \times \mathbb{R}^{d_{k}} \rightarrow \mathbb{R}^{n_{k^{\prime}}}\)</span>. 然后有接受概率如上。解释，分子就是从新到老的概率，下面是从老到新的概率，因为多了u，所以多一块u的概率，而反过来则没有。
后面那块则是一一映射g的jacobian矩阵。这一块是来自于因为对<span class="math inline">\(\theta\)</span>做了变换。在detail-balance condition equation那块做积分的时候会用到。</p>
<p>反过来reverse move的话（从高维move到低维），则有接受概率
<span class="math display">\[
\alpha\left[\left(k^{\prime}, \boldsymbol{\theta}_{k^{\prime}}^{\prime}\right),\left(k, \boldsymbol{\theta}_{k}\right)\right]=\alpha\left[\left(k, \boldsymbol{\theta}_{k}\right),\left(k^{\prime}, \boldsymbol{\theta}_{k^{\prime}}^{\prime}\right)\right]^{-1}
\]</span>
。</p>
<p>更一般的，我们可以放宽u长度的条件，也就是让<span class="math inline">\(d_{k \rightarrow k^{\prime}} \geq n_{k^{\prime}}-n_{k}\)</span>. 则在这种情况下，不确定的反向移动可以通过一个<span class="math inline">\(d_{k^{\prime} \rightarrow k}\)</span>-维的向量，<span class="math inline">\(\mathbf{u}^{\prime} \sim q_{d_{k^{\prime} \rightarrow k}}\left(\mathbf{u}^{\prime}\right)\)</span>,使得对应“dimension-match”条件能成立:在同一维度：<span class="math inline">\(n_{k}+d_{k \rightarrow k^{\prime}}=n_{k^{\prime}}+d_{k^{\prime} \rightarrow k}\)</span>.</p>
<p>则反过来的映射可以写成<span class="math inline">\(\boldsymbol{\theta}_{k}=g_{k^{\prime} \rightarrow k}\left(\boldsymbol{\theta}_{k^{\prime}}^{\prime} \mathbf{u}^{\prime}\right)\)</span> ,使得有<span class="math inline">\(\boldsymbol{\theta}_{k}=g_{k^{\prime} \rightarrow k}\left(g_{k \rightarrow k^{\prime}}\left(\boldsymbol{\theta}_{k}, \mathbf{u}\right), \mathbf{u}^{\prime}\right)\)</span>,以及<span class="math inline">\(\boldsymbol{\theta}_{k^{\prime}}^{\prime}=g_{k \rightarrow k^{\prime}}\left(g_{k^{\prime} \rightarrow k}\left(\boldsymbol{\theta}_{k^{\prime}}^{\prime} \mathbf{u}^{\prime}\right), \mathbf{u}\right)\)</span>.接受概率则可以写成:</p>
<p><span class="math display">\[
\alpha\left[\left(k, \theta_{k}\right),\left(k^{\prime}, \theta_{k^{\prime}}^{\prime}\right)\right]=\min \left\{1, \frac{\pi\left(k^{\prime}, \theta_{k^{\prime}} | \mathbf{x}\right) q\left(k^{\prime} \rightarrow k\right) q_{d_{k^{\prime} \rightarrow k}}\left(\mathbf{u}^{\prime}\right)}{\pi\left(k, \theta_{k} | \mathbf{x}\right) q\left(k \rightarrow k^{\prime}\right) q_{d_{k \rightarrow k^{\prime}}}(\mathbf{u})}\left|\frac{\partial g_{k \rightarrow k^{\prime}}\left(\boldsymbol{\theta}_{k}, \mathbf{u}\right)}{\partial\left(\theta_{k}, \mathbf{u}\right)}\right|\right\}
\]</span></p>
<blockquote>
<p>相当于，可以找一个更高维的中间变量，然后一个随机向量u，补齐各状态与中间变量之间的差异。这样就可以避免高维到低维，低维到高维的不对等关系，使得算法表达形式能统一。</p>
</blockquote>
<div id="example-dimension-matching" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Example Dimension Matching</h3>
<p>Example in Green(1995) and Brooks(1998). Suppose that model <span class="math inline">\(\mathcal M_1\)</span> has states (<span class="math inline">\(k=1,\theta_1\in \mathbb R^1\)</span>) and model <span class="math inline">\(\mathcal M_2\)</span> with states (<span class="math inline">\(k=2,\theta_2\in \mathbb R^2\)</span>). So under this setting, <span class="math inline">\((1,\theta^*)\)</span> denote the current state in <span class="math inline">\(\mathcal M_1\)</span> and <span class="math inline">\((2,(\theta^{(1)},\theta^{(2)}))\)</span> denote the proposed state in <span class="math inline">\(\mathcal M_2\)</span>.
Under dimension matching, we might generate a random scalar u, and let <span class="math inline">\(\theta^{(1)}=\theta^{*}+u \text { and } \theta^{(2)}=\theta^{*}-u\)</span>, with the reverse move given deterministically by <span class="math inline">\(\theta^{*}=\frac{1}{2}\left(\theta^{(1)}+\theta^{(2)}\right)\)</span>.</p>
<p>所以就把1维和2维的统一在了一起，并且给定了转换表达式。</p>
</div>
<div id="example-moment-matching-in-a-finite-mixture-of-univariate-normals" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Example: Moment Matching in a Finite Mixture of Univariate Normals</h3>
<p>如果模型是有限个单变量正态分布的混合，观测数据,<span class="math inline">\(x\)</span>,则服从混合正态分布,每块是<span class="math inline">\(N(\mu_j,\sigma_j)\)</span>. 模型之间的移动，Richardson and Green(1997) 实现了一个分割和合并的策略。</p>
<p>当两个正态<span class="math inline">\(j_1\)</span>和<span class="math inline">\(j_2\)</span> 合并成一个,<span class="math inline">\(j^*\)</span>,有如下确定的映射，同时保持了0，1，2阶的矩。
<span class="math display">\[
\begin{aligned} w_{j^{*}} &amp;=w_{j_{1}}+w_{j_{2}} \\ w_{j^{*}} \mu_{j^{*}} &amp;=w_{j_{1}} \mu_{j_{1}}+w_{j_{2}} \mu_{j_{2}} \\ w_{j^{*}}\left(\mu_{j^{*}}^{2}+\sigma_{j^{*}}^{2}\right) &amp;=w_{j_{1}}\left(\mu_{j 1}^{2}+\sigma_{j_{1}}^{2}\right)+w_{j_{2}}\left(\mu_{j_{2}}^{2}+\sigma_{j_{2}}^{2}\right) \end{aligned}
\]</span>
分割则是
<span class="math display">\[
\begin{aligned} w_{j_{1}} &amp;=w_{j^{*}} * u_{1}, \quad w_{j_{2}}=w_{j^{*}} *\left(1-u_{1}\right) \\ \mu_{j_{1}} &amp;=\mu_{j^{*}}-u_{2} \sigma_{j^{*}} \sqrt{\frac{w_{j_{2}}}{w_{j_{1}}}} \\ \mu_{j_{2}} &amp;=\mu_{j^{*}}+u_{2} \sigma_{j^{*}} \sqrt{\frac{w_{j_{1}}}{w_{j_{2}}}} \\ \sigma_{j_{1}}^{2} &amp;=u_{3}\left(1-u_{2}^{2}\right) \sigma_{j^{*}}^{2} \frac{w_{j^{*}}}{w_{j_{2}}} \\ \sigma_{j_{2}}^{2} &amp;=\left(1-u_{3}\right)\left(1-u_{2}^{2}\right) \sigma_{j^{*}}^{2} \frac{w_{j^{*}}}{w_{j_{2}}} \end{aligned}
\]</span>
同时随机量<span class="math inline">\(u_{1}, u_{2} \sim \operatorname{Beta}(2,2)\)</span>,以及<span class="math inline">\(u_{3} \sim \operatorname{Beta}(1,1)\)</span>。这样满足了维度相同，然后能算分割步的接受概率和合并的接受概率。</p>
</div>
</div>
<div id="mapping-functions-and-proposal-distribution" class="section level2">
<h2><span class="header-section-number">9.3</span> Mapping Functions and Proposal Distribution</h2>
<p>虽然维度匹配的想法很简单，但是实现却很复杂。因为存在一个任意的映射函数<span class="math inline">\(g_{k \rightarrow k^{\prime}}\)</span> 和proposal distribution <span class="math inline">\(q_{d_{k \rightarrow k^{\prime}}}(\mathbf{u})\)</span>。好的映射很自然的能提高采样的效率，体现在模型之间跳的接受率和chain本身的收敛。难点在于即是是很简单的nested model，也很难定义一个好的关系。模型之间的自由度只由<span class="math inline">\(q_{d_{k \rightarrow k^{\prime}}}(\mathbf{u})\)</span>决定。然而没有一个很显然的准则选择好的q。对比模型内的选择，随机游走Mh算法可能对应任意的接受概率，一个小的游走对应高的接受概率，大步游走则接受概率会比较低。可以借鉴这个“local”的想法到模型空间<span class="math inline">\(k\in\mathcal K\)</span>中。proposal来自于<span class="math inline">\(\theta_k\)</span> 在模型<span class="math inline">\(\mathcal M_k\)</span>比起另外一个<span class="math inline">\(\theta_{k&#39;}\)</span>来自于<span class="math inline">\(\mathcal M_{k&#39;}\)</span>会有更高的接受率，如果数据对应的likelihood差别不大的话。比如说Richardson 和Green(1997)提出的“birth/death” and “split.merge”映射,在切换模型的时候尽量切换likelihood差不多的模型。但是这并不容易实现。
尽管如此，RjMCMC的采样效果还是很差。</p>
<blockquote>
<p>所以要用的时候还是从最简单的birth-dead proposal和merge-split proposal开始看起吧。</p>
</blockquote>
<div id="marginalization-and-augmentation" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Marginalization and augmentation:</h3>
<p>Reduced- or fixed- dimensional sampler may be substituted because reversible jump MCMC would be heaby-handed.
In lower dimensions, the reversible jump sampler is often easier to implement, as the problems associated with mapping function specification are conceptually simpler to resolve.</p>
<p><a href="Example:Marginalization" class="uri">Example:Marginalization</a> in Variable Selection.</p>
<p>用辅助变量，则能把parameter变成固定维数的问题。Under certain prior specifications for the regression coefficient <span class="math inline">\(\beta\)</span> and error variance <span class="math inline">\(\sigma^2\)</span>，the <span class="math inline">\(\beta\)</span> coefficients can be analytically integrated out of the posterior. A gibbs sampler directly on model space is then available for <span class="math inline">\(\gamma\)</span>。也就是说主要关注选取哪个模型的问题的话可以把<span class="math inline">\(\beta\)</span>先积掉然后研究模型问题，等选定模型再估<span class="math inline">\(\beta\)</span>?</p>
<p><a href="Example:Marginalization" class="uri">Example:Marginalization</a> in Finite Mixture of Multivariate Normal Models.</p>
<p>在Clusting问题里面，待估参数不是那么重要，所以Tadesse et al. (2005)提出来选取特定的先验，正态的系数可以积掉，然后对分几类做Reversible Jump，这样问题就被大大简化了。</p>
<p>细节等要用这块再说。</p>
</div>
<div id="centering-and-order-methods" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Centering and Order Methods</h3>
<p>Brooks et al.(2003c) introduce a class of methods to achieve the automatic scaling of the proposal density <span class="math inline">\(q_{d_{k\rightarrow k&#39;}}(u)\)</span>, based on “local” move proposal distribution, which are centered around the point of equal likelihood values under current and proposed models.</p>
<blockquote>
<p>这块以后再看好了。。。从目前的文献来看，birth-death 和 split-merge 策略已经足够很多了？等要用再来看后来这个auto-adaptive的方法</p>
</blockquote>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basicmcmc.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="optimal-proposal-distributions-and-adaptive-mcmc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
