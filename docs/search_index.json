[
["statistician-tool-box.html", "Chapter 4 Statistician Tool Box 4.1 Matrix algebra 4.2 两个二次型相加 4.3 正定阵的基础定理： 4.4 对称阵的谱分解相关定理 4.5 Covariance Structure", " Chapter 4 Statistician Tool Box 4.1 Matrix algebra Definition 4.1 定义：矩阵\\(A(t)\\)的导数 如果矩阵\\(A(t)=\\left(a_{i j}(t)\\right)_{m \\times n}\\) 的每一个元素\\(a_{ij}(t)\\)是变量t的可微函数，则称A(t)可微，其导数（微商）定义为 \\[ A^{\\prime}(t)=\\frac{\\mathrm{d}}{\\mathrm{d} t} A(t)=\\left(\\frac{\\mathrm{d}}{\\mathrm{d} t} a_{i j}(t)\\right)_{m \\times n} \\] Definition 4.2 定义：矩阵对矩阵的导数 设 \\(\\boldsymbol{X}=\\left(\\xi_{i j}\\right)_{m \\times n}\\), \\(mn\\) 元函数 \\(f(\\boldsymbol{X})=f\\left(\\xi_{11}, \\xi_{12}\\cdots, \\xi_{1 n}, \\xi_{21}, \\cdots, \\xi_{m n} )\\right.\\),定义\\(f(X)\\) 对矩阵\\(X\\)的导数为 \\[\\begin{equation} \\frac{\\mathrm{d} f}{\\mathrm{d} \\boldsymbol{X}}=\\left(\\frac{\\partial f}{\\partial \\xi_{i j}}\\right)_{m \\times n}= \\left[ \\begin{array}{ccc} {\\frac{\\partial f}{\\partial \\xi_{11}}} &amp; {\\dots} &amp; {\\frac{\\partial f}{\\partial \\xi_{1 n}}} \\\\ {\\vdots} &amp; { } &amp; {\\vdots} \\\\ {\\frac{\\partial f}{\\partial \\xi_{m 1}}} &amp; {\\cdots} &amp; {\\frac{\\partial f}{\\partial \\xi_{m n}}} \\end{array}\\right] \\end{equation}\\] 根据这个定义，能够直接证明以下两个最常见的结论 \\[ \\frac{\\partial x&#39;a}{\\partial x}=a=\\frac{\\partial a&#39;x}{\\partial x} \\] \\[ \\frac{\\partial x&#39;Ax}{\\partial x}=(A+A&#39;)x \\] 证明： 将矩阵乘法\\(x^TAx\\)打开得到 \\[ \\begin{aligned} f(\\boldsymbol{x})=&amp; \\sum_{i=1}^{n} \\sum_{j=1}^{n} a_{i j} \\xi_{i} \\xi_{j}=\\\\ &amp; \\xi_{1} \\sum_{j=1}^{n} a_{1 j} \\xi_{j}+\\cdots+\\xi_{k} \\sum_{j=1}^{n} a_{k j} \\xi_{j}+\\cdots+\\xi_{n} \\sum_{j=1}^{n} a_{n j} \\xi_{j} \\end{aligned} \\] 且有 \\[ \\begin{align} \\frac{\\partial f}{\\partial \\xi_{k}}=\\xi_{1} a_{1 k}+\\cdots+\\xi_{k-1} a_{k-1, k}+\\left(\\sum_{j=1}^{n} a_{k j} \\xi_{j}+\\xi_{k} a_{k k}\\right)+\\\\ \\xi_{k+1} a_{k+1, k}+\\cdots+\\xi_{n} a_{\\# k}=\\sum_{j=1}^{n} a_{i j} \\xi_{j}+\\sum_{i=1}^{n} a_{k} \\xi_{i} \\end{align} \\] 所以有 \\[ \\frac{d f}{d x}=\\left[ \\begin{array}{c}{\\frac{\\partial f}{\\partial \\xi_{1}}} \\\\ {\\vdots} \\\\ {\\frac{\\partial f}{\\partial \\xi_{n}}}\\end{array}\\right]=\\left[ \\begin{array}{c}{\\sum_{j=1}^{n} a_{1 j} \\xi_{j}} \\\\ {\\vdots} \\\\ {\\sum_{j=1}^{n} a_{n j} \\xi_{j}}\\end{array}\\right]+\\left[ \\begin{array}{c}{\\sum_{i=1}^{n} a_{i 1} \\xi_{i}} \\\\ {\\vdots} \\\\ {\\sum_{i=1}^{n} a_{i n} \\xi_{i}}\\end{array}\\right]=\\\\ \\boldsymbol{A x}+\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{x}=\\left(\\boldsymbol{A}+\\boldsymbol{A}^{\\mathrm{T}}\\right) \\boldsymbol{x} \\] 4.1.1 Block diagonal matrices \\[ \\mathbf{A}=\\left[ \\begin{array}{cccc}{\\mathbf{A}_{1}} &amp; {0} &amp; {\\cdots} &amp; {0} \\\\ {0} &amp; {\\mathbf{A}_{2}} &amp; {\\cdots} &amp; {0} \\\\ {\\vdots} &amp; {\\vdots} &amp; {\\ddots} &amp; {\\vdots} \\\\ {0} &amp; {0} &amp; {\\cdots} &amp; {\\mathbf{A}_{n}}\\end{array}\\right] \\] property: \\[ \\begin{aligned} \\operatorname{det} \\mathbf{A} &amp;=\\operatorname{det} \\mathbf{A}_{1} \\times \\cdots \\times \\operatorname{det} \\mathbf{A}_{n} \\\\ \\operatorname{tr} \\mathbf{A} &amp;=\\operatorname{tr} \\mathbf{A}_{1}+\\cdots+\\operatorname{tr} \\mathbf{A}_{n} \\end{aligned} \\] It’s inverse: \\[ \\left( \\begin{array}{cccc}{\\mathbf{A}_{1}} &amp; {0} &amp; {\\cdots} &amp; {0} \\\\ {0} &amp; {\\mathbf{A}_{2}} &amp; {\\cdots} &amp; {0} \\\\ {\\vdots} &amp; {\\vdots} &amp; {\\ddots} &amp; {\\vdots} \\\\ {0} &amp; {0} &amp; {\\cdots} &amp; {\\mathbf{A}_{n}}\\end{array}\\right)=\\left( \\begin{array}{cccc}{\\mathbf{A}_{1}^{-1}} &amp; {0} &amp; {\\cdots} &amp; {0} \\\\ {0} &amp; {\\mathbf{A}_{2}^{-1}} &amp; {\\cdots} &amp; {0} \\\\ {\\vdots} &amp; {\\vdots} &amp; {\\ddots} &amp; {\\vdots} \\\\ {0} &amp; {0} &amp; {\\cdots} &amp; {\\mathbf{A}_{n}^{-1}}\\end{array}\\right) \\] 4.2 两个二次型相加 In vector formulation(Assuming \\(\\Sigma_1\\),\\(\\Sigma_2\\) are symmetric) 有： \\[ \\begin{array}{c}{-\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{m}_{1}\\right)^{T} \\mathbf{\\Sigma}_{1}^{-1}\\left(\\mathbf{x}-\\mathbf{m}_{1}\\right)} \\\\ {-\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{m}_{2}\\right)^{T} \\mathbf{\\Sigma}_{2}^{-1}\\left(\\mathbf{x}-\\mathbf{m}_{2}\\right)} \\\\ {=-\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{m}_{c}\\right)^{T} \\mathbf{\\Sigma}_{c}^{-1}\\left(\\mathbf{x}-\\mathbf{m}_{c}\\right)+C}\\end{array} \\] 其中： \\[ \\begin{aligned} \\mathbf{\\Sigma}_{c}^{-1}=&amp; \\mathbf{\\Sigma}_{1}^{-1}+\\mathbf{\\Sigma}_{2}^{-1} \\\\ \\mathbf{m}_{c} &amp;=\\left(\\mathbf{\\Sigma}_{1}^{-1}+\\mathbf{\\Sigma}_{2}^{-1}\\right)^{-1}\\left(\\mathbf{\\Sigma}_{1}^{-1} \\mathbf{m}_{1}+\\mathbf{\\Sigma}_{2}^{-1} \\mathbf{m}_{2}\\right) \\\\ C &amp;=\\frac{1}{2}\\left(\\mathbf{m}_{1}^{T} \\mathbf{\\Sigma}_{1}^{-1}+\\mathbf{m}_{2}^{T} \\mathbf{\\Sigma}_{2}^{-1}\\right)\\left(\\mathbf{\\Sigma}_{1}^{-1}+\\mathbf{\\Sigma}_{2}^{-1}\\right)^{-1}\\left(\\mathbf{\\Sigma}_{1}^{-1} \\mathbf{m}_{1}+\\mathbf{\\Sigma}_{2}^{-1} \\mathbf{m}_{2}\\right) \\\\ &amp;-\\frac{1}{2}\\left(\\mathbf{m}_{1}^{T} \\mathbf{\\Sigma}_{1}^{-1} \\mathbf{m}_{1}+\\mathbf{m}_{2}^{T} \\mathbf{\\Sigma}_{2}^{-1} \\mathbf{m}_{2}\\right) \\end{aligned} \\] 而对于我需要的情况，简化\\(m_1=0\\),\\(m_2=0\\) 那么就有，只需要\\(\\mathbf{\\Sigma}_{c}^{-1}=\\mathbf{\\Sigma}_{1}^{-1}+\\mathbf{\\Sigma}_{2}^{-1}\\),同时\\(m_c=0\\). 4.3 正定阵的基础定理： Theorem 1 For a \\(p \\times p\\) symmetric matrix \\(\\Sigma=\\left(\\sigma_{i j}\\right),\\) the following are equivalent: \\(\\Sigma\\) is nonnegative definite. all its leading principal minors are nonnegative definite, that is, the \\(i \\times i\\) matrices \\[ \\boldsymbol{\\Sigma}_{i i}=\\left(\\begin{array}{ccc} {\\sigma_{11}} &amp; {\\cdots} &amp; {\\sigma_{1 i}} \\\\ {\\vdots} &amp; {\\ddots} &amp; {\\vdots} \\\\ {\\sigma_{i 1}} &amp; {\\cdots} &amp; {\\sigma_{i i}} \\end{array}\\right), i=1, \\cdots, p \\] are nonnegative definite. all eigenvalues of \\(\\Sigma\\) are nonnegative. there exists a matrix A such that \\[ \\boldsymbol{\\Sigma}=A A^{\\prime} \\] there exists a lower triangular matrix \\(L\\) such that \\[ \\boldsymbol{\\Sigma}=L L^{\\prime} \\] there exist vectors \\(\\boldsymbol{u}_{1}, \\cdots, \\boldsymbol{u}_{p}\\) in \\(R^{p}\\) such that \\(\\sigma_{i j}=\\boldsymbol{u}_{i}^{\\prime} \\boldsymbol{u}_{j}\\) 4.4 对称阵的谱分解相关定理 Theorem 4 (The Spectral Decomposition) Let \\(A\\) be a \\(p \\times p\\) symmetric matrix with p pairs of eigenvalues and eigenvectors \\[ \\left(\\lambda_{1}, \\mathbf{e}_{1}\\right),\\left(\\lambda_{2}, \\mathbf{e}_{2}\\right), \\cdots,\\left(\\lambda_{p}, \\mathbf{e}_{p}\\right) \\] Then, The eigenvalues \\(\\lambda_{1}, \\ldots, \\lambda_{p}\\) are all real, and can be ordered from the largest to the smallest \\[ \\lambda_{1} \\geq \\lambda_{2} \\geq \\ldots \\geq \\lambda_{p} \\] The normalized eigenvectors \\(\\mathbf{e}_{1}, \\ldots, \\mathbf{e}_{p}\\) are mutually orthogonal and the matrix \\[ P=\\left(\\mathbf{e}_{1}, \\mathbf{e}_{2}, \\ldots, \\mathbf{e}_{p}\\right) \\] is an orthogonal matrix, that is, \\[ P P^{\\prime}=P^{\\prime} P=I \\] The spectral decomposition of \\(A\\) is the expansion \\[ A=\\lambda_{1} \\mathbf{e}_{1} \\mathbf{e}_{1}^{\\prime}+\\lambda_{2} \\mathbf{e}_{2} \\mathbf{e}_{2}^{\\prime}+\\cdots+\\lambda_{p} \\mathbf{e}_{p} \\mathbf{e}_{p}^{\\prime}=P \\Lambda P^{\\prime} \\] where \\(P\\) is as above and \\[ \\Lambda=\\operatorname{diag}\\left(\\lambda_{1}, \\cdots, \\lambda_{p}\\right) \\] is a diagonal matrix with \\(\\lambda_{1}, \\ldots, \\lambda_{p}\\) as its respective diagonal entries. The matrix \\(A\\) is nonnegative definite, if and only if all its eigenvalues are nonnegative. 4.5 Covariance Structure 4.5.1 Compound Symmetry Covariance结构 P54 of pourahmadi covariance book a sufficient condition for nonnegative definite is : \\(1+(p-1) \\rho \\geq 0\\) or \\(-(p-1)^{-1} \\leq \\rho \\leq 1\\) 4.5.2 Huynh-Feldt Structure \\(\\Sigma=\\sigma^{2}\\left(\\alpha I+a \\mathbf{1}_{p}^{\\prime}+\\mathbf{1}_{p} a^{\\prime}\\right)\\) The \\(\\Sigma\\) is nonnegative definite provided that \\[ \\alpha&gt;\\mathbf{1}_{p}^{\\prime} a-\\sqrt{p a^{\\prime} a} \\] 4.5.3 The One-Dependent Covariance Structure \\[ \\Sigma=\\sigma^{2}\\left(\\begin{array}{ccccc} {a} &amp; {b} &amp; {0} &amp; {\\cdots} &amp; {0} \\\\ {b} &amp; {a} &amp; {b} &amp; {\\ddots} &amp; {\\vdots} \\\\ {\\vdots} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {0} \\\\ {\\vdots} &amp; {} &amp; {\\ddots} &amp; {\\ddots} &amp; {b} \\\\ {0} &amp; {\\cdots} &amp; {0} &amp; {b} &amp; {a} \\end{array}\\right) \\] "]
]
