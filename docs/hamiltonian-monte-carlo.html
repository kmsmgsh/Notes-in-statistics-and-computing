<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 10 Hamiltonian Monte Carlo | Notes on Statistics</title>
  <meta name="description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 10 Hamiltonian Monte Carlo | Notes on Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Hamiltonian Monte Carlo | Notes on Statistics" />
  
  <meta name="twitter:description" content="This is a minimal notes on the problem facing and follow the idea by yufree.cn/notes" />
  

<meta name="author" content="Jiaming Shen">


<meta name="date" content="2020-02-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="optimal-proposal-distributions-and-adaptive-mcmc.html">
<link rel="next" href="bayes-variable-selection.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes in statistics and computing</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>preliminary</a></li>
<li class="chapter" data-level="" data-path="e69d82e4b883e69d82e585ab.html"><a href="e69d82e4b883e69d82e585ab.html"><i class="fa fa-check"></i>杂七杂八</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html"><i class="fa fa-check"></i><b>2</b> Mathematical statistic Trick</a><ul>
<li class="chapter" data-level="2.1" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#NormalForm"><i class="fa fa-check"></i><b>2.1</b> Normal distribution as exponential family</a></li>
<li class="chapter" data-level="2.2" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 密度变换公式：</a></li>
<li class="chapter" data-level="2.3" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#probability-mass-function"><i class="fa fa-check"></i><b>2.3</b> Probability mass function:</a></li>
<li class="chapter" data-level="2.4" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#vector-to-diagonal-matrix"><i class="fa fa-check"></i><b>2.4</b> Vector to diagonal matrix:</a></li>
<li class="chapter" data-level="2.5" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#gaussian-integral-trick"><i class="fa fa-check"></i><b>2.5</b> Gaussian Integral Trick</a></li>
<li class="chapter" data-level="2.6" data-path="mathematical-statistic-trick.html"><a href="mathematical-statistic-trick.html#asymptotic-issues"><i class="fa fa-check"></i><b>2.6</b> Asymptotic issues</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="general-notes.html"><a href="general-notes.html"><i class="fa fa-check"></i><b>3</b> General Notes</a><ul>
<li class="chapter" data-level="3.1" data-path="general-notes.html"><a href="general-notes.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 台湾教授彭明辉教授的研究生手册</a><ul>
<li class="chapter" data-level="3.1.1" data-path="general-notes.html"><a href="general-notes.html#section-3.1.1"><i class="fa fa-check"></i><b>3.1.1</b> 读论文的要求</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="general-notes.html"><a href="general-notes.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 论文报告的要求与技巧</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html"><i class="fa fa-check"></i><b>4</b> Statistician Tool Box</a><ul>
<li class="chapter" data-level="4.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#matrix"><i class="fa fa-check"></i><b>4.1</b> Matrix algebra</a><ul>
<li class="chapter" data-level="4.1.1" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#block-diagonal-matrices"><i class="fa fa-check"></i><b>4.1.1</b> Block diagonal matrices</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="statistician-tool-box.html"><a href="statistician-tool-box.html#sumSquare"><i class="fa fa-check"></i><b>4.2</b> 两个二次型相加</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html"><i class="fa fa-check"></i><b>5</b> Longitudinal data analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#linear-mixed-model"><i class="fa fa-check"></i><b>5.1</b> Linear mixed model</a><ul>
<li class="chapter" data-level="5.1.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#condition-mean-vs-marginal-mean"><i class="fa fa-check"></i><b>5.1.1</b> Condition Mean vs Marginal mean</a></li>
<li class="chapter" data-level="5.1.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#restricted-maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.1.2</b> Restricted maximum likelihood estimation</a></li>
<li class="chapter" data-level="5.1.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#prediction"><i class="fa fa-check"></i><b>5.1.3</b> Prediction</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#generalised-linear-mixed-models"><i class="fa fa-check"></i><b>5.2</b> Generalised linear mixed models</a><ul>
<li class="chapter" data-level="5.2.1" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#expFam"><i class="fa fa-check"></i><b>5.2.1</b> Exponential distribution family</a></li>
<li class="chapter" data-level="5.2.2" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#iteratively-reweighted-least-square-algorithm-iwls"><i class="fa fa-check"></i><b>5.2.2</b> Iteratively reweighted Least square algorithm (IWLS)</a></li>
<li class="chapter" data-level="5.2.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#glmms"><i class="fa fa-check"></i><b>5.2.3</b> GLMMs</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="longitudinal-data-analysis.html"><a href="longitudinal-data-analysis.html#the-bayesian-analysis-approach-for-covariance-modelling"><i class="fa fa-check"></i><b>5.3</b> The Bayesian analysis approach for covariance modelling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 统计图形笔记</a></li>
<li class="chapter" data-level="7" data-path="basicmcmc.html"><a href="basicmcmc.html"><i class="fa fa-check"></i><b>7</b> BasicMCMC</a><ul>
<li class="chapter" data-level="7.1" data-path="basicmcmc.html"><a href="basicmcmc.html#metropolis-hastings-update"><i class="fa fa-check"></i><b>7.1</b> Metropolis-Hastings Update</a><ul>
<li class="chapter" data-level="7.1.1" data-path="basicmcmc.html"><a href="basicmcmc.html#metropolis-update"><i class="fa fa-check"></i><b>7.1.1</b> Metropolis Update</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="basicmcmc.html"><a href="basicmcmc.html#the-gibbs-update"><i class="fa fa-check"></i><b>7.2</b> The Gibbs Update</a><ul>
<li class="chapter" data-level="7.2.1" data-path="basicmcmc.html"><a href="basicmcmc.html#variable-at-a-time-metropolis-hastings"><i class="fa fa-check"></i><b>7.2.1</b> Variable-at-a-Time Metropolis-Hastings</a></li>
<li class="chapter" data-level="7.2.2" data-path="basicmcmc.html"><a href="basicmcmc.html#the-gibbs-is-a-special-case-of-metropolis-hastings"><i class="fa fa-check"></i><b>7.2.2</b> The Gibbs is a special case of Metropolis-Hastings:</a></li>
<li class="chapter" data-level="7.2.3" data-path="basicmcmc.html"><a href="basicmcmc.html#gibbs-full-conditional-distibution"><i class="fa fa-check"></i><b>7.2.3</b> Gibbs Full conditional distibution</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="basicmcmc.html"><a href="basicmcmc.html#combining-updates"><i class="fa fa-check"></i><b>7.3</b> Combining Updates</a><ul>
<li class="chapter" data-level="7.3.1" data-path="basicmcmc.html"><a href="basicmcmc.html#composition"><i class="fa fa-check"></i><b>7.3.1</b> Composition</a></li>
<li class="chapter" data-level="7.3.2" data-path="basicmcmc.html"><a href="basicmcmc.html#palindromic-composition"><i class="fa fa-check"></i><b>7.3.2</b> Palindromic Composition</a></li>
<li class="chapter" data-level="7.3.3" data-path="basicmcmc.html"><a href="basicmcmc.html#state-independent-mixing"><i class="fa fa-check"></i><b>7.3.3</b> State-Independent Mixing</a></li>
<li class="chapter" data-level="7.3.4" data-path="basicmcmc.html"><a href="basicmcmc.html#subsampling"><i class="fa fa-check"></i><b>7.3.4</b> Subsampling</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="basicmcmc.html"><a href="basicmcmc.html#a-metropolis-example"><i class="fa fa-check"></i><b>7.4</b> A Metropolis Example</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html"><i class="fa fa-check"></i><b>8</b> Reversible Jump MCMC</a><ul>
<li class="chapter" data-level="8.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a><ul>
<li class="chapter" data-level="8.1.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#from-metropolis-hastings-to-reversible-jump"><i class="fa fa-check"></i><b>8.1.1</b> From Metropolis-Hastings to Reversible Jump</a></li>
<li class="chapter" data-level="8.1.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#application-area"><i class="fa fa-check"></i><b>8.1.2</b> Application Area</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#implementation"><i class="fa fa-check"></i><b>8.2</b> Implementation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#example-dimension-matching"><i class="fa fa-check"></i><b>8.2.1</b> Example Dimension Matching</a></li>
<li class="chapter" data-level="8.2.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#example-moment-matching-in-a-finite-mixture-of-univariate-normals"><i class="fa fa-check"></i><b>8.2.2</b> Example: Moment Matching in a Finite Mixture of Univariate Normals</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#mapping-functions-and-proposal-distribution"><i class="fa fa-check"></i><b>8.3</b> Mapping Functions and Proposal Distribution</a><ul>
<li class="chapter" data-level="8.3.1" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#marginalization-and-augmentation"><i class="fa fa-check"></i><b>8.3.1</b> Marginalization and augmentation:</a></li>
<li class="chapter" data-level="8.3.2" data-path="reversible-jump-mcmc.html"><a href="reversible-jump-mcmc.html#centering-and-order-methods"><i class="fa fa-check"></i><b>8.3.2</b> Centering and Order Methods</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html"><i class="fa fa-check"></i><b>9</b> Optimal Proposal Distributions and Adaptive MCMC</a><ul>
<li class="chapter" data-level="9.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#intro-1"><i class="fa fa-check"></i><b>9.1</b> Intro</a><ul>
<li class="chapter" data-level="9.1.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#mh-algorithm"><i class="fa fa-check"></i><b>9.1.1</b> MH algorithm</a></li>
<li class="chapter" data-level="9.1.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-scaling"><i class="fa fa-check"></i><b>9.1.2</b> Optimal Scaling</a></li>
<li class="chapter" data-level="9.1.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-mcmc"><i class="fa fa-check"></i><b>9.1.3</b> Adaptive MCMC</a></li>
<li class="chapter" data-level="9.1.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#comparing-markov-chains"><i class="fa fa-check"></i><b>9.1.4</b> Comparing Markov Chains</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-scaling-of-random-walk-metropolis"><i class="fa fa-check"></i><b>9.2</b> Optimal Scaling of Random-Walk Metropolis</a><ul>
<li class="chapter" data-level="9.2.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#basic-principle"><i class="fa fa-check"></i><b>9.2.1</b> Basic principle</a></li>
<li class="chapter" data-level="9.2.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#optimal-acceptance-rate-as-drightarrow-infty"><i class="fa fa-check"></i><b>9.2.2</b> Optimal Acceptance Rate as <span class="math inline">\(d\rightarrow \infty\)</span></a></li>
<li class="chapter" data-level="9.2.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#inhomogeneous-target-distributions"><i class="fa fa-check"></i><b>9.2.3</b> Inhomogeneous Target Distributions</a></li>
<li class="chapter" data-level="9.2.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#metropolis-adjusted-langevin-algorithm."><i class="fa fa-check"></i><b>9.2.4</b> Metropolis-Adjusted Langevin Algorithm.</a></li>
<li class="chapter" data-level="9.2.5" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#numerical-examples"><i class="fa fa-check"></i><b>9.2.5</b> Numerical Examples</a></li>
<li class="chapter" data-level="9.2.6" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#inhomogeneous-covariance"><i class="fa fa-check"></i><b>9.2.6</b> Inhomogeneous Covariance</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-mcmc-1"><i class="fa fa-check"></i><b>9.3</b> Adaptive MCMC</a></li>
<li class="chapter" data-level="9.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#ergodicity-of-adaptive-mcmc"><i class="fa fa-check"></i><b>9.4</b> Ergodicity of Adaptive MCMC</a><ul>
<li class="chapter" data-level="9.4.1" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-metropolis"><i class="fa fa-check"></i><b>9.4.1</b> Adaptive Metropolis</a></li>
<li class="chapter" data-level="9.4.2" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#adaptive-metropolis-within-gibbs"><i class="fa fa-check"></i><b>9.4.2</b> Adaptive Metropolis-within-Gibbs</a></li>
<li class="chapter" data-level="9.4.3" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#state-dependent-proposal-scalings"><i class="fa fa-check"></i><b>9.4.3</b> State-Dependent Proposal Scalings</a></li>
<li class="chapter" data-level="9.4.4" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#limit-theorem"><i class="fa fa-check"></i><b>9.4.4</b> Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#faq"><i class="fa fa-check"></i><b>9.5</b> FAQ</a></li>
<li class="chapter" data-level="9.6" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#conclusion"><i class="fa fa-check"></i><b>9.6</b> Conclusion</a></li>
<li class="chapter" data-level="9.7" data-path="optimal-proposal-distributions-and-adaptive-mcmc.html"><a href="optimal-proposal-distributions-and-adaptive-mcmc.html#a-tutorial-on-adaptive-mcmc"><i class="fa fa-check"></i><b>9.7</b> A tutorial on adaptive MCMC</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html"><i class="fa fa-check"></i><b>10</b> Hamiltonian Monte Carlo</a><ul>
<li class="chapter" data-level="10.0.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#properties-of-hamiltonian-dynamics"><i class="fa fa-check"></i><b>10.0.1</b> Properties of Hamiltonian Dynamics</a></li>
<li class="chapter" data-level="10.0.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#conservation-of-the-hamiltonian"><i class="fa fa-check"></i><b>10.0.2</b> Conservation of the Hamiltonian</a></li>
<li class="chapter" data-level="10.0.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#volume-preservation"><i class="fa fa-check"></i><b>10.0.3</b> Volume preservation</a></li>
<li class="chapter" data-level="10.0.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#symplecticness-"><i class="fa fa-check"></i><b>10.0.4</b> Symplecticness (辛？)</a></li>
<li class="chapter" data-level="10.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#discretizing-hamiltons-equations-the-leapfrog-method."><i class="fa fa-check"></i><b>10.1</b> Discretizing Hamilton’s Equations: The leapfrog method.</a><ul>
<li class="chapter" data-level="10.1.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#modification-of-eulers-method"><i class="fa fa-check"></i><b>10.1.1</b> Modification of Euler’s Method</a></li>
<li class="chapter" data-level="10.1.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-leapfrog-method"><i class="fa fa-check"></i><b>10.1.2</b> The leapfrog Method</a></li>
<li class="chapter" data-level="10.1.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#local-and-global-error-of-discretization-methods."><i class="fa fa-check"></i><b>10.1.3</b> Local and Global Error of discretization Methods.</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#mcmc-from-hamiltonian-dynamics."><i class="fa fa-check"></i><b>10.2</b> MCMC from Hamiltonian Dynamics.</a><ul>
<li class="chapter" data-level="10.2.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#probability-and-the-hamiltonian-canonical-distributions"><i class="fa fa-check"></i><b>10.2.1</b> Probability and the Hamiltonian: Canonical Distributions</a></li>
<li class="chapter" data-level="10.2.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-hamiltonian-monte-carlo-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> The Hamiltonian Monte Carlo Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#illustrations-of-hmc-and-its-benefits"><i class="fa fa-check"></i><b>10.2.3</b> Illustrations of HMC and Its Benefits</a></li>
<li class="chapter" data-level="10.2.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#the-benefit-of-avoiding-random-walks"><i class="fa fa-check"></i><b>10.2.4</b> The benefit of avoiding random walks</a></li>
<li class="chapter" data-level="10.2.5" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#sampling-from-a-100-dimensional-distribution"><i class="fa fa-check"></i><b>10.2.5</b> Sampling from a 100-Dimensional Distribution</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#hmc-in-practice-and-theory"><i class="fa fa-check"></i><b>10.3</b> HMC in Practice and Theory</a><ul>
<li class="chapter" data-level="10.3.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#effect-of-linear-transformation"><i class="fa fa-check"></i><b>10.3.1</b> Effect of Linear Transformation</a></li>
<li class="chapter" data-level="10.3.2" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#tuning-hmc"><i class="fa fa-check"></i><b>10.3.2</b> Tuning HMC</a></li>
<li class="chapter" data-level="10.3.3" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#combining-hmc-with-other-mcmc-updates"><i class="fa fa-check"></i><b>10.3.3</b> Combining HMC with Other MCMC Updates</a></li>
<li class="chapter" data-level="10.3.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#scaling-with-dimensionality"><i class="fa fa-check"></i><b>10.3.4</b> Scaling with Dimensionality</a></li>
<li class="chapter" data-level="10.3.5" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#hmc-for-hierarchical-models"><i class="fa fa-check"></i><b>10.3.5</b> HMC for Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#extensions-of-and-variations-on-hmc"><i class="fa fa-check"></i><b>10.4</b> Extensions of and Variations on HMC</a><ul>
<li class="chapter" data-level="10.4.1" data-path="hamiltonian-monte-carlo.html"><a href="hamiltonian-monte-carlo.html#discretization-by-splitting-handling-constraints-and-other-applications"><i class="fa fa-check"></i><b>10.4.1</b> Discretization by Splitting: Handling constraints and Other Applications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html"><i class="fa fa-check"></i><b>11</b> Bayes variable selection</a><ul>
<li class="chapter" data-level="11.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#prior-specification"><i class="fa fa-check"></i><b>11.1</b> Prior Specification</a></li>
<li class="chapter" data-level="11.2" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#summaries-the-posterior-distribution-and-model-averaged-inference"><i class="fa fa-check"></i><b>11.2</b> Summaries the posterior distribution and model averaged inference</a></li>
<li class="chapter" data-level="11.3" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#numerical-methods"><i class="fa fa-check"></i><b>11.3</b> Numerical Methods</a><ul>
<li class="chapter" data-level="11.3.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#empirical-bayes-by-marginal-maximum-likelihood"><i class="fa fa-check"></i><b>11.3.1</b> Empirical Bayes by Marginal Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#bayesian-asymptotically-analysis"><i class="fa fa-check"></i><b>11.4</b> Bayesian asymptotically analysis</a></li>
<li class="chapter" data-level="11.5" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#bayes-factor"><i class="fa fa-check"></i><b>11.5</b> Bayes factor</a><ul>
<li class="chapter" data-level="11.5.1" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#marginal-density-"><i class="fa fa-check"></i><b>11.5.1</b> Marginal density 居然可以这么来使</a></li>
<li class="chapter" data-level="11.5.2" data-path="bayes-variable-selection.html"><a href="bayes-variable-selection.html#section-11.5.2"><i class="fa fa-check"></i><b>11.5.2</b> 几个需要可能研究的玩意</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="advanced-r.html"><a href="advanced-r.html"><i class="fa fa-check"></i><b>12</b> Advanced R</a><ul>
<li class="chapter" data-level="12.0.1" data-path="advanced-r.html"><a href="advanced-r.html#vector"><i class="fa fa-check"></i><b>12.0.1</b> Vector</a></li>
<li class="chapter" data-level="12.0.2" data-path="advanced-r.html"><a href="advanced-r.html#types-and-tests"><i class="fa fa-check"></i><b>12.0.2</b> Types and tests:</a></li>
<li class="chapter" data-level="12.0.3" data-path="advanced-r.html"><a href="advanced-r.html#coercion"><i class="fa fa-check"></i><b>12.0.3</b> Coercion</a></li>
<li class="chapter" data-level="12.1" data-path="advanced-r.html"><a href="advanced-r.html#data.frame"><i class="fa fa-check"></i><b>12.1</b> Data.frame</a><ul>
<li class="chapter" data-level="12.1.1" data-path="advanced-r.html"><a href="advanced-r.html#ordering-integer-subsetting"><i class="fa fa-check"></i><b>12.1.1</b> Ordering (integer subsetting)</a></li>
<li class="chapter" data-level="12.1.2" data-path="advanced-r.html"><a href="advanced-r.html#calling-a-function-given-a-list-of-arguments"><i class="fa fa-check"></i><b>12.1.2</b> Calling a function given a list of arguments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="numeric-derivatives.html"><a href="numeric-derivatives.html"><i class="fa fa-check"></i><b>13</b> Numeric Derivatives</a></li>
<li class="chapter" data-level="14" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>14</b> Imputation</a></li>
<li class="chapter" data-level="15" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html"><i class="fa fa-check"></i><b>15</b> Simulation approach for computing the marginal likelihood</a><ul>
<li class="chapter" data-level="15.1" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#laplace-metropolis-approximation"><i class="fa fa-check"></i><b>15.1</b> Laplace-Metropolis approximation</a></li>
<li class="chapter" data-level="15.2" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#laplace-metropolis-approximation-1"><i class="fa fa-check"></i><b>15.2</b> Laplace-Metropolis approximation</a></li>
<li class="chapter" data-level="15.3" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#chibs-estimator-from-gibbss-sampling"><i class="fa fa-check"></i><b>15.3</b> Chib’s estimator from Gibbs’s sampling</a></li>
<li class="chapter" data-level="15.4" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#example-seemingly-unrelated-regression-model-with-informative-prior."><i class="fa fa-check"></i><b>15.4</b> Example: Seemingly unrelated regression model with informative prior.</a></li>
<li class="chapter" data-level="15.5" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#bridge-sampling-methods"><i class="fa fa-check"></i><b>15.5</b> Bridge sampling methods</a></li>
<li class="chapter" data-level="15.6" data-path="simulation-approach-for-computing-the-marginal-likelihood.html"><a href="simulation-approach-for-computing-the-marginal-likelihood.html#the-savage-dickey-density-ratio-approach"><i class="fa fa-check"></i><b>15.6</b> The savage-Dickey density ratio approach</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="r-web-scrape.html"><a href="r-web-scrape.html"><i class="fa fa-check"></i><b>16</b> R web scrape</a></li>
<li class="chapter" data-level="17" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html"><i class="fa fa-check"></i><b>17</b> Guide to Scientific Computing in C++</a><ul>
<li class="chapter" data-level="17.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#basics"><i class="fa fa-check"></i><b>17.1</b> Basics</a></li>
<li class="chapter" data-level="17.2" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#basics-in-c"><i class="fa fa-check"></i><b>17.2</b> Basics in C++</a></li>
<li class="chapter" data-level="17.3" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#redirect-console-output-to-file"><i class="fa fa-check"></i><b>17.3</b> Redirect Console Output to File</a><ul>
<li class="chapter" data-level="17.3.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#reading-from-the-command-line"><i class="fa fa-check"></i><b>17.3.1</b> Reading from the Command Line</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#pointer"><i class="fa fa-check"></i><b>17.4</b> Pointer</a></li>
<li class="chapter" data-level="17.5" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#functions"><i class="fa fa-check"></i><b>17.5</b> Functions</a><ul>
<li class="chapter" data-level="17.5.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#use-of-pointers-as-function-arguments."><i class="fa fa-check"></i><b>17.5.1</b> Use of Pointers as function arguments.</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#classess"><i class="fa fa-check"></i><b>17.6</b> Classess</a><ul>
<li class="chapter" data-level="17.6.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#header-files"><i class="fa fa-check"></i><b>17.6.1</b> Header Files</a></li>
</ul></li>
<li class="chapter" data-level="17.7" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#using-makefiles-to-compile-multiple-files"><i class="fa fa-check"></i><b>17.7</b> Using Makefiles to Compile Multiple Files</a></li>
<li class="chapter" data-level="17.8" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#section-17.8"><i class="fa fa-check"></i><b>17.8</b> 类的继承</a><ul>
<li class="chapter" data-level="17.8.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#-run-time-polymorphism"><i class="fa fa-check"></i><b>17.8.1</b> 继承类的实时多态 Run-Time Polymorphism</a></li>
</ul></li>
<li class="chapter" data-level="17.9" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#section-17.9"><i class="fa fa-check"></i><b>17.9</b> 模板</a><ul>
<li class="chapter" data-level="17.9.1" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#brief-survey-of-the-standard-template-library"><i class="fa fa-check"></i><b>17.9.1</b> Brief Survey of the Standard Template Library</a></li>
</ul></li>
<li class="chapter" data-level="17.10" data-path="guide-to-scientific-computing-in-c.html"><a href="guide-to-scientific-computing-in-c.html#class-for-linear-algebra"><i class="fa fa-check"></i><b>17.10</b> Class for linear algebra</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="rcpp.html"><a href="rcpp.html"><i class="fa fa-check"></i><b>18</b> Rcpp</a><ul>
<li class="chapter" data-level="18.1" data-path="rcpp.html"><a href="rcpp.html#rarmadillo"><i class="fa fa-check"></i><b>18.1</b> 一个R操作对应的armadillo操作的文档：</a></li>
<li class="chapter" data-level="18.2" data-path="rcpp.html"><a href="rcpp.html#rcpp-package"><i class="fa fa-check"></i><b>18.2</b> Rcpp package</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="statistical-computing.html"><a href="statistical-computing.html"><i class="fa fa-check"></i><b>19</b> Statistical Computing</a><ul>
<li class="chapter" data-level="19.1" data-path="statistical-computing.html"><a href="statistical-computing.html#generate-multivariate-normal-samples"><i class="fa fa-check"></i><b>19.1</b> Generate Multivariate Normal samples</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="r-trick.html"><a href="r-trick.html"><i class="fa fa-check"></i><b>20</b> R trick</a></li>
<li class="chapter" data-level="21" data-path="statistic-term.html"><a href="statistic-term.html"><i class="fa fa-check"></i><b>21</b> Statistic term</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes on Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hamiltonian-monte-carlo" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Hamiltonian Monte Carlo</h1>
<div id="properties-of-hamiltonian-dynamics" class="section level3">
<h3><span class="header-section-number">10.0.1</span> Properties of Hamiltonian Dynamics</h3>
<ol style="list-style-type: decimal">
<li>Hamiltonian dynamics is <em>reversible</em>. That is, the mapping <span class="math inline">\(T_{s}\)</span> from the state at time t,<span class="math inline">\((q(t),p(t))\)</span> to the state at time <span class="math inline">\(t+s,(q(t+s),p(t+s))\)</span>, is one-to-one, so hence there exists an inverse, <span class="math inline">\(T_{-s}\)</span>.</li>
</ol>
<p>可以从空间能量那一块来理解，由一个最普通的Hamilton的定义<span class="math inline">\(H(q, p)=U(q)+K(p)\)</span>,中，动能部分<span class="math inline">\(K(p)=K(-p)\)</span>,而由后文的<span class="math inline">\(K(p)==p^{T} M^{-1} p / 2\)</span>,则p是速度，K(p)是动能(kinetic energy)，所以和p的方向无关。所以inverse mapping也可以通过对negating p之后再作用<span class="math inline">\(T_s\)</span>,然后再negating p。</p>
<p>在一元的例子中，正逆变换都是逆时针转s radians。</p>
<p>而Hamiltonian dynamics的reversibility对MCMC更新非常重要，这样使用dynamics更新的MCMC就能使稳定分布是需要的分布。最简单证明稳定性的方法就是使用Reversible。</p>
</div>
<div id="conservation-of-the-hamiltonian" class="section level3">
<h3><span class="header-section-number">10.0.2</span> Conservation of the Hamiltonian</h3>
<p>A second property of the dynamics is that it keeps the Hamiltonian invariant (i.e. conserved).</p>
<p><span class="math display">\[
\frac{d H}{d t}=\sum_{i=1}^{d}\left[\frac{d q_{i}}{d t} \frac{\partial H}{\partial q_{i}}+\frac{d p_{i}}{d t} \frac{\partial H}{\partial p_{i}}\right]=\sum_{i=1}^{d}\left[\frac{\partial H}{\partial p_{i}} \frac{\partial H}{\partial q_{i}}-\frac{\partial H}{\partial q_{i}} \frac{\partial H}{\partial p_{i}}\right]=0
\]</span>
等式成立来源于Hamilton’s equations.</p>
<p>在一元的例子中，相当于是说旋转变换保持了Hamiltonian的值不变，是half the squared distance from the origin.</p>
<p>在Metropolis updates中，如果用 Hamiltonian dynamics作为proposal的话，也就是HMC，接受率是1如果H能保持invariant. 后面可以看到。但是在实践中只能保持H approximately invariant, 所以我们很难达到这个目标。</p>
</div>
<div id="volume-preservation" class="section level3">
<h3><span class="header-section-number">10.0.3</span> Volume preservation</h3>
<p>Hamiltonian dynamics的第三个基础性质是在(q,p)空间内他保持volume。（Liouville’s theorem的结论）。如果我们队映射<span class="math inline">\(T_s\)</span>作用到一个在（q,p）空间的区域R上的一些点，有体积V,则R在<span class="math inline">\(T_s\)</span>的像也有体积V。
在一维的例子中的体现就是映射只是旋转变化，明显不改变面基。当然也不改变区域的形状。后者则不总是成立。Hamiltonian dynamics 可能对区域在某个方向进行拉伸，当这个区域因为在其他方向非常集中，为了保证volume，所以进行拉伸。</p>
<p>在MCMC中，保证volume不变的意以不导致对于任意change in volume in the acceptance probability for Metropolis updates. 改变volume不会导致Metropolis updates的接受概率变化。如果我们考虑proposed新的状态，用一些随机的，非Hamiltonian，dynamics,我们可能会需要去计算Jacobian matrix for the mapping the dynamics defines的行列式，这可能做不到。</p>
<p>由之前的定义，divergence of the vector field have</p>
<p><span class="math display">\[
\sum_{i=1}^{d}\left[\frac{\partial}{\partial q_{i}} \frac{d q_{i}}{d t}+\frac{\partial}{\partial p_{i}} \frac{d p_{i}}{d t}\right]=\sum_{i=1}^{d}\left[\frac{\partial}{\partial q_{i}} \frac{\partial H}{\partial p_{i}}-\frac{\partial}{\partial p_{i}} \frac{\partial H}{\partial q_{i}}\right]=\sum_{i=1}^{d}\left[\frac{\partial^{2} H}{\partial q_{i} \partial p_{i}}-\frac{\partial^{2} H}{\partial p_{i} \partial q_{i}}\right]=0
\]</span></p>
<p>而有一个结论，A vector field with zero divergence can be shown to preserve volume.</p>
<p>如果不通过divergence，可以有一个不严格的证明。从之前提到的行列式的角度进行思考。</p>
<p>The volume preservation is equivalent to the determinant of the Jacobian matrix of <span class="math inline">\(T_s\)</span> having absolute value one, which is related to the well-known role of this determinant in regard to the effect of transformations on definite integrals and on probability density functions.</p>
<p>Jacobian matrix of <span class="math inline">\(T_s\)</span>, <span class="math inline">\(2d\times 2d\)</span>, as a mapping of <span class="math inline">\(z=(q,p)\)</span>, will be written as <span class="math inline">\(B_s\)</span>. In general, <span class="math inline">\(B_s\)</span> will depend on the values of <span class="math inline">\(q\)</span> and <span class="math inline">\(p\)</span> before the mapping. When <span class="math inline">\(B_s\)</span> is diagonal, it is easy to see that the absolute values of its diagonal elements are the factors by which <span class="math inline">\(T_s\)</span> wtrtches or compresses a region in each dimesnions, so that the product of these factos, which is equal to the absolute value of <span class="math inline">\(det(B_s)\)</span>, is the factor by which the volume of the region changes. Detail and general prove will not be put here, but, note that if rotate the coordinate system used, <span class="math inline">\(B_s\)</span> would no longer be diagonal, but the determinant of <span class="math inline">\(B_s\)</span> is invariant to such transformations, and so would still give the gactor by which the volume changes.</p>
<p>Consider Volume preservation for Hamiltonian dynamics in one dimension. Approximate <span class="math inline">\(T_\delta\)</span> for <span class="math inline">\(\delta\)</span> near zero as follows:</p>
<p><span class="math display">\[
T_{\delta}(q, p)=\left[ \begin{array}{c}{q} \\ {p}\end{array}\right]+\delta \left[ \begin{array}{l}{d q / d t} \\ {d p / d t}\end{array}\right]+\text { terms of order } \delta^{2} \text { or higher. }
\]</span>
Taking the time derivatives from the Hamiltonian equation, the Jacobian matrix can be written as
<span class="math display">\[
B_{\delta}=\left[ \begin{array}{cc}{1+\delta \frac{\partial^{2} H}{\partial q \partial p}} &amp; {\delta \frac{\partial^{2} H}{\partial p^{2}}} \\ {-\delta \frac{\partial^{2} H}{\partial q^{2}}} &amp; {1-\delta \frac{\partial^{2} H}{\partial p \partial q}}\end{array}\right]+\text { terms of order } \delta^{2} \text { or higher. }
\]</span></p>
<p>Then we can write the determinant of this matrix as</p>
<p><span class="math display">\[
\begin{aligned} \operatorname{det}\left(B_{\delta}\right) &amp;=1+\delta \frac{\partial^{2} H}{\partial q \partial p}-\delta \frac{\partial^{2} H}{\partial p \partial q}+\text { terms of order } \delta^{2} \text { or higher } \\ &amp;=1+\text { terms of order } \delta^{2} \text { or higher. } \end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\log (1+x) \approx x\)</span> for x near zero, <span class="math inline">\(log det (B_\delta)\)</span> is zero, except perhaps the terms of order <span class="math inline">\(\delta^2\)</span> or higher. Now consider <span class="math inline">\(log det (B_s)\)</span> for some time interval s that is not close to zero let <span class="math inline">\(\delta=s/n\)</span> (现在就close to zero了)，then可以把<span class="math inline">\(T_s\)</span>分解成作用n次<span class="math inline">\(T_\delta\)</span>, so <span class="math inline">\(det (B_s)\)</span> is the n-fold product of <span class="math inline">\(det(B_\delta)\)</span> evaluated at n points along the trajectory. Then we have</p>
<p><span class="math display">\[
\begin{aligned} \log \operatorname{det}\left(B_{S}\right) &amp;=\sum_{i=1}^{n} \log \operatorname{det}\left(B_{\delta}\right) \\ &amp;=\sum_{i=1}^{n}\left\{\text { terms of order } 1 / n^{2} \text { or smaller }\right\} \\ &amp;=\text { terms of order } 1 / n \text { or smaller. } \end{aligned}
\]</span>
所以对于不靠近0的<span class="math inline">\(B_s\)</span>, <span class="math inline">\(log det (B_s)\)</span>,也有log几乎为0当<span class="math inline">\(n\rightarrow 0\)</span>. 当然在sum过程中，<span class="math inline">\(B_\delta\)</span>的值可能会依赖于i，也就是轨迹上的点（p,q）的变化会影响<span class="math inline">\(T_s\)</span>. Assumeing that trajectories are not singular, the variation in <span class="math inline">\(B_\delta\)</span> must be bounded along any particular trajectory. （这个没懂，轨迹非退化，那么就有界然后就能通过n收敛？）
所以就preserves volume。</p>
<p>当高于一维的情况，Jacobian matrix有如下形式</p>
<p><span class="math display">\[
B_{8}=\left[ \begin{array}{c}{I+\delta\left[\frac{\partial^{2} H}{\partial q_{j} \partial p_{i}}\right]} &amp; {\delta\left[\frac{\partial^{2} H}{\partial p_{j} \partial p_{i}}\right]} \\ {-\delta\left[\frac{\partial^{2} H}{\partial q_{j} \partial q_{i}}\right]} &amp; {I-\delta\left[\frac{\partial^{2} H}{\partial p_{j} \partial q_{i}}\right]}\end{array}\right]+\text { terms of order } \delta^{2} \text { or higher. }
\]</span>
类似的形式，但是变成了分块矩阵。</p>
<p>高阶的项消掉了，剩下的项处理方式和1维的时候类似。</p>
</div>
<div id="symplecticness-" class="section level3">
<h3><span class="header-section-number">10.0.4</span> Symplecticness (辛？)</h3>
<p>volume不变性是Hamiltonian dynamics being symplectic的一个结论。假设<span class="math inline">\(z=(q,p)\)</span>, 定义J是<span class="math inline">\(J=\left[ \begin{array}{cc}{0_{d \times d}} &amp; {I_{d \times d}} \\ {-I_{d \times d}} &amp; {0_{d \times d}}\end{array}\right]\)</span>, symplecticness condition is that the Jacobian matrix, <span class="math inline">\(B_s\)</span>, of the mapping <span class="math inline">\(T_s\)</span> satisfies</p>
<p><span class="math display">\[
B_{s}^{T} J^{-1} B_{s}=J^{-1}
\]</span>
This implies volume conservation, since <span class="math inline">\(det(B^T_s)det(J^{-1})det(B_s)=det(J^{-1})\)</span> 则有<span class="math inline">\(det(B_s)^2=1\)</span>. 当d&gt;1的时候，symplecticness condition is stronger than volume preservation. Hamiltonian dynamics and the symplecticness condition can be generalized to where J is any matrix for which <span class="math inline">\(J^T=-J\)</span> and <span class="math inline">\(det(j)\neq 0\)</span>.</p>
<p>在实践中，Reversibility, preservation of volume, and symplecticness可以确实被保证，也必须被保证。</p>
</div>
<div id="discretizing-hamiltons-equations-the-leapfrog-method." class="section level2">
<h2><span class="header-section-number">10.1</span> Discretizing Hamilton’s Equations: The leapfrog method.</h2>
<p>为了在计算机中实现，Hamilton’s equations must be approximated by discretizing time, using some small stepsize,<span class="math inline">\(\epsilon\)</span>. 在0时刻开始，iteratively compute (approximately) the state at times <span class="math inline">\(\epsilon,2\epsilon,3\epsilon,etc.\)</span></p>
<p>In discussing how to do this, assume that the Hamiltonian has the form <span class="math inline">\(H(q,p)=U(q)+K(p)\)</span>. 虽然这个方法对各种定义的动能都适用，但是为了简化操作还是假设<span class="math inline">\(K(p)=p^{T} M^{-1} p / 2\)</span>. 然后M是diagonal的，对角元是<span class="math inline">\(m_1,...,m_d\)</span>, so that
<span class="math display">\[
K(p)=\sum_{i=1}^{d} \frac{p_{i}^{2}}{2 m_{i}}
\]</span>
相当于动能是每个方向上的动能之和。</p>
<div id="eulers-method" class="section level4">
<h4><span class="header-section-number">10.1.0.1</span> Euler’s Method</h4>
<p>最广为人知的逼近微分方程系统的解的方法是Euler’s method. 对于Hamilton’s equations, this method performs the following step, for each component of position and momentum, indexed by i=1,…,d:</p>
<p><span class="math display">\[
\begin{array}{l}{p_{i}(t+\varepsilon)=p_{i}(t)+\varepsilon \frac{d p_{i}}{d t}(t)=p_{i}(t)-\varepsilon \frac{\partial U}{\partial q_{i}}(q(t))} \\ {q_{i}(t+\varepsilon)=q_{i}(t)+\varepsilon \frac{d q_{i}}{d t}(t)=q_{i}(t)+\varepsilon \frac{p_{i}(t)}{m_{i}}}\end{array}
\]</span>
对于每个分量的速度和（势能？），下一时刻的等于这一时刻加上一阶导乘以步长，然后通过Hamilton’s equation就等于另外一个量的变化量。（减去的势能等于增加的动能？增加的动能等于减去的势能？）</p>
<p>对于一般的Euler法解的轨迹并不好,轨迹叉到无穷上了，但是真实的轨道是一个圆、</p>
</div>
<div id="modification-of-eulers-method" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Modification of Euler’s Method</h3>
<p><span class="math display">\[
\begin{array}{c}{p_{i}(t+\varepsilon)=p_{i}(t)-\varepsilon \frac{\partial U}{\partial q_{i}}(q(t))} \\ {q_{i}(t+\varepsilon)=q_{i}(t)+\varepsilon \frac{p_{i}(t+\varepsilon)}{m_{i}}}\end{array}
\]</span></p>
<p>变化是使用<span class="math inline">\(p_i(t+\epsilon)\)</span>代替了<span class="math inline">\(p_i(t)\)</span>。也就是说使用了“目前的动量”代替了前一时刻的动量。</p>
<p>图中显示了虽然不完美，但是这个方法更靠近了真实的轨迹。 事实上，修改后的方法确实确保了volue，这样帮助避免发散到无穷或者螺旋地回到起点，这样就让volume expand to infinity or contracting to zero.</p>
<p>要探究modification of Euler’s method preserve volume, exactly despite the finite discretization of time, 注意这两个变换 from <span class="math inline">\((q(t), p(t))\)</span> to <span class="math inline">\(q(t),p(t+\epsilon)\)</span> 通过modification的第一个等式，然后第二个等式是从<span class="math inline">\((q(t), p(t+\varepsilon))\)</span>变换到<span class="math inline">\((q(t+\varepsilon), p(t+\varepsilon))\)</span>. 这是“shear” transformation (剪羊毛？没懂)，每次变换只变换一个值，要么<span class="math inline">\(p_i\)</span>，要么<span class="math inline">\(q_i\)</span>,这样只会变一个参数，而any shear transformation will preserve volume,因为这样变换的Jacobian只有一个元，并且值为1.</p>
</div>
<div id="the-leapfrog-method" class="section level3">
<h3><span class="header-section-number">10.1.2</span> The leapfrog Method</h3>
<p>Leapfrog method可以得到更好的结果</p>
<p><span class="math display">\[
\begin{aligned} p_{i}(t+\varepsilon / 2) &amp;=p_{i}(t)-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}(q(t)) \\ q_{i}(t+\varepsilon) &amp;=q_{i}(t)+\varepsilon \frac{p_{i}(t+\varepsilon / 2)}{m_{i}} \\ p_{i}(t+\varepsilon) &amp;=p_{i}(t+\varepsilon / 2)-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}(q(t+\varepsilon)) \end{aligned}
\]</span>
从冲量对应的变量开始半步更新，然后再重新开始做全更新。使用新的（更新了半步的）冲量变量，然后最后做剩下半步冲量变量的更新，使用新的位置变量（<span class="math inline">\(q_i(t+\epsilon)\)</span>）。一个类似的方法可以对任何的动能函数成立，只需要用<span class="math inline">\(\partial K / \partial p_{i}\)</span>替换<span class="math inline">\(p_{i} / m_{i}\)</span>.
这个方法当然也保证了volume，因为第一个更新通过第三个更新是shear transformation，由于对称性，这也是reversible的by simply negating p again.</p>
</div>
<div id="local-and-global-error-of-discretization-methods." class="section level3">
<h3><span class="header-section-number">10.1.3</span> Local and Global Error of discretization Methods.</h3>
<p>How the error from discretizing the dynamics behaves in the limit as the stepsize, <span class="math inline">\(\epsilon\)</span>, goes to zero; Leimkuhler and Reich(2004) provide a much more detail discussion.</p>
<p>The error goes to zero as <span class="math inline">\(\epsilon\)</span> goes to zero, so that any upper limit on the error will apply (apart from a usually unknown constant factor) to any differentiable function of state. For example, if the error for (q,p) is no more than order <span class="math inline">\(\epsilon^2\)</span>, the error for <span class="math inline">\(H(q,p)\)</span> will also be no more than order <span class="math inline">\(\epsilon^2\)</span>.</p>
<p>The <em>local error</em> is the error after one step, that moves from time t to time <span class="math inline">\(t+\epsilon\)</span>. The global error is the error after simulating for some fixed time interval, s, which will require <span class="math inline">\(s/epsilon\)</span> steps.</p>
<p>If the local error is order <span class="math inline">\(\epsilon^p\)</span>, the global error will be order <span class="math inline">\(\epsilon^{p-1}\)</span>. If we instead fix <span class="math inline">\(\epsilon\)</span> and consider increasing the time, s, for which the trajectory is simulated, the error can in general increase exponentially with s. Interesting. However, this is often not what happens when simulating Hamiltonian dynamics with a symplectic method, as can be seen in Figure. The Euler method and its modification above have order <span class="math inline">\(\epsilon^2\)</span> local error and order <span class="math inline">\(\epsilon\)</span> global error. Leapfrog method has order <span class="math inline">\(\epsilon^3\)</span> local error and order <span class="math inline">\(\epsilon^2\)</span> global error. As shown by Leimkuhler and Reich, this difference is a consequence of leapfrog being reversible, since any reversible method must have global error that is of even order in <span class="math inline">\(\epsilon.\)</span></p>
</div>
</div>
<div id="mcmc-from-hamiltonian-dynamics." class="section level2">
<h2><span class="header-section-number">10.2</span> MCMC from Hamiltonian Dynamics.</h2>
<p>要用Hamiltonian dynamics去得到某个分布的sample需要把density function translate to potential energy function and introducing “momentum” variables to go with the original variables of interest (now seen as “position” variables). We can then simulate a Markov chain in which each iteration resamples the momentum and then does a Metropolis update with a proposal found using Hamiltonian dynamics.</p>
<div id="probability-and-the-hamiltonian-canonical-distributions" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Probability and the Hamiltonian: Canonical Distributions</h3>
<p>可以把要抽的分布函数和一个潜在的能量函数通过 <em>canonical distribution</em> 联系起来。给定某些能量函数,<span class="math inline">\(E(x)\)</span>,对于某些状态,x,x of some physical system, the canonical distribution over states has probability or probability density function</p>
<p><span class="math display">\[
P(x)=\frac{1}{Z}exp(\frac{-E(x)}{T})
\]</span>
Here, T is the temperature of the system (温度！这是要用上原子（量子？）领域的玩意了吗╮(╯_╰)╭)，比如弄个热力学定律啥的。。。。。或者说温度和动能有某种对应关系？
Z is normalizing constant needed for this function to sum or integrate to one. Viewing this the opposite way, if we are interested in some distribution with density function <span class="math inline">\(P(x)\)</span>, we can obtain it as a canonical distribution with <span class="math inline">\(T=1\)</span> by setting <span class="math inline">\(E(x)=-\log P(x)-\log Z\)</span>.</p>
<p>为什么这么写是因为，P（x）的值域是(0,1),那么logP(x)则是 <span class="math inline">\((-\infty,0)\)</span>,
<span class="math inline">\(-\log P(x)-\log Z\)</span> 则是<span class="math inline">\((-\log Z,\infty)\)</span>, as long as Z is any convenient positive constant, <span class="math inline">\(-\log Z\)</span>可以达到<span class="math inline">\(-\infty\)</span>,所以能量函数的值域就是<span class="math inline">\((-\infty,\infty)\)</span>. 或者反过来说就是对能量函数来个log link function让他落到概率区间<span class="math inline">\((-1,1)\)</span>.</p>
<p>A Hamiltonian is an energy function for the joint state of “position”, q, and “momentum”,p, and so defines a joint distribution for them as follows:</p>
<p><span class="math display">\[
P(q,p)=\frac{1}{Z}exp(\frac{-H(q,p)}{T})
\]</span></p>
<p>所以一个Hamiltonian就是对当前位置的能量函数q，加上动量函数，p组合起来。所以可以定义一个联合分布如上。
Note that the invariance of H under Hamiltonian dynamics means that a Hamiltonian trajectory will (if simulated exactly) move within a hypersurface of cosntant probability density.
所以这里的Hamiltonian dynamics的不变形就表现在一个Hamiltonian 轨道将会在等常数概率的超平面上移动。
If <span class="math inline">\(H(q,p)=U(q)+K(p)\)</span>, the joint density is
<span class="math display">\[
P(q,p)=\frac{1}{Z}exp(\frac{-U(q)}{T})exp(\frac{-K(p)}{T})
\]</span>
and we see that q and p are independent, and each have canonical distributions, with energy functions <span class="math inline">\(U(q)\)</span> and <span class="math inline">\(K(p)\)</span>. We will use <span class="math inline">\(q\)</span> to represent the variables of interest, and introduce p just to allow Hamiltonian dynamics to operate.</p>
<p>所以在加情况下的Hamiltonian，就能分成两个分布，把要抽的分布丢进q，然后找一个能形成Hamiltonian dynamics的q就行了。</p>
<p>在Bayesian统计中，模型参数的后验分布是一般比较关心的，所以这些参数会作为位置，q。把后验分布转化成canonical distribution (with T=1) using a potential energy function defined as
<span class="math display">\[
U(q)=-\log [\pi(q) L(q | D)],
\]</span>
where <span class="math inline">\(\pi(q)\)</span> is the prior density, and <span class="math inline">\(L(q|D)\)</span> is the likelihood function given data D.</p>
</div>
<div id="the-hamiltonian-monte-carlo-algorithm" class="section level3">
<h3><span class="header-section-number">10.2.2</span> The Hamiltonian Monte Carlo Algorithm</h3>
<p>下一步就进入HMC。
HMC仅限于<span class="math inline">\(\mathbb R^d\)</span>上的连续分布，并且density function can be evaluated(perhaps up to an unknown normalizing constant). For the moment, It will also assume that the density is nonzero everywhere (but this is relaxed in following section). We must also be able to compute the partial derivatives of the log of the density function. These derivatives must therefore exist, except perhaps on a set of point with probability zero, for shich some arbitrary value could be returned.
条件，要求还行？连续，可以算density，可以算log density的偏导。不可导的点的概率测度应为0.
HMC samples from the canonical distribution （就是上式P(q,p)。）q是the distribution of interest, as specified using the potential energy function <span class="math inline">\(U(q)\)</span>. 选择distribution of momentum variable,p, 独立于q,然后给定动能函数,K(p). 目前应用在HMC上的主要是二次能量函数，就之前提起来那个，可以让p是0均值的多元正态分布。最经常的设置是the components of p are specified to be independent, with component i having variance <span class="math inline">\(m_i\)</span>. The kinetic energy function producing this distribution (Setting T=1) is
<span class="math display">\[
K(p)=\sum_{i=1}^{d} \frac{p_{i}^{2}}{2 m_{i}}.
\]</span>
在5.4会讲<span class="math inline">\(m_i\)</span>的选取对表现的影响。</p>
<div id="the-two-steps-of-the-hmc-algorithm" class="section level4">
<h4><span class="header-section-number">10.2.2.1</span> The two steps of the HMC algorithm</h4>
<p>每次HMC算法的迭代有两部。第一步only the momentum; the second may change both position and momentum. 第二步 may change both position and momentum. Both steps leave the canonical joint distribution of <span class="math inline">\((q,p)\)</span> invariant, and hence their combination also leaves this distribution invariant.</p>
<p>In the first step, new values for the momentum variables are randomly drawn from their Gaussian distribution, independently of the current values of the position variables. 由上一步的假设，有
<span class="math display">\[
p_i\sim N(0,m_i)
\]</span>
因为q没变，p来自于正确的条件分布(conditional on q,但是独立，所以无所谓)。所以这一步明显让canonical joint distribution invariant.
第二步则用Metropolis update,using Hamiltonian dynamics to propose a new state. Starting at current state (q,p), Hamiltonian dynamics is simulated for L steps using the leapfrog method (or other reversible method that preserves volume，所以基础的Euler法不行？因为没有preserve reversible), with stepsize of <span class="math inline">\(\epsilon\)</span>. 这里，L和<span class="math inline">\(\epsilon\)</span>是模型参数，需要找一个能有好表现的。The momentum variables at the end of this L-step trajectory are then negated, giving a proposed state<span class="math inline">\((q^*,p^*)\)</span>. This proposed state is accepted at the next state of the Markov chain with probability
<span class="math display">\[
\min \left[1, \exp \left(-H\left(q^{*}, p^{*}\right)+H(q, p)\right)\right]=\min \left[1, \exp \left(-U\left(q^{*}\right)+U(q)-K\left(p^{*}\right)+K(p)\right)\right]
\]</span></p>
<p>The negation of the momentum variables at the end of the trajectory makes the Metropolis proposal symmetrical, as needed for the acceptance probability above to be valid. This negation need not be done in practice, since <span class="math inline">\(K(p)=K(-p)\)</span>, and the momentum will be replaced before it is used again.(第一步得重新抽。)</p>
<p>所以HMC可以是看做从q和p的联合分布中抽，Metropolis步用了Hamiltonian dynamics的proposal，使得（q,p）的概率分布保持不变，或者几乎不变。移动到（q,p）这一点只靠第一步的随机抽。对于q，Hamiltonian dynamics for <span class="math inline">\((q,p)\)</span>可以产生q的值来自于不同的概率分布，或者等价于一个很不一样的potential energy，<span class="math inline">\(U(q)\)</span>.但是，重抽样动量变量依然很难得到q的一个合适的分布，不用重抽样，<span class="math inline">\(H(q, p)=U(q)+K(p)\)</span>会是（几乎）常数，而<span class="math inline">\(K(p)\)</span> and <span class="math inline">\(U(q)\)</span>是非负的，那么U(q)将不会超出H(q,p)的初始值，如果没有对p的resampling。</p>
<blockquote>
<p>所以这里的resampling就是指对p的正态抽样？
从他的代码来看，negate momentum的意思就是让p=-p,所以为什么要这样好像提了好几次但是没懂，硕士要让proposal symmetric? 为啥要这样啊</p>
</blockquote>
<pre class="sourceCode r"><code class="sourceCode r">HMC =<span class="st"> </span><span class="cf">function</span> (U, grad_U, epsilon, L, current_q)
  {
    q =<span class="st"> </span>current_q
    p =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(q),<span class="dv">0</span>,<span class="dv">1</span>)  <span class="co"># independent standard normal variates</span>
    current_p =<span class="st"> </span>p
    <span class="co"># Make a half step for momentum at the beginning</span>
    p =<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span><span class="kw">grad_U</span>(q) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>
    <span class="co"># Alternate full steps for position and momentum</span>
    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L)
    {
      <span class="co"># Make a full step for the position</span>
      q =<span class="st"> </span>q <span class="op">+</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span>p
      <span class="co"># Make a full step for the momentum, except at end of trajectory</span>
      <span class="cf">if</span> (i<span class="op">!=</span>L) p =<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span><span class="kw">grad_U</span>(q)
}
    <span class="co"># Make a half step for momentum at the end.</span>
    p =<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>epsilon <span class="op">*</span><span class="st"> </span><span class="kw">grad_U</span>(q) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>
    <span class="co"># Negate momentum at end of trajectory to make the proposal symmetric</span>
    p =<span class="st"> </span><span class="op">-</span>p
    <span class="co"># Evaluate potential and kinetic energies at start and end of trajectory</span>
    current_U =<span class="st"> </span><span class="kw">U</span>(current_q)
    current_K =<span class="st"> </span><span class="kw">sum</span>(current_pˆ2) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>
    proposed_U =<span class="st"> </span><span class="kw">U</span>(q)
    proposed_K =<span class="st"> </span><span class="kw">sum</span>(pˆ2) <span class="op">/</span><span class="st"> </span><span class="dv">2</span>
    <span class="co"># Accept or reject the state at end of trajectory, returning either</span>
    <span class="co"># the position at the end of the trajectory or the initial position</span>
    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span><span class="kw">exp</span>(current_U<span class="op">-</span>proposed_U<span class="op">+</span>current_K<span class="op">-</span>proposed_K))
    {
      <span class="kw">return</span> (q)  <span class="co"># accept</span>
    }
<span class="cf">else</span> {
      <span class="kw">return</span> (current_q)  <span class="co"># reject</span>
    }
}</code></pre>
</div>
<div id="proof-that-hmc-leaves-the-canonical-distribution-invariant." class="section level4">
<h4><span class="header-section-number">10.2.2.2</span> Proof that HMC leaves the canonical distribution Invariant.</h4>
<p>首先是证Metropolis update是reversible的，并且收敛到canonical distribution function for q and p.这是detail balance condition
思路是<span class="math inline">\(A_k\)</span> 是<span class="math inline">\((q,p)\)</span> 空间的一个分割，那么用Hamilton dynamic的Metropolis update就能把<span class="math inline">\(A_k\)</span>映到<span class="math inline">\(B_k\)</span>,由于leapfrog steps的reversibility，所以<span class="math inline">\(B_k\)</span>也是<span class="math inline">\((q,p)\)</span>空间的一个分割，并且由于保volume,所以<span class="math inline">\(A_k\)</span>的volume和<span class="math inline">\(B_k\)</span>的相等。则有下式</p>
<p><span class="math display">\[
P\left(A_{i}\right) T\left(B_{j} | A_{i}\right)=P\left(B_{j}\right) T\left(A_{i} | B_{j}\right)
\]</span></p>
<p>由于当<span class="math inline">\(i\neq j\)</span>时有，<span class="math inline">\(T\left(A_{i} | B_{j}\right)=T\left(B_{j} | A_{i}\right)=0\)</span>,而当<span class="math inline">\(i=j\)</span>时，由于reversibility，所以成立。故满足detail-balance condition.
当<span class="math inline">\(A_k\)</span>和<span class="math inline">\(B_k\)</span>越来越小的时候，Hamiltonian变成在每个区域几乎是常数。令这个数是<span class="math inline">\(H_X\)</span>在区域<span class="math inline">\(X\)</span>中。同安G的，那么canonical probability density and the transition probabilities become effectively constant within each region as well.
那么上式就能变成
<span class="math display">\[
\frac{V}{Z} \exp \left(-H_{A_{k}}\right) \min \left[1, \exp \left(-H_{B_{k}}+H_{A_{k}}\right)\right]=\frac{V}{Z} \exp \left(-H_{B_{k}}\right) \min \left[1, \exp \left(-H_{A_{k}}+H_{B_{k}}\right)\right]
\]</span></p>
<p>这个式子更直接，因为保Volme，所以H不会变，都相等。</p>
<p>Detail-balance implies Metropolis update leaves the canonical distribution for q and p invariant.</p>
<p>下一步是说每一步的概率<span class="math inline">\(P({B_k})\)</span>都相等，</p>
<p><span class="math display">\[
\begin{aligned} P\left(B_{k}\right) R\left(B_{k}\right)+\sum_{i} P\left(A_{i}\right) T\left(B_{k} | A_{i}\right) &amp;=P\left(B_{k}\right) R\left(B_{k}\right)+\sum_{i} P\left(B_{k}\right) T\left(A_{i} | B_{k}\right) \\ &amp;=P\left(B_{k}\right) R\left(B_{k}\right)+P\left(B_{k}\right) \sum_{i} T\left(A_{i} | B_{k}\right) \\ &amp;=P\left(B_{k}\right) R\left(B_{k}\right)+P\left(B_{k}\right)\left(1-R\left(B_{k}\right)\right) \\ &amp;=P\left(B_{k}\right) \end{aligned}
\]</span>
The Metropolis update within HMC therefore leaves the canonical distribution invariant.</p>
</div>
<div id="ergodicity-of-hmc" class="section level4">
<h4><span class="header-section-number">10.2.2.3</span> Ergodicity of HMC</h4>
<p>Typically, HMC will also be “ergodic”. Any value can be sampled for the momentum variables, which can then affect the position variables in arbitrary ways.
However, ergodicity can fail if the L leapfrog steps in a trajectory produce an exact periodicity for some function of state. The one-dimensional example solutions are periodic with period <span class="math inline">\(2\pi\)</span>. When <span class="math inline">\(L\epsilon\)</span> is exactly <span class="math inline">\(2\pi\)</span>, HMC may return to the same possition coordinate.
For nearby values of <span class="math inline">\(L\)</span> and <span class="math inline">\(\epsilon\)</span>, HMC may theoretically ergodic, but take very long time to move about the full state space.</p>
<p>The potential nonergodicity problem can be solved by random choosing <span class="math inline">\(\epsilon\)</span> or L in a fairly small interval. Although in real problems interactions between variables typically prevent any exact periodicities from occurring, near periodicities might still slow HMC considerably.</p>
</div>
</div>
<div id="illustrations-of-hmc-and-its-benefits" class="section level3">
<h3><span class="header-section-number">10.2.3</span> Illustrations of HMC and Its Benefits</h3>
<p>Now illustrate some practical issues with HMC.</p>
<div id="trajectories-for-a-two-dimensional-problem" class="section level4">
<h4><span class="header-section-number">10.2.3.1</span> Trajectories for a Two-Dimensional Problem</h4>
<p>Example 1: Sampling from two variables which is bivariate Gaussian, mean of zero, standard deviations of one, and correlation 0.95. Regard this as “position” variables, and introduce two corresponding “momentum” variables which is also Gaussian (as previous demonstrate), with mean 0 and 1 sd, and 0 correlation. Then the Hamiltonian can be define as</p>
<p><span class="math display">\[
H(q, p)=q^{T} \Sigma^{-1} q / 2+p^{T} p / 2, \quad \text { with } \Sigma=\left[ \begin{array}{cc}{1} &amp; {0.95} \\ {0.95} &amp; {1}\end{array}\right]
\]</span></p>
</div>
</div>
<div id="the-benefit-of-avoiding-random-walks" class="section level3">
<h3><span class="header-section-number">10.2.4</span> The benefit of avoiding random walks</h3>
<p>最大的好处，HMC会几乎固定的朝一个方向走</p>
</div>
<div id="sampling-from-a-100-dimensional-distribution" class="section level3">
<h3><span class="header-section-number">10.2.5</span> Sampling from a 100-Dimensional Distribution</h3>
</div>
</div>
<div id="hmc-in-practice-and-theory" class="section level2">
<h2><span class="header-section-number">10.3</span> HMC in Practice and Theory</h2>
<p>Requires proper tuning of <span class="math inline">\(L\)</span> and <span class="math inline">\(\epsilon\)</span>. Then the tuning will be discussed below. How performance can be improved by using whatever knowledge is available regarding the scales of variables and their correlations. An additional benefit of HMC- its better scaling with dimensionality than simple Metropolis methods.</p>
<div id="effect-of-linear-transformation" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Effect of Linear Transformation</h3>
<p>If the variables beging sampled are transformed by multiplication by some nonsingular matrix, A. However, performance stays the same (except perhaps in terms of computation time per iteration), if at the same time the corresponding momentum variables are multiplied by <span class="math inline">\((A^T)^{-1}\)</span>. These facts provide insight into the operation of HMC. Also can help us improve performance when we have some knowledge of the scales and correlations of the variables.</p>
<p>Let new variables be <span class="math inline">\(q&#39;=Aq\)</span>. Then <span class="math inline">\(P&#39;(q&#39;)=P(A^{-1}q&#39;)/|det(A)|\)</span>, where <span class="math inline">\(P(q)\)</span> is the density for q. If the distribution for q is the canonical distribution for a potential energy function <span class="math inline">\(U(q)\)</span>, we can obtain the distribution for <span class="math inline">\(q&#39;\)</span> as the canonical distribution for <span class="math inline">\(U&#39;(q&#39;)=U(A^{-1}q&#39;)\)</span>. 因为<span class="math inline">\(|det(A)|\)</span> 是一个常数，我们需要不把log|deta(A)| 放到potential energy里面。</p>
<p>用之前的动能函数，但是把动量变量变成<span class="math inline">\(p&#39;=(A^T)^{-1}p\)</span>,然后新的动能方程是<span class="math inline">\(K&#39;(p&#39;)=K(A^Tp&#39;)\)</span>. 如果使用二次型的动能,<span class="math inline">\(K(p)=p^{T} M^{-1} p / 2\)</span>,则新的动能是</p>
<p><span class="math display">\[
K^{\prime}\left(p^{\prime}\right)=\left(A^{T} p^{\prime}\right)^{T} M^{-1}\left(A^{T} p^{\prime}\right) / 2=\left(p^{\prime}\right)^{T}\left(A M^{-1} A^{T}\right) p^{\prime} / 2=\left(p^{\prime}\right)^{T}\left(M^{\prime}\right)^{-1} p^{\prime} / 2
\]</span>
其中，<span class="math inline">\(M^{\prime}=\left(A M^{-1} A^{T}\right)^{-1}=\left(A^{-1}\right)^{T} M A^{-1}\)</span>.</p>
<p>如果我哦们用动能变量的变形，则Hamiltonian Dynamics对于新的变量,<span class="math inline">\((q&#39;,p&#39;)\)</span>,足够重复原始的dynamics（对于(q,p)）的，所以HMC的表现应该是一样的。为了证明这个，假设我们有对于<span class="math inline">\((q&#39;,p&#39;)\)</span>的Hamiltonian dynamics,结果对于原来的变量则是</p>
<p><span class="math display">\[
\begin{aligned} \frac{d q}{d t} &amp;=A^{-1} \frac{d q^{\prime}}{d t}=A^{-1}\left(M^{\prime}\right)^{-1} p^{\prime}=A^{-1}\left(A M^{-1} A^{T}\right)\left(A^{T}\right)^{-1} p=M^{-1} p \\ \frac{d p}{d t} &amp;=A^{T} \frac{d p^{\prime}}{d t}=-A^{T} \nabla U^{\prime}\left(q^{\prime}\right)=-A^{T}\left(A^{-1}\right)^{T} \nabla U\left(A^{-1} q^{\prime}\right)=-\nabla U(q) \end{aligned}
\]</span>
结果和原始的Hamiltonian dynamics for (q,p)一样。</p>
<p>如果A是正交矩阵，<span class="math inline">\(A^{-1}=A^{T}\)</span>,则HMC的performance不会变，如果对q 和 p 乘以A。
如果我们给动量选择了一个旋转对称分布，with <span class="math inline">\(M=mI\)</span>.(momentum variables are independent, each having variance m).这些变换不会对能量函数造成变化，所以也不会对momentum的distribution造成变化。因为我们有<span class="math inline">\(M^{\prime}=\left(A(m I)^{-1} A^{T}\right)^{-1}=m I\)</span>.</p>
<p>如果RWM的proposal也有rotation symmetric的性质（比如Gaussian with covariance matrix mI）. Gibbs 则没有rotationally invariant. 但是，Gibbs对某个变量的rescaling是不变的。 Gibbs sampling is not rotationally invariant, nor is a scheme in which the Metropolis algorithm is used to update each variable in turn. However, Gibbs sampling is invariant to rescaling of the variables(transformation by a diagonal matrix), which is not true for HMC or random-walk Metropolis, unless the kinetic energy or proposal distribution is transformed in a corresponding way.</p>
<p>Suppose that we have estimate, <span class="math inline">\(\Sigma\)</span>, of the covariance matrix for q, and suppose also that q has at least a roughly Gaussian distribution. How can we use this information to improve the performance of HMC? One way is to transform the variables so that their covariance matrix is close to the identity, by finding the Cholesky decomposition, <span class="math inline">\(\Sigma=LL^T\)</span>, which L being lower-triangular, and letting <span class="math inline">\(q&#39;=L^{-1}q\)</span>. We then let our kinetic energy function be <span class="math inline">\(K(p)=p^Tp/2\)</span>.</p>
<p>Since the momentum variables are independent, and the position variables are close to independent with variances close to one, HMC should perform well using trajectories with a small number of leapfrog steps, which will move all variables to a nearly independent point More realistically, the estimate <span class="math inline">\(\Sigma\)</span> may not be very good, but this transformation could still improve performance compared to using the same kinetic energy with the original q variables.</p>
<p>An equivalent way to make use of the estimated covariance <span class="math inline">\(\Sigma\)</span> is to keep the original <span class="math inline">\(q\)</span> variables, but use the kinetic energy function <span class="math inline">\(K(p)=p^{T} \Sigma p / 2\)</span>, that is, we let the momentum variables have covariance <span class="math inline">\(\Sigma^{-1}\)</span>. The equivalence can be seen by transforming this kinetic energy to correspond to a transformation to <span class="math inline">\(q&#39;=L^{-1}q\)</span>, which gives <span class="math inline">\(K(p&#39;)=(p&#39;)^TM&#39;^{-1}p&#39;\)</span> with <span class="math inline">\(M^{\prime}=\left(L^{-1}\left(L L^{T}\right)\left(L^{-1}\right)^{T}\right)^{-1}=I\)</span>.</p>
<p>也就是说，如果我们知道position的covariance，那么通过线性变换把这个相关性除掉，然后再有动量变量设为独立的，那么就是两个独立变量的抽，这样HMC的trajectories就只用很小的leapfrog step就能遍历附近的独立的点。就算<span class="math inline">\(\Sigma\)</span>的估计不好，但是这个变换也能提高HMC的表现效果。</p>
<p>这个方法的限制是当矩阵很大的时候，矩阵运算很花时间。
所以对于高维问题一般用对角的covariance matrix <span class="math inline">\(\Sigma\)</span> ,相当于只考虑variable的scale而不考虑之间的相关性。</p>
</div>
<div id="tuning-hmc" class="section level3">
<h3><span class="header-section-number">10.3.2</span> Tuning HMC</h3>
<p>一个HMC应用的阻碍是需要选择合适的Leapfrog step, <span class="math inline">\(\epsilon\)</span>, 以及Leapfrog steps的迭代次数，L。这两个参数共同决定了轨道的虚时间。大部分mcmc方法都有参数需要tuned，除了Gibbs，推出来全条件分布以后就能直接抽了。但是，tuning HMC比起其他简单的Metropolis会更难。</p>
<div id="preliminary-runs-and-trace-plots" class="section level4">
<h4><span class="header-section-number">10.3.2.1</span> Preliminary Runs and Trace Plots</h4>
<p>要Tuning HMC一般需要先尝试性的run几次去寻找<span class="math inline">\(\epsilon\)</span>和L。为了判断这几次run的效果如何， trace plots of quantities that are thought to be indicative of overall convergence should be examined. 但是尝试性的run也可能会造成misleading。因为<span class="math inline">\(\epsilon\)</span>和<span class="math inline">\(L\)</span>的最佳选择不一定是第一次达到平稳的时候的选择。如果必要的话，HMC的每一步迭代中，<span class="math inline">\(\epsilon\)</span>和 <span class="math inline">\(L\)</span>都进行随机选取。
建议多跑跑几次基于不同的随机性设置，这样比较隔绝的极大值点就更容易被探测到。和其他所有MCMC方法相比，HMC不会更少（或者更多）容易被隔离的极值点所影响。如果找到了有隔离的极值点存在，需要做一些事情去处理这个问题。只是把约束在单个极值点的链组合起来是不可取的，可以尝试使用沿着解集轨道“tempering”回火结合的HMC进行处理多个极值点问题。</p>
</div>
<div id="what-stepsize" class="section level4">
<h4><span class="header-section-number">10.3.2.2</span> What Stepsize?</h4>
<p>选取一个合适的leapfrog stepsize <span class="math inline">\(\epsilon\)</span>也是很重要的。 太大的stepsize会导致很低的接受率，太小的stepsize会浪费很多计算时间。</p>
<p>幸运的是，stepsize的选择几乎与要走多少步(L)独立。Hamiltonian的误差，基本上不会因为多走几步而增加，所以stepsize如果足够小的话，这个dynamics会很稳定。</p>
<p>Stability的问题可以从一个一维的问题中看出
<span class="math display">\[
H(q, p)=\frac{q^{2}}{2 \sigma^{2}}+\frac{p^{2}}{2}
\]</span>
q的分布是<span class="math inline">\(N(0,\sigma^2)\)</span> ,对这个系统的leapfrog step是一个线性映射从<span class="math inline">\((q(t),p(t))\)</span>到<span class="math inline">\((q(t+\epsilon),p(t+\epsilon))\)</span>. 从之前Leapfrog方法的定义式中可以dehumidifier，这个线性变换可以表述成如下的矩阵形式：</p>
<p><span class="math display">\[
\left[ \begin{array}{c}{q(t+\varepsilon)} \\ {p(t+\varepsilon)}\end{array}\right]=\left[ \begin{array}{cc}{1-\varepsilon^{2} / 2 \sigma^{2}} &amp; {\varepsilon} \\ {-\varepsilon / \sigma^{2}+\varepsilon^{3} / 4 \sigma^{4}} &amp; {1-\varepsilon^{2} / 2 \sigma^{2}}\end{array}\right] \left[ \begin{array}{l}{q(t)} \\ {p(t)}\end{array}\right]
\]</span>
所以这个映射到底是会有一个稳定的轨道呢，还是会发散到无穷，由这个线性变换的矩阵的特征值所决定，而这个特征是
<span class="math display">\[
\left(1-\frac{\varepsilon^{2}}{2 \sigma^{2}}\right) \pm\left(\frac{\varepsilon}{\sigma}\right) \sqrt{\varepsilon^{2} / 4 \sigma^{2}-1}
\]</span>
当<span class="math inline">\(\frac{\epsilon}{\sigma}&gt;2\)</span>时，特征值是实数，所以存在至少一个绝对值大于1的根。此时，解的轨道使用Leapfrod 方法用这个<span class="math inline">\(\epsilon\)</span>算就会unstable。当<span class="math inline">\(\epsilon/\sigma&lt;2\)</span>时，特征值是复数，然后有如下性质：
<span class="math display">\[
\left(1-\frac{\varepsilon^{2}}{2 \sigma^{2}}\right)^{2}+\left(\frac{\varepsilon^{2}}{\sigma^{2}}\right)\left(1-\frac{\varepsilon^{2}}{4 \sigma^{2}}\right)=1
\]</span>
所以Trajectory此时就是stable的。</p>
<p>对于多元的问题，动能函数选择<span class="math inline">\(K(p)=p^Tp/2\)</span>，则stability的限制就会由distribution的宽度，在最受限制的方向。比如说对于Gaussian分布，这个东西将会是q的协方差矩阵最小特征值的平方根。而如果使用带质量阵的动能函数<span class="math inline">\(K(p)=p^{T} M^{-1} p / 2\)</span>,则可以使用之前提到的线性变换的方法变成上面这种情况再去求特征值。</p>
<p>如果使用一个会产生不稳定轨道的<span class="math inline">\(\epsilon\)</span>,那么H的值就会随着L以指数级增大，并且会导致接受概率极端的小。对于低维的问题，使用一个稍微比stability limit小的<span class="math inline">\(\epsilon\)</span>也足够能得到一个比较好的接受率。但是对于高维就不一样，stepsize可能需要reduced further than this才能让error in H to a level that produces a good acceptance probability.</p>
<p>如果使用过大的<span class="math inline">\(\epsilon\)</span>,那么可能会对HMC的表现产生非常不好的影响。在这种情况下，HMC会对tuning更加的敏感，甚至超过RWM。RWM需要选择一个scaling，作为random walk的deviation，但是表现的变化是相对很光滑的，而不像HMC当<span class="math inline">\(\epsilon\)</span>超出stability limit的范围之后，会急剧的下降。（但是高维的情况RWM也会变化的很剧烈，所以这个区别并不是很明显）。</p>
<p>当Stepsize太大导致的HMC表现急剧下降不是一个很重要的问题如果stability limit是一个常数，这个问题很明显来自于实验性的run，所以可以被修复。Real danger是stability limit may differ for several regions of the state space that all have substantial probability. 当preliminary runs开始于一个stability limit很大的区域，那么去一个稍微小一点的<span class="math inline">\(\epsilon\)</span>可能会比较合适。但是如果对于其他区域这个<span class="math inline">\(\epsilon\)</span>还是超过了stability limit，那么可能HMC永远不会访问这个区域，甚至这个区域有不可忽略的概率也不行，会导致非常严重的错误。为什么这会发生呢，考虑如果run ever does visit the region where the chosen <span class="math inline">\(\epsilon\)</span> would produce instability, it wills tay there for a very long time, since the acceptance probability with that <span class="math inline">\(\epsilon\)</span> will be very small.如果run到了这个区域，会因为接受概率一直很小所以就呆在这里很长时间。尽管HMC产出的是稳定分布，但是基本上不会从一个<span class="math inline">\(\epsilon\)</span>是stability的移动的这边。一个例子是如果从一个very light tails的分布进行抽样，（lighter than Gaussian distribution）,for which the log of the density will fall faster than quadratically. In the tails, the gradient of the log density will be large, and a small stepsize will be needed for stability. 这个在Langevin method中有详细描述。</p>
<p>这个问题可以通过选取一个随机的<span class="math inline">\(\epsilon\)</span>进行减轻。即便这个分布的均值太大，合适的小的<span class="math inline">\(\epsilon\)</span>也有可能偶尔会选到。随机选择步长应该处于计算轨道之前，而不是每次更新轨道leapfrog step的时候。有一个“捷径”方法能够当随机选取的步长不合适的时候节约计算时间。</p>
</div>
<div id="what-trajectory-length" class="section level4">
<h4><span class="header-section-number">10.3.2.3</span> What Trajectory Length?</h4>
<p>选取一个合适的轨道长度对于一个要系统的，而不是随机游走的探索状态空间的HMC非常重要。很多分布非常难以取样因为他们紧紧的在某些方向被约束住了，但是在其他方向的约束没那么紧。用足够长的轨道就能探索没那么多限制的方向。</p>
<p>但是轨道也可能过长。</p>
<p>对于更复杂的问题，不能用肉眼观察轨道的长度选取的如何，找到变量的线性组合使得约束最小会非常困难，甚至于不可能，当最小的方向其实是一个非线性曲线或者平面的时候。
所以对tajectory长度的试错在某些情况就很有必要。假设对于某些非常困难的问题，用一个L=100的轨道可能会合适starting point。如果实验性的run（和某些合适的<span class="math inline">\(\epsilon\)</span>）,显示HMCHMC达到了几乎独立的点在一次迭代之后，一个更小的L值可能就应该在下一步的时候使用。如果是是L=100还是有很高的自相关的话，可能得在下一次迭代中尝试<span class="math inline">\(L=1000\)</span>.</p>
<p>之后会讨论随机变化的轨道长可能很有必要。这样可以避免选到一个靠近周期性的长度。</p>
</div>
<div id="using-multiple-stepsizes." class="section level4">
<h4><span class="header-section-number">10.3.2.4</span> Using Multiple Stepsizes.</h4>
<p>所以可以通过找相对的variable的scales去提高HMC的表现。这可以通过两个等价的方法得到。第一个是如果<span class="math inline">\(s_i\)</span>是<span class="math inline">\(q_i\)</span>合适的scale，我们可以变换q，使得<span class="math inline">\(q_{i}^{\prime}=q_{i} / s_{i}\)</span>.或者也可以用另外一个动能函数<span class="math inline">\(K(p)=p^TM^{-1}p\)</span>,其中M的对角元是<span class="math inline">\(m_i=1/s_i^2\)</span>.
第三种等价方式是最方便的，是使用不同的stepsizes for different pairs of position and momentum variables. 考虑一个leapfrog update with <span class="math inline">\(m_i=1/s_i^2\)</span>:
<span class="math display">\[
\begin{aligned} p_{i}(t+\varepsilon / 2) &amp;=p_{i}(t)-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}(q(t)) \\ q_{i}(t+\varepsilon) &amp;=q_{i}(t)+\varepsilon s_{i}^{2} p_{i}(t+\varepsilon / 2) \\ p_{i}(t+\varepsilon) &amp;=p_{i}(t+\varepsilon / 2)-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}(q(t+\varepsilon)) \end{aligned}
\]</span>
Define <span class="math inline">\((q^{(0)},p^{(0)})\)</span> 为leapfrog的开始点，然后定义<span class="math inline">\(\left(q^{(1)}, p^{(1)}\right)\)</span> to be the final state (i.e.<span class="math inline">\((q(t+\varepsilon), p(t+\varepsilon))\)</span>),定义<span class="math inline">\(p^{(1/2)}\)</span>作为半途的动量。我们现在重新写leapfrog step如下：
<span class="math display">\[
\begin{aligned} p_{i}^{(1 / 2)} &amp;=p_{i}^{(0)}-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}\left(q^{(0)}\right) \\ q_{i}^{(1)} &amp;=q_{i}^{(0)}+\varepsilon s_{i}^{2} p_{i}^{(1 / 2)} \\ p_{i}^{(1)} &amp;=p_{i}^{(1 / 2)}-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}\left(q^{(1)}\right) \end{aligned}
\]</span>
这是一个标准的leapfrog更新过程，以下就加上rescaled momentum variables: <span class="math inline">\(\tilde{p}_{i}=s_{i} p_{i}\)</span> 并且有步长<span class="math inline">\(\epsilon_i=s_i\epsilon\)</span>, 我们可以写leapfrog update如下：</p>
<p><span class="math display">\[
\begin{aligned} \tilde{p}_{i}^{(1 / 2)} &amp;=\tilde{p}_{i}^{(0)}-\left(\varepsilon_{i} / 2\right) \frac{\partial U}{\partial q_{i}}\left(q^{(0)}\right) \\ q_{i}^{(1)} &amp;=q_{i}^{(0)}+\varepsilon_{i} \tilde{p}_{i}^{(1 / 2)} \\ \tilde{p}_{i}^{(1)} &amp;=\tilde{p}_{i}^{(1 / 2)}-\left(\varepsilon_{i} / 2\right) \frac{\partial U}{\partial q_{i}}\left(q^{(1)}\right) \end{aligned}
\]</span>
这就是像一个leapfrog update对于<span class="math inline">\(m_i=1\)</span>,但是对于不同的<span class="math inline">\((q_i,p_i)\)</span>对有不同的stepsize。当然，这个连续的<span class="math inline">\((q,\tilde p)\)</span> 不在被解释为一个Hamiltonian dynamics在一致的时间点，但是没有结论说使用这些HMC的轨道。注意当我们在计算轨道之前抽栋梁的时候，每一个<span class="math inline">\(\tilde p_i\)</span> 是独立的来自于Gaussian分布（<span class="math inline">\(N(0,I)\)</span>），而不是<span class="math inline">\(s_i\)</span>.
这个多个stepsize的方法一般来说是最便利的，特别是估计的scales，<span class="math inline">\(s_i\)</span>不是固定的，然后momentum只是partially refreshed。</p>
</div>
</div>
<div id="combining-hmc-with-other-mcmc-updates" class="section level3">
<h3><span class="header-section-number">10.3.3</span> Combining HMC with Other MCMC Updates</h3>
<p>对于某些问题，MCMC孤立的使用HMC是不可能或者不受欢迎的。两个情况Non-HMC更新将会很有必要，1当某些变量是离散的，2当log probability density的导数很难计算或者不可能计算的时候。</p>
<p>这种情况的时候，HMC可以只对其他变量生效。另外一个例子是当特殊的MCMC更新过程可以帮助MCMC收敛，同时HMC不会，比如说，从两个孤立的极值点之间移动。但是并不会完全代替HMC。下面将会讨论，比如说Bayesian hierarchical models可能用HMC结合Gibbs最好。
在这些情况里，一个或者多个HMC updates对于所有变量，或者变量的子集可以代替一个或者多个更新，从而使得对于全部变量的联合分布保持稳定invariant.
没看懂
The HMC updates can be viewed as either leaving this same joint distribution invariant, or as leaving invariant the conditional distribution of the variables that HMC changes, given the current values of the variables that are fixed during the HMC udpate. These are equivalent views.</p>
<p>When both HMC and other updates are used, it may be best to use shorter trajectories for HMC than would be used if only HMC were being done. This allows the other updates to be done more often, which presumably helps sampling. Finding the optimal tradeoff is likely to be difficult, however, A variation on HMC that reduces the need for such a tradeoff is described below.</p>
</div>
<div id="scaling-with-dimensionality" class="section level3">
<h3><span class="header-section-number">10.3.4</span> Scaling with Dimensionality</h3>
<p>Main benefits of HMC was illustrated, its ability to avoid the inefficient exploration of the state space via a random walk.</p>
<div id="creating-distributions-of-increasing-dimensionality-by-replication" class="section level4">
<h4><span class="header-section-number">10.3.4.1</span> Creating Distributions of Increasing Dimensionality by Replication</h4>
<p>How performance scales with dimensionality, assume something about how the distribution changes with dimensionality, <span class="math inline">\(d\)</span>.</p>
<p>Dimensionality increases by adding independent replicas of variables- that is, the potential energy function for <span class="math inline">\(q=(q_1,...,q_d)\)</span> has the form <span class="math inline">\(U(q)=\sum u_i(q_i)\)</span>, for functions <span class="math inline">\(u_i\)</span> drawn independently from some distribution. 实际情况肯定不是这样，但是这样可以作为一个有道理的模型来看看提高维度对某些问题的影响。</p>
<p>虽然在独立的情况下Gibbs表现的非常好，对每个变量进行Metropolis update 也很好，但是这两个算法都是not invariant to rotation. 所以对于某些特殊问题表现可能就不会太好。</p>
</div>
<div id="scaling-of-hmc-and-random-walk-metropolis" class="section level4">
<h4><span class="header-section-number">10.3.4.2</span> Scaling of HMC and Random-Walk Metropolis</h4>
<p>Discuss informally how well HMC and random-walk Metropolis scale with dimension, loosely following Creutz.</p>
<p>The following relationship holds when any Metropolis-style algorithm is used to sample a density <span class="math inline">\(P(x)=(1 / Z) \exp (-E(x))\)</span>:
<span class="math display">\[
1=\mathrm{E}\left[P\left(x^{*}\right) / P(x)\right]=\mathrm{E}\left[\exp \left(-\left(E\left(x^{*}\right)-E(x)\right)\right)\right]=\mathrm{E}[\exp (-\Delta)]
\]</span>
当x是目前的状态，假设分布是<span class="math inline">\(P(x)\)</span>,<span class="math inline">\(x^*\)</span> 是proposed state，任意的<span class="math inline">\(\Delta=E\left(x^{*}\right)-E(x)\)</span>. Jensen’s inequality暗示了能量的区别是非负的
<span class="math display">\[
\mathrm{E}[\Delta] \geq 0
\]</span>
不等式一般情况下是严格的。</p>
<blockquote>
<p>有个问题，怎么通过Jesen不等式得到这个？
Jesen不等式的形式：<span class="math inline">\(\varphi(\mathrm{E}[X]) \leq \mathrm{E}[\varphi(X)]\)</span> 怎么和<span class="math inline">\(\Delta\)</span>那个形式联系起来？感觉好奇怪.想复杂了，就是因为那个1的问题，然后用jesen不等式，也就是函数在里面是1，所以函数在外面就大于等于log(1)=0</p>
</blockquote>
<p>When <span class="math inline">\(U(q)=\sum u_i(q_i)\)</span> and proposals are produced independently for each i, we can apply these relationship either to a single variable (or pair of variables) or to the entire state. For a single variable(pair), write <span class="math inline">\(\Delta_1\)</span> for <span class="math inline">\(E(x^*)-E(x)\)</span>, with <span class="math inline">\(x=q_i\)</span> and <span class="math inline">\(E(x)=u_i(q_i)\)</span></p>
<blockquote>
<p>中间过程没怎么看懂，看看结论先。</p>
</blockquote>
<p>With increasing dimension by replicating variables will lead to increasing energy differences, since <span class="math inline">\(\Delta _d\)</span> is the su of <span class="math inline">\(\Delta_1\)</span> for each variable, each of which has positive mean. This will lead to a decrease in the acceptance probability , which is <span class="math inline">\(min(1,exp(-\Delta_d))\)</span> unless the width of the proposal distribution or the leapfrog stepsize is decreased to compensate.</p>
<blockquote>
<p>后面的大致意思是对于RWM维度越高，那么对于目前状态和proposal的state，potential energy的差异是每个变量造成的差异之和。如果使用固定的标准差<span class="math inline">\(\zeta\)</span>，potential energy difference的mean and variance随着d增大线性增长。这样会导致很低的接受率。所以为了保证能接受的表现，标准差应该随着d增加而降低，iteration次数应该需要保证几乎独立的点。</p>
</blockquote>
<p>而和RWM类似，HMC的话，q是独立的，使用能量函数<span class="math inline">\(K(p)=\Sigma p_{i}^{2} / 2\)</span>,不同的<span class="math inline">\((q_i,p_i)\)</span> 组都能服从Hamiltonian dynamics通过potential energy 用q，Kinetic energy用p。而解轨道中的虚拟世界虚拟时间不会随着维度增加而增加(我觉得是指<span class="math inline">\(\epsilon L\)</span>不受维度影响？)但是使用leapfrog方法得到的proposal state的接受概率是the sum of the errors pertaining to each <span class="math inline">\((q_i,p_i)\)</span> pair. 对于固定的步长<span class="math inline">\(\epsilon\)</span>,一个固定的轨道长度，<span class="math inline">\(\epsilon L\)</span>, mean and variance of the error in H grow linearly with d. This will also lead a progressively lower acceptance rate as dimensionality increases.</p>
<p>所以为了判断哪个方法更好需要决定我们需要多迅速的改变<span class="math inline">\(\zeta\)</span> 和 <span class="math inline">\(\epsilon\)</span> 当d 增加的时候。当d增加的时候，那么<span class="math inline">\(\eta\)</span>和<span class="math inline">\(\epsilon\)</span>会趋向于0，同时<span class="math inline">\(\Delta_1\)</span> 将会也趋于0。使用二阶逼近<span class="math inline">\(exp(-\Delta_1)\)</span>，则有<span class="math inline">\(1-\Delta_{1}+\Delta_{1}^{2} / 2\)</span> ，则带入上式就有
<span class="math display">\[
\mathrm{E}\left[\Delta_{1}\right] \approx \frac{\mathrm{E}\left[\Delta_{1}^{2}\right]}{2}
\]</span></p>
<p>则对于<span class="math inline">\(\Delta_1\)</span>方差,是均值的2倍,那么对于<span class="math inline">\(\Delta_d\)</span>的方差也是其2倍均值。（even when <span class="math inline">\(\Delta_d\)</span> is not small）.</p>
<p>为了达到一个好的接受率，我们必须保证<span class="math inline">\(\Delta_d\)</span>在1附近，since a large mean will not be saved by a similarly large standard deviation(which would produce fairly frequent acceptances as <span class="math inline">\(\Delta_d\)</span> occasionally takes on a negative value).</p>
<p>It follows that <span class="math inline">\(E[\Delta_d]\)</span> is proportional to <span class="math inline">\(d^2_\zeta\)</span>, wo we can maintain a reasonable acceptance rate by letting <span class="math inline">\(\zeta\)</span> be proportional to <span class="math inline">\(d^{-1/2}\)</span>. The number of iterations needed to reach a nearly independent point will be proportional to <span class="math inline">\(\zeta^{-1/2}\)</span>. The number of iterations needed to reach a nearly independent point will be proportional to <span class="math inline">\(\zeta^{-2}\)</span>, which will be proportional to d.</p>
</div>
<div id="optimal-acceptance-rates" class="section level4">
<h4><span class="header-section-number">10.3.4.3</span> Optimal Acceptance Rates</h4>
<p>继续扩展以上的理论，我们可决定如果基于最优的tuning parameter<span class="math inline">\(\zeta\)</span>或者<span class="math inline">\(\epsilon\)</span>,对应的接受率是多少。</p>
<p>为了找到这个接受率，我们先注意Metropolis methods满足Detailed Balance条件，接受概率对于正负<span class="math inline">\(\Delta_d\)</span>是一样的。因为所有的proposal对于负的<span class="math inline">\(\Delta_d\)</span> 都接受，所以接受率一般来说是简单的两倍概率对于proposal有一个负的<span class="math inline">\(\Delta_d\)</span>.对于一个很大的d，中心极限定理暗示了<span class="math inline">\(\Delta_d\)</span>的分布是Gaussian，因为其为d个独立的<span class="math inline">\(\Delta_1\)</span>的值。</p>
<p>因为<span class="math inline">\(\Delta_d\)</span>的方差是二倍均值，<span class="math inline">\(E[\Delta_d]=\mu\)</span>. 则接受概率能写成如下形式：
对于大的d有：</p>
<p><span class="math display">\[
P(\text { accept })=2 \Phi\left(\frac{(0-\mu)}{\sqrt{2 \mu}}\right)=2 \Phi(-\sqrt{\mu / 2})=a(\mu)
\]</span></p>
<p><span class="math inline">\(\Phi(z)\)</span> 是标准高斯的累计分布函数。</p>
<p>对于RWM，得到一个独立的点的花销与<span class="math inline">\(1/(a\zeta^2)\)</span>成比例。我们可以从上面看出<span class="math inline">\(\mu=E[\Delta_d]\)</span> 与<span class="math inline">\(\zeta^2\)</span>成比例，所以花销服从以下比例
<span class="math display">\[
C_{\mathrm{rw}} \propto \frac{1}{(a(\mu) \mu)}
\]</span>
数值计算发现这个极小当<span class="math inline">\(\mu=2.8\)</span>和<span class="math inline">\(a(\mu)=0.23\)</span></p>
<p>The same optimal <span class="math inline">\(23\%\)</span> acceptance rate for random-walk Metropolis was previously obtained using a more formal analysis by Roberts et al (1997). The optimal 65% acceptance rate for HMC .</p>
</div>
<div id="exploring-the-distribution-of-potential-energy" class="section level4">
<h4><span class="header-section-number">10.3.4.4</span> Exploring the Distribution of Potential Energy</h4>
<p>更好的Scaling会让HMC的表现取决于resampling of the momentum variables. 我们可以看到考虑how well the methods explore the distribution of the potential energy, <span class="math inline">\(U(q)=\sum u_i(q_i)\)</span> .用动量的变量去探索能量函数。</p>
<blockquote>
<p>等等，能量不应该是distribution of interest吗？得返回去看看</p>
</blockquote>
<p>Because <span class="math inline">\(U(q)\)</span> is a sum of d independent terms, its standard deviation will grow in proportion to <span class="math inline">\(d^{1/2}\)</span>.</p>
<p>根据Caracciolo et al.(1994),expected change in potential energy from a single Metropolis update will be no more than order 1- intuitively, large upwards canges are unlikely to be accepted, and since Metropolis updates satisfy detailed balance, large downward changes must also be rare(in equilibrium). Because changes in <span class="math inline">\(U\)</span> will follow a random walk (due again to detailed balance). 因为U中的改变将会服从一次随机游走（也是一万年detailed balance），it will take at least order <span class="math inline">\((d^{1/2}/1)^2=d\)</span> Metropolis updates to explore the distribution of <span class="math inline">\(U\)</span>.</p>
<p>在HMC的第一步中，动量的重抽量一般贵改变潜在能量by an amount that is proportional to <span class="math inline">\(d^{1/2}\)</span>,因为kinetic energy is also a sum of d independent terms, and hence has standard deviation that grows as <span class="math inline">\(d^{1/2}\)</span> (more precisely, its standard deviation is <span class="math inline">\(d^{1/2}/2^{1/2}\)</span>). If the second step of HMC proposes a distant point this change in kinetic energy (and hence in H) will tend, by the end of the trajectory, to have become equally split between kinetic and potential energy. If the endpoint of this trajectory is accepted, the change in potential energy from a single HMC iteration will be proportional to <span class="math inline">\(d^{1/2}\)</span>, comparable to its overall range of variation. So, in contrast to random-walk Metropolis, we may hope that only a few HMC iterations will be sufficient to move to a nearly independent point, even for high-dimensional distributions.
Analyzing how well method explore the distribution of <span class="math inline">\(U\)</span> can also provide insight into their performance on distributions that are not well modeled by replication of variables, as we will see in the next section.</p>
<blockquote>
<p>这个order又是怎么一回事。。。</p>
</blockquote>
</div>
</div>
<div id="hmc-for-hierarchical-models" class="section level3">
<h3><span class="header-section-number">10.3.5</span> HMC for Hierarchical Models</h3>
<p>Many Bayesian models are defined hierarchically. A large set of low-level parameters have prior distributions that are determined by fewer higher-level “hyperparameters,” which in turn may have priors determined by fewer higher-level“hyperparameters,” which in turn may have priors determined by yet-higher-level hyperparameters.</p>
<p>可以应用HMC于这些模型通过一个很显然的办法： (注意对variance hyperparameters做一个log变换，使得无限制)。然而，最好应用HMC仅仅对lower-level parameters, 原因以下会讨论，（5.4.3一般性的讨论了如何applying HMC对variable的子集。）
Bayesian neural network model会作为一个例子。这些模型一般来讲有几组low-level parameters,每个associated variance hyperparameter. The posterior distribution of these hyperparameters reflects important aspects of the data,比如说哪些变量和目标任务最相关。The efficiency with which values for these hyperparameters are sampled from the posterior distribution can often determine the overall efficiency of the MCMC method.
抽样hyperparameter的efficiency决定了MCMC的全局效率？？
I use HMC only for the low-level parameters in Bayesian neural network models, with the hyperparameters being fixed during an HMC update. These HMC updates alternate with Gibbs sampling updates of the hyperparameters, which (in the simpler versions of the models) are independent given the low-level parameters, and have conditional distributions of standard form.
所以用HMC的只是“低阶”变量，超参数则用Gibbs更新，一个混合的过程。The leapfrog stepsizes used can be set using heuristic that are based on the current hyperparameter values. (I use the multiple stepsize approach described at the end of Section 5.4.2, equivalent to using different mass values, <span class="math inline">\(m_i\)</span>, for different parameters.)leapfrog方法的步长用启发式方法进行决定，基于目前的超参数。比如说，the size of the network “weights” on connections out of a “hidden unit” determine how sensitive the likelihood function is to changes in weights on connections into the hidden unit; the variance of the weights on these outgoing connections. If the hyperparameters were changed by the same HMC updates as change the lower-level parameters, using them to set stepsizes would not be valid， since a reversed trajectory would use different stepsizes, and hence not retrace the original trajectory. Without a good way to set setpsizes, HMC for the low-level parameters would likely be much less efficient.</p>
<blockquote>
<p>真不是很懂好奇怪，大概emmmm，这样更新会有点问题，所以只用一部分？</p>
</blockquote>
<p>Choo(2000) bypassed （跳过了？） this problem by using a modification of HMC in which trajectories are simulated by alternating leapfrog steps tha update only the hyperparameters with leapfrog steps that update only the low-level parameters. This procedure maintains both reversibility and volume preservation. However, performance did not improve as hoped because of a second issue with hierarchical models.</p>
<p>In these Bayesian neural network models, and many other hierarchical models, the joint distribution of both low-level parameters and hyperparameters is highly skewed.</p>
</div>
</div>
<div id="extensions-of-and-variations-on-hmc" class="section level2">
<h2><span class="header-section-number">10.4</span> Extensions of and Variations on HMC</h2>
<p>Modify the basic HMC algorithm can be done in many ways. Improve its efficiency, make it useful for a wider range of distribution.</p>
<p>Alternatives to the leapfrog discretization of Hamilton’s equation will be discussed. Also how the HMC handle distributions with constraints on the variables.
Another algorithm that leapfrog step only done once, which may be useful when not all variables are updated by HMC.</p>
<p>Computing a short cut method that avoids computing the whole trajectory when the stepsize is inappropriate.</p>
<div id="discretization-by-splitting-handling-constraints-and-other-applications" class="section level3">
<h3><span class="header-section-number">10.4.1</span> Discretization by Splitting: Handling constraints and Other Applications</h3>
<p>Leapfrog method 不是解决Hamilton’s等式，保证volume和reversible的唯一方法。所以也有其他方法可以用于HMC。 很多“symplectic integration method”被设计出来了，很多用于其他方面的应用而不是HMC（比如模拟太阳系几百万年的运行用于检验稳定性。）设计一个有更高阶的准确度的方法是可能的，比如（McLachlan and Atela,1992）。使用这些方法在HMC上，当维度增加时HMC的会渐进地产出更好的结果。尽管如此，还是值得研究Hamiltonian dynamics如何被模拟，这样才能指出如何处理variable的constrain，也可以提高探索偏分析的结果。</p>
<div id="splitting-the-hamiltonian" class="section level4">
<h4><span class="header-section-number">10.4.1.1</span> Splitting the Hamiltonian</h4>
<p>Many symplectic discretization of Hamiltonian dynamics can be derived by “splitting” the Hamiltonian into several terms, and then, for each term in succession, simulating the dynamics defined by that term for some small time step,
很多辛离散化Hamiltonian dynamics可以用“splitting” Hamiltonian 成为几项导出，然后对于每项一连串的模拟dynamics由定义一些小的时间步骤，重复这个过程知道达到要求的总模拟时间。如果每一项的模拟可以解析的得到，那么我们可以得到辛逼近对dynamics如果可以做到的话。</p>
<p>这个更一般的方法可以从Leimkuhler and Reich (2004,section4.2)和(Sexton and Weingarten (1992))找到.</p>
<p>假设Hamiltonian可以写成k个项的和</p>
<p><span class="math display">\[
H(q, p)=H_{1}(q, p)+H_{2}(q, p)+\cdots+H_{k-1}(q, p)+H_{k}(q, p)
\]</span>
假设我们能 <em>exactly</em> 实现Hamiltonian dynamics 基于每个<span class="math inline">\(H_i\)</span>,for <span class="math inline">\(i=1,...,k\)</span>, with <span class="math inline">\(T_{i,\epsilon}\)</span> 作为一个映射定义在应用dynamics基于<span class="math inline">\(H_i\)</span> for time <span class="math inline">\(\epsilon\)</span>. As shown by Leimkuhler and Reich, 如果<span class="math inline">\(H_i\)</span> 二阶可微，那么这些映射的组合<span class="math inline">\(T_{1, \varepsilon} \circ T_{2, \varepsilon} \circ \cdots \circ T_{k-1, \varepsilon} \circ T_{k, \varepsilon}\)</span>,是一个可行的离散化Hamiltonian dynamics 基于H。 这个会产生exact dynamics in the limit <span class="math inline">\(\epsilon\)</span> goes to zero, with global error of order <span class="math inline">\(\epsilon\)</span> or less.</p>
<p>更多的，这个离散化将会保volume,也是symplectic的，因为这些性质对每个<span class="math inline">\(T_{i,\epsilon}\)</span>都成立。这个离散化也是reversible的,如果<span class="math inline">\(H_{i}(q, p)=H_{k-i+1}(q, p)\)</span>.就如前文所提到的，所有的reversible的方法都有偶数阶的global error in <span class="math inline">\(\epsilon\)</span>,意味着global error must be of order <span class="math inline">\(\epsilon^2\)</span>,或者更好。</p>
<p>我们可以推出leapfrog方法来自于对称分解Hamiltonian，如果有<span class="math inline">\(H(q, p)=U(q)+K(p)\)</span>,那么我们可以把Hamiltonian写成
<span class="math display">\[
H(q, p)=\frac{U(q)}{2}+K(p)+\frac{U(q)}{2}
\]</span>
与之对应的分解是<span class="math inline">\(H_{1}(q, p)=H_{3}(q, p)=U(q) / 2\)</span>和 <span class="math inline">\(H_{2}(q, p)=K(p)\)</span>,则此时Hamiltonian dynamics基于<span class="math inline">\(H_1\)</span>就是
<span class="math display">\[
\begin{aligned} \frac{d q_{i}}{d t} &amp;=\frac{\partial H_{1}}{\partial p_{i}}=0 \\ \frac{d p_{i}}{d t} &amp;=-\frac{\partial H_{1}}{\partial q_{i}}=-\frac{1}{2} \frac{\partial U}{\partial q_{i}} \end{aligned}
\]</span>
应用这个dynamics for time <span class="math inline">\(\epsilon\)</span>,just adds <span class="math inline">\(-(\epsilon/2)\partial U / \partial q_{i}\)</span> to each <span class="math inline">\(p_i\)</span>, which is the first part of a leapfrog step. The dynamics based on <span class="math inline">\(H_2\)</span> is as follows:</p>
<p><span class="math display">\[
\begin{aligned} \frac{d q_{i}}{d t} &amp;=\frac{\partial H_{2}}{\partial p_{i}}=\frac{\partial K}{\partial p_{i}} \\ \frac{d p_{i}}{d t} &amp;=-\frac{\partial H_{2}}{\partial q_{i}}=0 \end{aligned}
\]</span>
If <span class="math inline">\(K(p)=\frac{1}{2}\sum p_i^2/m_i\)</span>, applying this dynamics for time <span class="math inline">\(\epsilon\)</span> results in adding <span class="math inline">\(\epsilon p_i/m_i\)</span> to each <span class="math inline">\(q_i\)</span> which is the second part of a leapfrog step. Finally, <span class="math inline">\(H_3\)</span> produces the third part of a leapfrog step (Equation 5.20), which is the same as the first part, since <span class="math inline">\(H_3=H_1\)</span>.</p>
<blockquote>
<p>以下感觉吃力了，需要重新理清楚之前的理论才好深入，所以先泛读一遍吧，笔记就不写了</p>
</blockquote>
</div>
<div id="handling-constraints" class="section level4">
<h4><span class="header-section-number">10.4.1.2</span> Handling Constraints</h4>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="optimal-proposal-distributions-and-adaptive-mcmc.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayes-variable-selection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
