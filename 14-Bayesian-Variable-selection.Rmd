# Bayes variable selection

The model uncertainty problem with the $2^p$ competing models:
 
$$
M_{\gamma} : \boldsymbol{y}=\alpha \mathbf{1}_{n}+\boldsymbol{X}_{\gamma} \boldsymbol{\beta}_{\gamma}+\boldsymbol{\varepsilon}
$$
 
The Null model:

$$
M_{0} : y=\alpha 1_{n}+\varepsilon
$$


Assuming that one of the models in $\mathcal M$ is the true model, the posterior probability of any model is
$$
\operatorname{Pr}\left(M_{\gamma^{*}} | \boldsymbol{y}\right)=\frac{m_{\gamma *}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma^{*}}\right)}{\sum_{\gamma} m_{\gamma}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma}\right)}
$$
where $Pr(M_\gamma)$ is the prior probability of $M_\gamma$ and $m_\gamma$ is the integrated likelihood with respect to the prior $\pi_\gamma$:
$$
m_{\gamma}(\boldsymbol{y})=\int f_{\gamma}\left(\boldsymbol{y} | \boldsymbol{\beta}_{\gamma}, \alpha, \sigma\right) \pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma}, \alpha, \sigma^{2}\right) d \boldsymbol{\beta}_{\gamma} d \alpha d \sigma^{2}
$$
也就是把所有不确定因素积掉以后，y的分布函数,also called the (prior) marginal likelihood.

For $\gamma=0$ this integrated lkelihood becomes:

$$
m_{0}(\boldsymbol{y})=\int f_{0}(\boldsymbol{y} | \alpha, \sigma) \pi_{0}\left(\alpha, \sigma^{2}\right) d \alpha d \sigma^{2}
$$

可以用Bayes factor替换 integrated likelihood function:

$$
\operatorname{Pr}\left(M_{\gamma^{*}} | \boldsymbol{y}\right)=\frac{B_{\gamma^{*}}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma^{*}}\right)}{\sum_{\gamma} B_{\gamma}(\boldsymbol{y}) \operatorname{Pr}\left(M_{\gamma}\right)}
$$

As stated in the introduction, we are mainly interested in software that implements the formal Bayesian answer which implies that we use the posterior distribution.

Due to the following three aspects  


- The priors that the package accomodates, that is, $\pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma}, \alpha, \sigma^{2}\right)$ and $Pr(M_\gamma)$

- the tools provided to summarize the posterior distribution and obtain model averaged inference

- the numerical methods implemented to compute the posterior distribution

## Prior Specification

The two inputs that are needed to obtain the posterior distribution are $\pi_\gamma$ and $Pr(M_r)$ the $2^p$ prior distributions for the parameters within each model and the prior distribution over the model space, respectively.

不失一般性，先验分布$\pi_\gamma$ 可以写成
$$
\pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma}, \alpha, \sigma^{2}\right)=\pi_{\gamma}\left(\boldsymbol{\beta}_{\gamma} | \alpha, \sigma^{2}\right) \pi_{\gamma}\left(\alpha, \sigma^{2}\right)
$$
，也就是coefficient，intercept，和variance的先验，
基于最方便的方法，基础Jeffreys' prior is used for the parameters that are common to all models:

$$
\pi_{\gamma}\left(\alpha, \sigma^{2}\right)=\sigma^{-2}
$$
对于$\beta$,则要么使用正态，要么使用mixture正态，中心点在0. ("by reasons of similarity",Jeffreys,1961) and scaled by $\sigma^{2}\left(\boldsymbol{X}_{\gamma}^{t} \boldsymbol{X}_{\gamma}\right)^{-1}$, "a matrix suggested by the form of the information matrix." times a factor g, normally called a "g-prior". 目前的研究表明这样的方便的prior拥有一系列的最优的性质可以扩展到对超参数g做特殊的先验。 Among these properties are invariance under affine transformations of the covariates, several types of predictive matching and consistency ( Bayarri et al., 2002).

The specification of g has inspired many interesting studies in the literature. Of these, we have collected the most popular one in Table1.

Relatedd with the conventional priors, which inspired by asymptotically reproducing the popular Bayesian Information Criterion (Schwarz,1978). Raftery proposes using the same covariance matrix as the Unit Information Prior, but with mean the maximum likelihood estimator $\hat\beta_\gamma$ (instead of the zero mean of the conventional prior).

Other priors specifically used in model uncertainty problems are the *spike and slab priors.*
Assume that the components of $\beta$ are independent, each having a mixture of two distributions: one highly concentrated on zero (the spike) and the other one quite disperse (the slab). There are two different developments of this idea in the literature. 

There are two different developments of this idea. Original version is Mitchell and Beauchamp (1988), the spike is a degenerate distribution at zero so this fits with what we have called the formal approach.

Another proposal by George and McCulloch (1993) which the spike is a continuous distribution with a small variance also received a lot of attention, perhaps for computational advantages.

模型空间的prior,$\mathcal M$, 一个非常受欢迎的起点是

$$
Pr(M_\gamma|\theta)=\theta^{p_\gamma}(1-\theta)^{p-p_\gamma}
$$
where $p_\gamma$ is the number of covariates in $M_\gamma$ and the hyperparameter $\theta\in(0,1)$ has the interpretation of the common probability that a given variable is included (independently of all others).

Among the most popular default choice for $\theta$ are

- Fixed $\theta=1/2$, which assign equal prior probability to each model i.e. $\operatorname{Pr}\left(M_{\gamma}\right)=1 / 2^{p}$

- Random $\theta \sim \operatorname{Unif}(0,1)$, giving euqal probability to each possible number of covariates or model size.

一般来说，固定的$\theta$会在多样性上表现非常差，在测试中，伪造的解释变量经常在结果中出现，然后lead to 更有信息的先验。这套个情况可以用随机的$\theta$进行避免，第二个proposal 见Scott and Berger (2010.) Lay and Steel(2009) 考虑使用$\theta\sim Beta(1,b)$ 可以导出binomial-beta 先验 for the number of covariates in the model or the model size,W:
$$
\operatorname{Pr}(W=w | b) \propto \left( \begin{array}{c}{p} \\ {w}\end{array}\right) \Gamma(1+w) \Gamma(b+p-w), w=0,1, \ldots, p
$$
注意$b=1$ 时退化到uniform prior on $\theta$ and also on $W$. Ley and Steel (2009), this setting is useful to incorporate prior information about the mean model size, say $w^*$. This would translate into $b=(p-w^*)/w^*$.

## Summaries the posterior distribution and model averaged inference

The simplest summary of the posterior model distribution is its mode

$$
\underset{\gamma}{\arg \max } \operatorname{Pr}\left(M_{\gamma} | \boldsymbol{y}\right)
$$

This model is the model most supported by the information (data and prior) 

This is normally called *HPM*(jighest posterior model) or MAP (maximum a posteriori) model.

When p is moderate to large, posterior inclusion probabilities (PIP) are very useful.

$$
\operatorname{Pr}\left(\gamma_{i}=1 | \boldsymbol{y}\right)=\sum_{x_{i} \in M_{\gamma}} \operatorname{Pr}\left(M_{\gamma} | \boldsymbol{y}\right)
$$
这个可以理解成每个变量解释response的重要性。

这个概率也可以用于定义另外一个summary，叫median probability model(MPM) which is the model containing the covariates with inclusion probability larger than 0.5. This model in some case is optimal for prediction.



## Numerical Methods











